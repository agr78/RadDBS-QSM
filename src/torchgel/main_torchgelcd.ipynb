{"cells":[{"cell_type":"markdown","metadata":{},"source":["This notebook will show how to use the `gel` package to perform group elastic net regression. Suppose we have feature matrices $A_j$ ($j = 1 \\dots p$), each of size $m \\times n_j$ (i.e. $m$ $n_j$-dimensional samples), and targets $y$ (a $m$ dimensional vector). Group elastic net finds coefficients $\\beta_j$ ($n_j$ dimensional vector), and $\\beta_0$ (scalar) by solving the optimization problem\n","\n","$$ \\min_{\\beta_0,\\dots,\\beta_p} \\frac{1}{2}\\left\\|y - \\beta_0 - \\sum_{j=1}^p A_j\\beta_j\\right\\|^2 + m\\sum_{j=1}^p \\sqrt{n_j}\\left(\\lambda_1\\|\\beta_j\\| + \\lambda_2\\|\\beta_j\\|^2\\right). $$\n","\n","We will test this with some generated data."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["m_tr = 100  # number of samples\n","ns = [10, 20, 5, 200, 1, 20]  # group sizes\n","\n","# data distribution parameters\n","X_scale = 2.0\n","X_bias = 3.0\n","\n","# coefficient distribution parameters\n","β_scale = 0.125\n","β_bias = 6.0\n","\n","# true support\n","support = [0, 1, 0, 1, 1, 0]"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Generate data\n","import torch\n","\n","A_trs = [torch.randn(m_tr, n_j)*X_scale + X_bias for n_j in ns]\n","βs = [s_j*(torch.randn(n_j, 1)*β_scale + β_bias) for s_j, n_j in zip(support, ns)]\n","y_tr = sum([A_j@β_j for A_j, β_j in zip(A_trs, βs)])[:, 0]"]},{"cell_type":"markdown","metadata":{},"source":["We will now use `gel` to solve group elastic net with a single pair of regularization values. There are two steps to this. First, `make_A` is used to convert the features as needed by the library. This, and other parameters are passed to gel_solve.\n","\n","There are two implementations, `gelfista` and `gelcd`, which use proximal gradient descent and coordinate descent respectively. Refer to the respective source files for details on these methods. Here, we will use `gelfista`. `gelcd` is used in the same way."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from gel.gelcd import make_A, gel_solve, block_solve_newton"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["A_conv_tr = make_A(\n","    A_trs,\n","    torch.tensor(ns),  # note that this needs to be a LongTensor\n","    device=torch.device(\"cpu\"),\n","    dtype=torch.float32,\n",")\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# print(A_conv_tr.shape)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# print(y_tr.shape)  # note that y should be one dimensional"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["l_1 = 1.0  # λ_1\n","l_2 = 0.1  # λ_2"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","\n","\u001b[A\u001b[A\n","\n","\u001b[A\u001b[A"]},{"ename":"TypeError","evalue":"'NoneType' object is not subscriptable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-638944562d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Cs=None,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Is=None,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n","\u001b[0;32m~/anaconda3/envs/pdradenv/lib/python3.7/site-packages/gel/gelcd.py\u001b[0m in \u001b[0;36mgel_solve\u001b[0;34m(A, y, l_1, l_2, ns, b_init, block_solve_fun, block_solve_kwargs, max_cd_iters, rel_tol, Cs, Is, verbose)\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;31m# Add C_j and I_j to the arguments if using Newton's method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock_solve_fun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mblock_solve_newton\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                     \u001b[0mblock_solve_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"C_j\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m                     \u001b[0mblock_solve_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"I_j\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"]}],"source":["b_0, B = gel_solve(\n","    A_conv_tr,\n","    y_tr,\n","    l_1,\n","    l_2,\n","    ns,\n","    b_init=None,     # initialization value - useful if solving multiple instances\n","    block_solve_fun=block_solve_newton,\n","    # block_solve_kwargs=None,\n","    # max_cd_iters=None,\n","    # rel_tol=1e-6,\n","    # Cs=None,\n","    # Is=None,\n","    verbose=True,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["The returned `b_0` is a scalar, and `B` is a matrix. Each row of `B` is the coefficient vector for the corresponding group, padded with 0s. So we will first extract the vectors."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["βhats = [B[j, :ns[j]] for j in range(len(ns))]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(b_0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print([βhat_j.norm() for βhat_j in βhats])"]},{"cell_type":"markdown","metadata":{},"source":["This is the solution for a single (l_1, l_2) pair. But practically, we want to use a grid of values. So now, we will use the `gelpaths` module to sweep over multiple regularization values and pick the best based on validation results."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# First, we generate validation data.\n","m_val = 30\n","A_vals = [torch.randn(m_val, n_j)*X_scale + X_bias for n_j in ns]\n","y_val = sum([A_j@β_j for A_j, β_j in zip(A_vals, βs)])[:, 0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We need to standardize the data to prevent repeated computations.\n","A_tr_μs = [A_j.mean(dim=0, keepdim=True) for A_j in A_trs]\n","A_tr_σs = [A_j.std(dim=0, keepdim=True) for A_j in A_trs]\n","A_tr_stans = [(A_j - A_μ_j) / A_σ_j for A_j, A_μ_j, A_σ_j in zip(A_trs, A_tr_μs, A_tr_σs)]\n","A_val_stans = [(A_j - A_μ_j) / A_σ_j for A_j, A_μ_j, A_σ_j in zip(A_vals, A_tr_μs, A_tr_σs)]\n","\n","y_tr_μ = y_tr.mean()\n","y_tr_σ = y_tr.std()\n","y_tr_stan = (y_tr - y_tr_μ) / y_tr_σ"]},{"cell_type":"markdown","metadata":{},"source":["`gelpaths` follows a 2-step procedure. For each (l_1, l_2) pair, the corresponding group elastic net problem is solved; this is then used to find the support by finding non-zero coefficient vectors. The support is then used to solve ridge regression over a range of regularization values.\n","\n","For each ridge solution, a summary function is called. This is where we will compute the validation error, to find the optimal regularization values. Note that the summary function expects the features as a single matrix. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_val = torch.cat(A_val_stans, dim=1).t()  # note the transpose\n","\n","def summary(_support, _b):\n","    if _support is None:\n","        # empty support, so just use the computed training mean\n","        yhat_val = y_tr_μ\n","        _support_set = {}\n","    else:\n","        X_val_supp = X_val[_support]\n","        yhat_val = (X_val_supp.t() @ _b) * y_tr_σ + y_tr_μ  # we have to rescale\n","        _support_set = set([s.item() for s in _support])\n","        \n","    rmse = ((y_val - yhat_val) ** 2).mean().sqrt().item()\n","\n","    # compute group support\n","    group_support = []\n","    start_idx = 0\n","    for n_j in ns:\n","        group_idxs = range(start_idx, start_idx + n_j)\n","        if all(i in _support_set for i in group_idxs):\n","            group_support.append(1)\n","        elif not any(i in _support_set for i in group_idxs):\n","            group_support.append(0)\n","        else:\n","            # This shouldn't happen! Groups are either all in or all out.\n","            raise AssertionError\n","        start_idx += n_j\n","\n","    return group_support, rmse"]},{"cell_type":"markdown","metadata":{},"source":["There are two implementations in `gelpaths`. `gel_paths` requires manually specifying regularization values, and `gel_paths2` picks them automatically. We will use `gel_paths2` here."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from gel.gelpaths import gel_paths2\n","\n","l_rs = torch.logspace(-10, 10, 100).tolist()  # regularization strengths for ridge regression\n","\n","summaries = gel_paths2(\n","    gel_solve,\n","    {\n","        # arguments to gel_solve\n","        \"t_init\": None,\n","        \"ls_beta\": 0.99,\n","        \"max_iters\": 3000,\n","        \"rel_tol\": 1e-5,\n","    },\n","    make_A,                                   # we don't explicitly call make_A here\n","    A_tr_stans,                               # we need to pass the standardized A values\n","    y_tr_stan,                                # note: standardized ouput vector\n","    ks=[0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99], # this is a list of values to trade-off\n","                                              #   between l_1 and l_2 regularizations, with\n","                                              #   l_1 = k*l and l_2 = (1 - k)*l\n","    n_ls=50,                                  # number of l values for each k value\n","    l_eps=1e-5,                               # ratio of minimum to maximum l value;\n","                                              #    n_ls number of l values are generated\n","                                              #    according to this ratio, for each k\n","    l_rs=l_rs,                                # ridge regularization values\n","    summ_fun=summary,                         # the summary function\n","    supp_thresh=1e-4,                         # norm threshold for considering coefficient vectors\n","                                              #     to be 0 (default 1e-6)\n","    device=torch.device(\"cpu\"),               # pytorch device (default cpu)\n","    verbose=True,                             # verbosity (default False)\n","    ls_grid=None,                             # override l value computation with a fixed grid\n","                                              #     (default is None)\n","    aux_rel_tol=1e-3,                         # tolerance for solving auxiliary problems\n","                                              #     to get better initializations (default 1e-3)\n","    dtype=torch.float32,                      # torch dtype (default float32)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Let us analyze the results."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_summ = min(summaries.items(), key=lambda z: z[1][1])\n","print(best_summ)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_support_hat = best_summ[1][0]\n","print(best_support_hat)"]},{"cell_type":"markdown","metadata":{},"source":["We were able to correctly identify the support! Now, we can perform a more thorough ridge regression on the identified support. For this we will use the `ridgepaths` module."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_tr_best_supp_hat = torch.cat(\n","    [A_j for A_j, s_j in zip(A_tr_stans, best_support_hat) if s_j == 1],\n","    dim=1,\n",").t()\n","\n","X_val_best_supp_hat = torch.cat(\n","    [A_j for A_j, s_j in zip(A_val_stans, best_support_hat) if s_j == 1],\n","    dim=1,\n",").t()\n","\n","def summary_best_supp_hat(support, b):\n","    # support is ignored\n","    yhat_val = (X_val_best_supp_hat.t() @ b) * y_tr_σ + y_tr_μ\n","    rmse = ((y_val - yhat_val) ** 2).mean().sqrt().item()\n","    return rmse"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from gel.ridgepaths import ridge_paths\n","\n","l_rs_big = torch.logspace(-20, 20, 10000).tolist()\n","\n","ridge_summaries = ridge_paths(\n","    X_tr_best_supp_hat,  # X needs to be pre-indexed using the support\n","    y_tr_stan,\n","    1,                   # this doesn't matter in this case since it is ignored; just shouldn't be None\n","    l_rs_big,\n","    summary_best_supp_hat,\n","    verbose=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_ridge_summ = min(ridge_summaries.items(), key=lambda z: z[1])\n","print(best_ridge_summ)"]},{"cell_type":"markdown","metadata":{},"source":["The validation RMSE doesn't change much, but the identified best lambda value _has_ changed. So now, using the identified support and ridge regularization value, we can train a final ridge regression model using the training and validation data combined to further increase performance. This can be tested using a separate test set if available."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dummy_summary(support, b):\n","    # Just return b\n","    return b"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Combine training and validation data.\n","A_trvals = [torch.cat([A_j_tr, A_j_val], dim=0) for A_j_tr, A_j_val in zip(A_trs, A_vals)]\n","y_trval = torch.cat([y_tr, y_val])\n","\n","# Compute combined statistics.\n","A_trval_μs = [A_j.mean(dim=0, keepdim=True) for A_j in A_trvals]\n","A_trval_σs = [A_j.std(dim=0, keepdim=True) for A_j in A_trvals]\n","y_trval_μ = y_trval.mean().item()\n","y_trval_σ = y_trval.std().item()\n","\n","# Standardize data using combined statistics.\n","A_trval_stans = [(A_j - A_j_μ) / A_j_σ for A_j, A_j_μ, A_j_σ in zip(A_trvals, A_trval_μs, A_trval_σs)]\n","y_trval_stan = (y_trval - y_trval_μ) / y_trval_σ\n","\n","# Project to support.\n","X_trval_best_supp_hat = torch.cat([\n","    A_j_trval\n","    for A_j_trval, s_j in zip(A_trval_stans, best_support_hat)\n","    if s_j == 1\n","], dim=1).t()\n","print(X_trval_best_supp_hat.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_lambda = best_ridge_summ[0]\n","final_summary = ridge_paths(\n","    X_trval_best_supp_hat,\n","    y_trval_stan,\n","    1,\n","    [best_lambda],\n","    dummy_summary,\n",")\n","final_b = final_summary[best_lambda]"]},{"cell_type":"markdown","metadata":{},"source":["Here, since we know the true β, we can evaluate by comparing to it. We will assume knowledge of the fact that all features have the same normal distribution, and compute how well we estimate its parameters."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from math import sqrt\n","\n","X_trval = torch.cat(A_trvals, dim=1)\n","X_bias_hat = X_trval.mean().item()\n","X_scale_hat = X_trval.std().item()\n","\n","supplen = sum(n_j for n_j, s_j in zip(ns, best_support_hat) if s_j == 1)\n","β_bias_hat = y_trval_μ / (X_bias_hat * supplen)\n","β_scale_hat = sqrt((((y_trval_σ ** 2) / supplen) - (β_bias_hat * X_scale_hat) ** 2) / ((X_bias_hat ** 2) + (X_scale_hat ** 2)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_scale_hat, X_bias_hat  # true values (2, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["β_scale_hat, β_bias_hat  # true values (0.125, 6)"]},{"cell_type":"markdown","metadata":{},"source":["We are able to estimate the parameters reasonably well, with most of the error in the scale parameter. And it should be noted again, that we did perfectly recover the support. To conclude, we compute the error on the combined training validation data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["yhat_trval = y_trval_σ * (X_trval_best_supp_hat.t() @ final_b) + y_trval_μ\n","rmse = ((yhat_trval - y_trval) ** 2).mean().sqrt().item()\n","print(rmse)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"pdradenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":2}
