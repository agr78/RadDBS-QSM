{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from econml.sklearn_extensions.linear_model import WeightedLassoCV\n",
    "import sklearn.model_selection as sms\n",
    "import sklearn.linear_model as slm\n",
    "import sklearn.preprocessing as skp\n",
    "import sklearn.metrics as sme\n",
    "import sklearn.feature_selection as skf\n",
    "import sklearn.ensemble as ske\n",
    "import sklearn.utils as sku\n",
    "import sklearn.decomposition as skd\n",
    "import sklearn.neural_network as skn\n",
    "from celer import GroupLassoCV\n",
    "from sklearnex import patch_sklearn, config_context\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from IPython.display import HTML\n",
    "import util_arch as util\n",
    "from scipy.spatial import cKDTree\n",
    "import nibabel as nib\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from mlp_feats import MLP\n",
    "from mlp_feats import train_model, model_cv\n",
    "import loss_landscapes\n",
    "import loss_landscapes.metrics\n",
    "import copy\n",
    "from torchviz import make_dot\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('''\n",
    "<style>\n",
    ".jupyter-matplotlib {\n",
    "    background-color: #000;\n",
    "}\n",
    "\n",
    ".widget-label, .jupyter-matplotlib-header{\n",
    "    color: #fff;\n",
    "}\n",
    "\n",
    ".jupyter-button {\n",
    "    background-color: #333;\n",
    "    color: #fff;\n",
    "}\n",
    "</style>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe y should also be scaled? \n",
    "# Perhaps a transform would be more effective, or scaling implemented with consistent cross-validation\n",
    "# Different scaling methods? \n",
    "#     This seems most important for noise-sensitive models like LARS. All other use StandardScaler()\n",
    "# Transformers?\n",
    "# Model-specific scaling methods?\n",
    "#     Yes, see above\n",
    "# Common cross-validation function ✓\n",
    "#     Use built-in functions wherever possible and `utils.gridsearch_pickparams()` elsewhere\n",
    "# Quantile loss\n",
    "# RANSAC\n",
    "# Data augmentation? (Mixup)\n",
    "# Data generation? (SMOGN)\n",
    "# Combine CHH dataset ✓\n",
    "# Implement CV and test ✓\n",
    "# Print selected features ✓\n",
    "# Make magnitude templates\n",
    "# Sample weights ✓\n",
    "# Look at segmentations by error ✓ (Appears to have most difference in red nucleus, which includes surrounding (white?) matter for underperforming cases)\n",
    "# Extract features from current (1:6) eroded ROIs ✓\n",
    "# Test features from eroded ROIs\n",
    "# Extract features from all ROIs\n",
    "# Plot segmentation variance against error for each case across all ROIs ✓\n",
    "# Why does excluding the subthalamic nucleus increase the correlation (r=0.5 -> r=0.6)?\n",
    "# Best performance with all ROIs: cvn=6, k=1800\n",
    "# Best performance with ROIs 0:4, excluding STN: cvn=6, k=1800\n",
    "# Should the pre-operative UPDRS be appended once or to each ROI? ✓\n",
    "# Plot histogram of features for successful and unsuccessful predictions ✓\n",
    "# Check which cases are unilateral, such as 29, 44, 45, 46\n",
    "# Compile bad case list: 2, 58, 59, 64, 68, 72, 75, 77, 78, 81, 85, 89, 90\n",
    "# How many neurologists overall have been involved in the UPDRS-III scoring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get case IDs\n",
    "case_list = open('/home/ali/RadDBS-QSM/data/docs/cases_90','r')\n",
    "lines = case_list.read()\n",
    "lists = np.loadtxt(case_list.name,comments=\"#\", delimiter=\",\",unpack=False,dtype=str)\n",
    "case_id = []\n",
    "for lines in lists:     \n",
    "    case_id.append(lines[-9:-7])\n",
    "\n",
    "# Load scores\n",
    "file_dir = '/home/ali/RadDBS-QSM/data/docs/QSM anonymus- 6.22.2023-1528.csv'\n",
    "motor_df = util.filter_scores(file_dir,'pre-dbs updrs','stim','CORNELL ID')\n",
    "# Find cases with all required scores\n",
    "subs,pre_imp,post_imp,pre_updrs_off = util.get_full_cases(motor_df,\n",
    "                                                          'CORNELL ID',\n",
    "                                                          'OFF (pre-dbs updrs)',\n",
    "                                                          'ON (pre-dbs updrs)',\n",
    "                                                          'OFF meds ON stim 6mo')\n",
    "# Load extracted features\n",
    "npy_dir = '/home/ali/RadDBS-QSM/data/npy/'\n",
    "phi_dir = '/home/ali/RadDBS-QSM/data/phi/phi/'\n",
    "roi_path = '/data/Ali/atlas/mcgill_pd_atlas/PD25-subcortical-labels.csv'\n",
    "n_rois = 6\n",
    "all_rois = False\n",
    "Phi_all, X_all, R_all, K_all, ID_all = util.load_featstruct(phi_dir,npy_dir+'X/',npy_dir+'R/',npy_dir+'K/',n_rois,1595,all_rois)\n",
    "\n",
    "ids = np.asarray(ID_all).astype(int)\n",
    "# Find overlap between scored subjects and feature extraction cases\n",
    "c_cases = np.intersect1d(np.asarray(case_id).astype(int),np.asarray(subs).astype(int))\n",
    "# Complete case indices with respect to feature matrix\n",
    "c_cases_idx = np.in1d(ids,c_cases)\n",
    "X_all_c = X_all[c_cases_idx,0:4,:]\n",
    "K_all_c = K_all[c_cases_idx,0:4,:]\n",
    "R_all_c = R_all[c_cases_idx,0:4,:]\n",
    "print(np.unique(R_all_c))\n",
    "# Re-index the scored subjects with respect to complete cases\n",
    "s_cases_idx = np.in1d(subs,ids[c_cases_idx])\n",
    "subsc = subs[s_cases_idx]\n",
    "pre_imp = pre_imp[s_cases_idx]\n",
    "post_imp = post_imp[s_cases_idx]\n",
    "pre_updrs_off = pre_updrs_off[s_cases_idx]\n",
    "per_change = post_imp\n",
    "# Reshape keys and ROIs\n",
    "if all_rois == True:\n",
    "    K_all_cu = np.empty((K_all_c.shape[0],K_all_c.shape[1],K_all_c.shape[2]+1),dtype=object)\n",
    "    K_all_cu[:,:,:-1] = K_all_c\n",
    "    K_all_cu[:,:,-1] = 'pre_updrs'\n",
    "    K = K_all_cu.reshape((K_all_cu.shape[0],K_all_cu.shape[1]*K_all_cu.shape[2]))[0]\n",
    "    R = R_all_c.reshape((R_all_c.shape[0],R_all_c.shape[1]*R_all_c.shape[2]))\n",
    "else:\n",
    "    K = K_all_c.reshape((K_all_c.shape[0],K_all_c.shape[1]*K_all_c.shape[2]))[0]\n",
    "    K = np.append(K,['pre_updrs'],0)\n",
    "    R = R_all_c.reshape((R_all_c.shape[0],R_all_c.shape[1]*R_all_c.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qsms = util.full_path('/home/ali/RadDBS-QSM/data/nii/qsm')\n",
    "# segs = util.full_path('/home/ali/RadDBS-QSM/data/nii/qsm')\n",
    "# chi = []\n",
    "# for j in np.arange(len(qsms)):\n",
    "#     data = nib.load(qsms[j]).get_fdata()\n",
    "#     if int(qsms[j][-9:-7]) == int(segs[j][-9:-7]):\n",
    "#         try:\n",
    "#             mask = nib.load(segs[j]).get_fdata()\n",
    "#             img = util.pad_to((data[:,:,~(mask==0).all((0,1))])[192:320,192:320,:],128,128,108)\n",
    "#             chi.append(img)\n",
    "#             print('Loading',qsms[j],'of shape',str(img.shape))\n",
    "#         except:\n",
    "#             print('Skipping',qsms[j])\n",
    "#             subsc = np.delete(subsc,j)\n",
    "#             per_change = np.delete(per_change,j)\n",
    "#             X_all_c = np.delete(X_all_c,j,axis=0)\n",
    "#             pre_updrs_off = np.delete(pre_updrs_off,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = int(1e4)\n",
    "scoring = 'r2'\n",
    "results_bls = np.zeros_like(per_change)\n",
    "results_ls = np.zeros_like(per_change)\n",
    "results_gls = np.zeros_like(per_change)\n",
    "curves = np.zeros((2,len(per_change),num_epochs))\n",
    "gerror = np.zeros_like(per_change)\n",
    "alphas = np.logspace(-4,-2,100)\n",
    "Ks = []\n",
    "Kstg = []\n",
    "w = []\n",
    "wg = []\n",
    "visualize_loss = 1\n",
    "num_neighbors = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(len(subsc)):\n",
    "    test_id = subsc[j]\n",
    "    test_index = subsc == test_id\n",
    "    train_index = subsc != test_id\n",
    "    X_train = X_all_c[train_index,:,:]\n",
    "    X_test = X_all_c[test_index,:,:]\n",
    "    y_train = per_change[train_index]\n",
    "    y_test = per_change[test_index]\n",
    "    # Cross validation\n",
    "    cvn = 6\n",
    "    X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                X_train,train_index,X_test,test_index,pre_updrs_off,False,False,False)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "      # Feature selection\n",
    "      sel = skf.SelectKBest(skf.r_regression,k=1800)\n",
    "      X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "      X_test_ss = sel.transform(X_test_ss0)\n",
    "      #Ks.append(sel.transform(K.reshape(1, -1)))\n",
    "      y_n = cKDTree(X0_ss).query(X_test_ss, k=1)[1]\n",
    "    \n",
    "    # LASSO\n",
    "    lasso = slm.LassoCV(max_iter=1e4,cv=cvn,n_jobs=-1,alphas=alphas)\n",
    "    est_ls = lasso.fit(X0_ss,y_train)\n",
    "    results_ls[j] = est_ls.predict(X_test_ss)\n",
    "    w.append(est_ls.coef_)\n",
    "    \n",
    "    # Gradient boosting\n",
    "    # import xgboost as xgb\n",
    "    # gsc = sms.GridSearchCV(\n",
    "    #         estimator=xgb.XGBRegressor(n_jobs=1),\n",
    "    #         param_grid={\"learning_rate\": (0.05, 0.10, 0.15),\n",
    "    #                     \"max_depth\": [1,2,3,4,5],\n",
    "    #                     \"n_estimators\": [10, 100],\n",
    "    #                     \"min_child_weight\": [1, 3, 5, 7],\n",
    "    #                     \"gamma\":[ 0.0, 0.1, 0.2],\n",
    "    #                     \"colsample_bytree\":[ 0.3, 0.4],\n",
    "    #                     \"alpha\":[1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "    #         cv=2, scoring='r2', n_jobs=-1)\n",
    "    # est_gr = gsc.fit(X0_ss0, y_train)\n",
    "    # results_bls[j] = est_gr.predict(X_test_ss0)\n",
    "    # print(gsc.best_params_)\n",
    "    # print(gsc.best_score_)\n",
    "\n",
    "    print('Lasso predicts',str(np.round(results_ls[j],3)),\n",
    "          #'and gradient boost predicts',str(np.round(results_bls[j],2)),\n",
    "          'for case',str(int(subsc[j])),'with',str(np.round(per_change[j],2)),\n",
    "          'with regularization',str(np.round(est_ls.alpha_,4)))#,'and',\n",
    "          #str(np.round(est_gr.best_params_['alpha'],2)),\n",
    "          #'and learning rate',str(est_gr.best_params_['learning_rate']))\n",
    "    # MLP\n",
    "    encoder = MLP()#in_size=X0_ss.shape[1],activation=nn.Softplus())\n",
    "    model_initial = copy.deepcopy(encoder)\n",
    "    # yt, encoder, X_trained, y_trained, X_val, y_val, train_curve, val_curve = train_model(X_all=X0_ss,\n",
    "    #                  y_all=y_train,\n",
    "    #                  model=encoder,\n",
    "    #                  X_test=X_test_ss,\n",
    "    #                  lr=1e-4,\n",
    "    #                  lr_decay=500,\n",
    "    #                  max_step=1,\n",
    "    #                  alpha=est_ls.alpha_,\n",
    "    #                  reg_type='l1',\n",
    "    #                  num_epochs=num_epochs,\n",
    "    #                  batch_size=X0_ss.shape[0]-num_neighbors,\n",
    "    #                  case_id=str(int(subsc[j])),\n",
    "    #                  num_neighbors=0,\n",
    "    #                  random_val=True,\n",
    "    #                  verbose=True,\n",
    "    #                  save_state=False)\n",
    "    # curves[0,j,:] = np.squeeze(train_curve)\n",
    "    # curves[1,j,:] = np.squeeze(val_curve)\n",
    "    yt, encoder, alphas, _ = model_cv(X_all=X0_ss,\n",
    "                     y_all=y_train,\n",
    "                     model=encoder,\n",
    "                     X_test=X_test_ss,\n",
    "                     lrs=np.asarray([1e-4]),\n",
    "                     lr_decay=None,\n",
    "                     alphas=np.logspace(-4,1,100),\n",
    "                     reg_type='l1',\n",
    "                     num_epochs=100,\n",
    "                     batch_size=X0_ss.shape[0],\n",
    "                     case_id=str(int(subsc[j])),\n",
    "                     num_neighbors=0,\n",
    "                     random_val=True,\n",
    "                     verbose=False,\n",
    "                     save_state=False,cvn=cvn)\n",
    "    # if visualize_loss > 0:\n",
    "    #   # Loss visualizations\n",
    "    #   model_final = copy.deepcopy(encoder.cpu())\n",
    "    #   criterion = torch.nn.MSELoss()\n",
    "    #   metric = loss_landscapes.metrics.Loss(criterion, X_trained.cpu(), torch.unsqueeze(y_trained.cpu(),dim=1))\n",
    "    #   steps = 40\n",
    " \n",
    "    #   plt.plot(np.linspace(1,num_epochs,num_epochs),train_curve,'-',np.linspace(1,num_epochs,num_epochs),val_curve,'--')\n",
    "    #   plt.title('Convergence')\n",
    "    #   plt.xlabel('Epoch')\n",
    "    #   plt.ylabel('Loss')\n",
    "    #   plt.legend(['Training','Validation'])\n",
    "    #   plt.show()\n",
    "    #   plt.style.use('dark_background')\n",
    "    # # if visualize_loss > 1:\n",
    "    # #   # 1D loss\n",
    "    # #   loss_data = loss_landscapes.linear_interpolation(model_initial, model_final, metric, steps ,deepcopy_model=True)\n",
    "    # #   plt.plot([(1/steps)*i for i in range(steps)], loss_data)\n",
    "    # #   plt.title('Linear Interpolation of Loss')\n",
    "    # #   plt.xlabel(r'Interpolation Coefficient $\\alpha$')\n",
    "    # #   plt.ylabel(r'Loss $L(\\theta(\\alpha))$')\n",
    "    # #   axes = plt.gca()\n",
    "    # #   plt.show()\n",
    "    # #   # 2D loss\n",
    "    # #   loss_data_fin = loss_landscapes.random_plane(model_final, metric, 10, steps, normalization='filter', deepcopy_model=True)\n",
    "    # #   plt.contour(loss_data_fin, levels=50)\n",
    "    # #   plt.title('Loss Contours around Trained Model')\n",
    "    # #   plt.show()\n",
    "    # #   # 3D loss\n",
    "    # #   fig = plt.figure()\n",
    "    # #   ax = plt.axes(projection='3d')\n",
    "    # #   X = np.array([[j for j in range(steps)] for i in range(steps)])\n",
    "    # #   Y = np.array([[i for _ in range(steps)] for i in range(steps)])\n",
    "    # #   ax.plot_surface(X, Y, loss_data_fin, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "    # #   ax.set_title('Surface Plot of Loss Landscape')\n",
    "    # #   plt.show()\n",
    "\n",
    "    results_bls[j] = yt\n",
    "    print('MLP predicts',str(np.round(yt.item(),3)),\n",
    "            'for case',str(int(subsc[j])),'with',str(np.round(per_change[j],2)),\n",
    "            'with regularization',str(alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bls[results_bls>1]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.eval_prediction(np.vstack((pre_imp,\n",
    "                               results_ls,\n",
    "                               results_bls,\n",
    "                               )),\n",
    "                               per_change,\n",
    "                               ['LCT',\n",
    "                                'Lasso',\n",
    "                                'MLP'\n",
    "                                ],(30,5))\n",
    "plt.ylim([0,2])\n",
    "plt.xlim([0,2])\n",
    "plt.style.use('default')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1,num_epochs,num_epochs),np.mean(curves[0,:,:],axis=0),np.linspace(1,num_epochs,num_epochs),np.mean(curves[1,:,:],axis=0))\n",
    "plt.fill_between(np.linspace(1,num_epochs,num_epochs), np.mean(curves[0,:,:],axis=0)-np.std(curves[0,:,:],axis=0),np.mean(curves[0,:,:],axis=0)+np.std(curves[0,:,:],axis=0),\n",
    "                 color='blue', alpha=0.2)\n",
    "plt.fill_between(np.linspace(1,num_epochs,num_epochs), np.mean(curves[1,:,:],axis=0)-np.std(curves[1,:,:],axis=0),np.mean(curves[1,:,:],axis=0)+np.std(curves[1,:,:],axis=0),\n",
    "                 color='orange', alpha=0.2)\n",
    "plt.title('Convergence')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.ylim([-0,0.5])\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()\n",
    "plt.style.use('dark_background')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
