{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from econml.sklearn_extensions.linear_model import WeightedLassoCV\n",
    "import sklearn.model_selection as sms\n",
    "import sklearn.linear_model as slm\n",
    "import sklearn.preprocessing as skp\n",
    "import sklearn.metrics as sme\n",
    "import sklearn.feature_selection as skf\n",
    "import sklearn.ensemble as ske\n",
    "import sklearn.utils as sku\n",
    "import sklearn.decomposition as skd\n",
    "import sklearn.neighbors as skn\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from celer import GroupLassoCV\n",
    "from sklearnex import patch_sklearn, config_context\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from IPython.display import HTML\n",
    "import util\n",
    "from scipy.spatial import cKDTree\n",
    "import nibabel as nib\n",
    "import os\n",
    "import pickle\n",
    "from torch import nn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('''\n",
    "<style>\n",
    ".jupyter-matplotlib {\n",
    "    background-color: #000;\n",
    "}\n",
    "\n",
    ".widget-label, .jupyter-matplotlib-header{\n",
    "    color: #fff;\n",
    "}\n",
    "\n",
    ".jupyter-button {\n",
    "    background-color: #333;\n",
    "    color: #fff;\n",
    "}\n",
    "</style>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe y should also be scaled? \n",
    "# Perhaps a transform would be more effective, or scaling implemented with consistent cross-validation\n",
    "# Different scaling methods? \n",
    "#     This seems most important for noise-sensitive models like LARS. All other use StandardScaler()\n",
    "# Transformers?\n",
    "# Model-specific scaling methods?\n",
    "#     Yes, see above\n",
    "# Common cross-validation function ✓\n",
    "#     Use built-in functions wherever possible and `utils.gridsearch_pickparams()` elsewhere\n",
    "# Quantile loss\n",
    "# RANSAC\n",
    "# Data augmentation? (Mixup)\n",
    "# Data generation? (SMOGN)\n",
    "# Combine CHH dataset ✓\n",
    "# Implement CV and test ✓\n",
    "# Print selected features ✓\n",
    "# Make magnitude templates\n",
    "# Sample weights ✓\n",
    "# Look at segmentations by error ✓ (Appears to have most difference in red nucleus, which includes surrounding (white?) matter for underperforming cases)\n",
    "# Extract features from current (1:6) eroded ROIs\n",
    "# Extract features from all ROIs\n",
    "# Plot segmentation variance against error for each case across all ROIs ✓\n",
    "# Why does excluding the subthalamic nucleus increase the correlation (r=0.5 -> r=0.6)?\n",
    "# Best performance with all ROIs: cvn=6, k=1800\n",
    "# Best performance with ROIs 0:4, excluding STN: cvn=6, k=1800\n",
    "# Should the pre-operative UPDRS be appended once or to each ROI? ✓\n",
    "# Plot histogram of features for successful and unsuccessful predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment with CHH data\n",
    "X0_gt = np.load('/home/ali/RadDBS-QSM/data/npy/old/X0_gt_chh_rois.npy')\n",
    "df = pd.read_csv('/home/ali/RadDBS-QSM/data/xlxs/updrs_iii_chh.csv')\n",
    "# Patient IDs\n",
    "subject_id = np.asarray(df[df.columns[0]])[1:]\n",
    "n_rois = 6\n",
    "# Data\n",
    "s_directory = open('/home/ali/RadDBS-QSM/data/roi/roi_list','r').read().splitlines()\n",
    "# Load\n",
    "with open('/home/ali/RadDBS-QSM/data/pickles/segs_chh', \"rb\") as fp:  \n",
    "    segs = pickle.load(fp)\n",
    "    n_cases = len(segs)\n",
    "with open('/home/ali/RadDBS-QSM/data/pickles/qsms_chh', \"rb\") as fp:  \n",
    "    qsms = pickle.load(fp)\n",
    "with open('/home/ali/RadDBS-QSM/data/phi/chh/Phi_mcl_gt_roi_chh', \"rb\") as fp:  \n",
    "    Phi_gt = pickle.load(fp)\n",
    "L = int(len(X0_gt)/n_cases)\n",
    "n_features = int(L/n_rois)\n",
    "# Only extract ROI if it is present in all cases\n",
    "seg_labels_all = segs[0]\n",
    "case_number = np.zeros_like(np.asarray(s_directory))\n",
    "for i in range(n_cases):\n",
    "    case_number[i] = float(s_directory[i][-2:])\n",
    "subject_id_corr = subject_id[np.in1d(subject_id,case_number)]\n",
    "for i in range(n_cases):\n",
    "    try:\n",
    "        print('Found ROIs',str(np.unique(segs[i])),'at segmentation directory file',s_directory[i],'for case',str(subject_id_corr[i]))\n",
    "    except:\n",
    "        print('Case',subject_id[i],'quarantined')\n",
    "pre_updrs_iii_off =  np.asarray(df[df.columns[3]][1:][np.in1d(subject_id,subject_id_corr)]).astype(float)                             \n",
    "pre_updrs_iii_on =  np.asarray(df[df.columns[4]][1:][np.in1d(subject_id,subject_id_corr)]).astype(float) \n",
    "post_updrs_iii_off =  np.asarray(df[df.columns[6]][1:][np.in1d(subject_id,subject_id_corr)]).astype(float) \n",
    "\n",
    "per_change = (pre_updrs_iii_off-post_updrs_iii_off)/pre_updrs_iii_off\n",
    "pre_updrs_off = pre_updrs_iii_off\n",
    "X_all_c = X0_gt.reshape(n_cases,n_rois,n_features)\n",
    "X_all_c = X_all_c[:,0:4,:]\n",
    "lct_change = (pre_updrs_iii_off-pre_updrs_iii_on)/pre_updrs_iii_off\n",
    "pre_imp = lct_change\n",
    "subsc = subject_id_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'r2'\n",
    "r = 1\n",
    "results_bls = np.zeros_like(per_change)\n",
    "results_ls = np.zeros(r*len(per_change))\n",
    "results_gls = np.zeros_like(per_change)\n",
    "gerror = np.zeros_like(per_change)\n",
    "alphas = np.logspace(-5,-3,100)\n",
    "Ks = []\n",
    "Kstg = []\n",
    "w = []\n",
    "wg = []\n",
    "pcases = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Js = []\n",
    "err_var = np.zeros_like(per_change)\n",
    "rerror = np.zeros_like(per_change)\n",
    "kappa = []\n",
    "c = 0\n",
    "for j in np.arange(len(subsc)):\n",
    "    test_id = subsc[j]\n",
    "    test_index = subsc == test_id\n",
    "    train_index = subsc != test_id\n",
    "    X_train = X_all_c[train_index,:,:]\n",
    "    X_test = X_all_c[test_index,:,:]\n",
    "    y_train = per_change[train_index]\n",
    "    y_test = per_change[test_index]\n",
    "\n",
    "    y_cat = y_train <= 0.3\n",
    "    idy = np.where(y_cat==1)\n",
    "    # Cross validation\n",
    "    X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                X_train,train_index,X_test,\n",
    "                                                test_index,pre_updrs_off,None,None,None,None,None,None,None,None,None,False,False,False)\n",
    "    # 10\n",
    "    cvn = 10\n",
    "    cv_scores = np.zeros((cvn,1))\n",
    "    rs = 1\n",
    "    for jj in np.arange(2,cvn):\n",
    "      # Resample to avoid stratification errors\n",
    "      while np.sum(y_cat) < cvn:\n",
    "        np.random.seed(rs)\n",
    "        idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "        X0_ss0 = np.append(X0_ss0,X0_ss0[idyr,:].reshape(1,-1),axis=0)\n",
    "        y_train = np.append(y_train,y_train[idyr])\n",
    "        y_cat = y_train <= 0.3\n",
    "        rs = rs+1\n",
    "        \n",
    "      skf_g = sms.StratifiedKFold(n_splits=jj,shuffle=True,random_state=0)\n",
    "      skf_gen = skf_g.split(X0_ss0,y_cat)\n",
    "      lasso = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "        # Feature selection\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        sel = skf.RFECV(lasso,step=1000,cv=skf_gen)\n",
    "        X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "        X0_ss = X0_sst\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "        cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "        print('LassoCV score for',jj,'is',cv_scores[jj])\n",
    "        \n",
    "    with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "      best_cv = np.argmax(cv_scores)\n",
    "\n",
    "      # Break any ties\n",
    "      if np.sum(cv_scores == best_cv) > 1:\n",
    "        cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "        for jjj in (cv_scores == cv_scores(best_cv)):\n",
    "          if jjj > 0:\n",
    "            skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "            skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "            X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "            X0_ss = X0_sst\n",
    "            lasso = slm.LassoLarsCV(max_iter=1000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "            est_ls = lasso.fit(X0_ss,y_train)\n",
    "            cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "        best_cv = np.argmax(cv_scores_tb)\n",
    "      \n",
    "      # Fit whole dataset with optimal cv\n",
    "      lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      sel = skf.RFECV(lasso,step=1000,cv=best_cv)\n",
    "      X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "      X_test_sst = sel.transform(X_test_ss0)\n",
    "      X0_ss = X0_sst\n",
    "      X_test_ss = X_test_sst\n",
    "      dx, y_n = cKDTree(X0_ss).query(X_test_ss, k=1)\n",
    "\n",
    "    # LASSO\n",
    "    with warnings.catch_warnings():\n",
    "      warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "      lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      est_ls = lasso.fit(X0_ss,y_train)\n",
    "    results_ls[c] = est_ls.predict(X_test_ss)\n",
    "    print('Lasso predicts',str(np.round(results_ls[c],4)),\n",
    "              'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv)\n",
    "  \n",
    "    c=c+1\n",
    "\n",
    "      \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.eval_prediction(np.vstack((np.repeat(pre_imp,r),\n",
    "                               results_ls,\n",
    "                               )),\n",
    "                               np.repeat(per_change,r),\n",
    "                               ['LCT',\n",
    "                                'Lasso',\n",
    "                                ],(15,5))\n",
    "plt.ylim([0,2])\n",
    "plt.xlim([0,2])\n",
    "plt.style.use('default')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
