{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as slm\n",
    "import sklearn.preprocessing as skp\n",
    "import sklearn.feature_selection as skf\n",
    "from sklearnex import patch_sklearn\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import util\n",
    "from scipy.spatial import cKDTree\n",
    "from perceptron_lasso import MLP\n",
    "from perceptron_lasso import model_cv\n",
    "import loss_landscapes.metrics\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('''\n",
    "<style>\n",
    ".jupyter-matplotlib {\n",
    "    background-color: #000;\n",
    "}\n",
    "\n",
    ".widget-label, .jupyter-matplotlib-header{\n",
    "    color: #fff;\n",
    "}\n",
    "\n",
    ".jupyter-button {\n",
    "    background-color: #333;\n",
    "    color: #fff;\n",
    "}\n",
    "</style>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe y should also be scaled? \n",
    "# Perhaps a transform would be more effective, or scaling implemented with consistent cross-validation\n",
    "# Different scaling methods? \n",
    "#     This seems most important for noise-sensitive models like LARS. All other use StandardScaler()\n",
    "# Transformers?\n",
    "# Model-specific scaling methods?\n",
    "#     Yes, see above\n",
    "# Common cross-validation function ✓\n",
    "#     Use built-in functions wherever possible and `utils.gridsearch_pickparams()` elsewhere\n",
    "# Quantile loss\n",
    "# RANSAC\n",
    "# Data augmentation? (Mixup)\n",
    "# Data generation? (SMOGN)\n",
    "# Combine CHH dataset ✓\n",
    "# Implement CV and test ✓\n",
    "# Print selected features ✓\n",
    "# Make magnitude templates\n",
    "# Sample weights ✓\n",
    "# Look at segmentations by error ✓ (Appears to have most difference in red nucleus, which includes surrounding (white?) matter for underperforming cases)\n",
    "# Extract features from current (1:6) eroded ROIs ✓\n",
    "# Test features from eroded ROIs\n",
    "# Extract features from all ROIs\n",
    "# Plot segmentation variance against error for each case across all ROIs ✓\n",
    "# Why does excluding the subthalamic nucleus increase the correlation (r=0.5 -> r=0.6)?\n",
    "# Best performance with all ROIs: cvn=6, k=1800\n",
    "# Best performance with ROIs 0:4, excluding STN: cvn=6, k=1800\n",
    "# Should the pre-operative UPDRS be appended once or to each ROI? ✓\n",
    "# Plot histogram of features for successful and unsuccessful predictions ✓\n",
    "# Check which cases are unilateral, such as 29, 44, 45, 46\n",
    "# Compile bad case list: 2, 58, 59, 64, 68, 72, 75, 77, 78, 81, 85, 89, 90\n",
    "# How many neurologists overall have been involved in the UPDRS-III scoring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get case IDs\n",
    "case_list = open('/home/ali/RadDBS-QSM/data/docs/cases_90','r')\n",
    "lines = case_list.read()\n",
    "lists = np.loadtxt(case_list.name,comments=\"#\", delimiter=\",\",unpack=False,dtype=str)\n",
    "case_id = []\n",
    "for lines in lists:     \n",
    "    case_id.append(lines[-9:-7])\n",
    "\n",
    "# Load scores\n",
    "file_dir = '/home/ali/RadDBS-QSM/data/docs/QSM anonymus- 6.22.2023-1528.csv'\n",
    "motor_df = util.filter_scores(file_dir,'pre-dbs updrs','stim','CORNELL ID')\n",
    "# Find cases with all required scores\n",
    "subs,pre_imp,post_imp,pre_updrs_off = util.get_full_cases(motor_df,\n",
    "                                                          'CORNELL ID',\n",
    "                                                          'OFF (pre-dbs updrs)',\n",
    "                                                          'ON (pre-dbs updrs)',\n",
    "                                                          'OFF meds ON stim 6mo')\n",
    "# Load extracted features\n",
    "npy_dir = '/home/ali/RadDBS-QSM/data/npy/'\n",
    "phi_dir = '/home/ali/RadDBS-QSM/data/phi/phi/'\n",
    "roi_path = '/data/Ali/atlas/mcgill_pd_atlas/PD25-subcortical-labels.csv'\n",
    "n_rois = 6\n",
    "all_rois = False\n",
    "Phi_all, X_all, R_all, K_all, ID_all = util.load_featstruct(phi_dir,npy_dir+'X/',npy_dir+'R/',npy_dir+'K/',n_rois,1595,all_rois)\n",
    "\n",
    "ids = np.asarray(ID_all).astype(int)\n",
    "# Find overlap between scored subjects and feature extraction cases\n",
    "c_cases = np.intersect1d(np.asarray(case_id).astype(int),np.asarray(subs).astype(int))\n",
    "# Complete case indices with respect to feature matrix\n",
    "c_cases_idx = np.in1d(ids,c_cases)\n",
    "X_all_c = X_all[c_cases_idx,:,:]\n",
    "K_all_c = K_all[c_cases_idx,:,:]\n",
    "R_all_c = R_all[c_cases_idx,:,:]\n",
    "print(np.unique(R_all_c))\n",
    "# Re-index the scored subjects with respect to complete cases\n",
    "s_cases_idx = np.in1d(subs,ids[c_cases_idx])\n",
    "subsc = subs[s_cases_idx]\n",
    "pre_imp = pre_imp[s_cases_idx]\n",
    "post_imp = post_imp[s_cases_idx]\n",
    "pre_updrs_off = pre_updrs_off[s_cases_idx]\n",
    "per_change = post_imp\n",
    "# Reshape keys and ROIs\n",
    "if all_rois == True:\n",
    "    K_all_cu = np.empty((K_all_c.shape[0],K_all_c.shape[1],K_all_c.shape[2]+1),dtype=object)\n",
    "    K_all_cu[:,:,:-1] = K_all_c\n",
    "    K_all_cu[:,:,-1] = 'pre_updrs'\n",
    "    K = K_all_cu.reshape((K_all_cu.shape[0],K_all_cu.shape[1]*K_all_cu.shape[2]))[0]\n",
    "    R = R_all_c.reshape((R_all_c.shape[0],R_all_c.shape[1]*R_all_c.shape[2]))\n",
    "else:\n",
    "    K = K_all_c.reshape((K_all_c.shape[0],K_all_c.shape[1]*K_all_c.shape[2]))[0]\n",
    "    K = np.append(K,['pre_updrs'],0)\n",
    "    R = R_all_c.reshape((R_all_c.shape[0],R_all_c.shape[1]*R_all_c.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = int(1e4)\n",
    "scoring = 'r2'\n",
    "results_bls = np.zeros_like(per_change)\n",
    "results_ls = np.zeros_like(per_change)\n",
    "results_gls = np.zeros_like(per_change)\n",
    "curves = np.zeros((2,len(per_change),num_epochs))\n",
    "gerror = np.zeros_like(per_change)\n",
    "alphas = np.logspace(-4,-2,10)\n",
    "Ks = []\n",
    "Kstg = []\n",
    "w = []\n",
    "wg = []\n",
    "visualize_loss = 1\n",
    "num_neighbors = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(len(subsc)):\n",
    "    test_id = subsc[j]\n",
    "    test_index = subsc == test_id\n",
    "    train_index = subsc != test_id\n",
    "    X_train = X_all_c[train_index,:,:]\n",
    "    X_test = X_all_c[test_index,:,:]\n",
    "    y_train = per_change[train_index]\n",
    "    y_test = per_change[test_index]\n",
    "    # Cross validation\n",
    "    cvn = 5\n",
    "    X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                X_train,train_index,X_test,test_index,pre_updrs_off,\n",
    "                                                True,False,False)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "      # Feature selection\n",
    "      sel = skf.SelectKBest(skf.r_regression,k=100)\n",
    "      X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "      X_test_ss = sel.transform(X_test_ss0)\n",
    "      y_n = cKDTree(X0_ss).query(X_test_ss, k=1)[1]\n",
    "    \n",
    "    # # LASSO\n",
    "    # lasso = slm.LassoCV(max_iter=1e2,cv=cvn,n_jobs=-1,alphas=alphas)\n",
    "    # est_ls = lasso.fit(X0_ss,y_train)\n",
    "    # results_ls[j] = est_ls.predict(X_test_ss)\n",
    "    # w.append(est_ls.coef_)\n",
    "    # print('Lasso predicts',str(np.round(results_ls[j],3)),\n",
    "    #   'for case',str(int(subsc[j])),'with',str(np.round(per_change[j],2)),\n",
    "    #   'with regularization',str(np.round(est_ls.alpha_,4)))\n",
    "    \n",
    "    # MLP\n",
    "    encoder = MLP(in_size=X0_ss.shape[1])\n",
    "    yt, encoder, _, alpha_js = model_cv(X_all=X0_ss,\n",
    "                     y_all=y_train,\n",
    "                     model=encoder,\n",
    "                     X_test=X_test_ss,\n",
    "                     warm_start_weights=[],\n",
    "                     early_stopping=[],\n",
    "                     tol=1e-3,\n",
    "                     lrs=np.asarray([1e-3]),\n",
    "                     lr_decay=None,\n",
    "                     alphas=alphas,\n",
    "                     reg_type='l1',\n",
    "                     thresh=None,\n",
    "                     num_epochs=int(1e3),\n",
    "                     batch_size=X0_ss.shape[0],\n",
    "                     case_id=str(int(subsc[j])),\n",
    "                     num_neighbors=0,\n",
    "                     random_val=True,\n",
    "                     verbose=1,\n",
    "                     save_state=False,cvn=cvn)\n",
    "\n",
    "    results_bls[j] = yt\n",
    "    print('MLP predicts',str(np.round(yt.item(),3)),\n",
    "            'for case',str(int(subsc[j])),'with',str(np.round(per_change[j],2)),\n",
    "            'with regularization',str(alpha_js))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bls[results_bls>1] = 1\n",
    "util.eval_prediction(np.vstack((pre_imp,\n",
    "                               results_ls,\n",
    "                               results_bls,\n",
    "                               )),\n",
    "                               per_change,\n",
    "                               ['LCT',\n",
    "                                'Lasso',\n",
    "                                'MLP'\n",
    "                                ],(30,5))\n",
    "plt.ylim([0,2])\n",
    "plt.xlim([0,2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.linspace(1,num_epochs,num_epochs),np.mean(curves[0,:,:],axis=0),np.linspace(1,num_epochs,num_epochs),np.mean(curves[1,:,:],axis=0))\n",
    "# plt.fill_between(np.linspace(1,num_epochs,num_epochs), np.mean(curves[0,:,:],axis=0)-np.std(curves[0,:,:],axis=0),np.mean(curves[0,:,:],axis=0)+np.std(curves[0,:,:],axis=0),\n",
    "#                  color='blue', alpha=0.2)\n",
    "# plt.fill_between(np.linspace(1,num_epochs,num_epochs), np.mean(curves[1,:,:],axis=0)-np.std(curves[1,:,:],axis=0),np.mean(curves[1,:,:],axis=0)+np.std(curves[1,:,:],axis=0),\n",
    "#                  color='orange', alpha=0.2)\n",
    "# plt.title('Convergence')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# #plt.ylim([-0,0.5])\n",
    "# plt.legend(['Training','Validation'])\n",
    "# plt.show()\n",
    "# plt.style.use('dark_background')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
