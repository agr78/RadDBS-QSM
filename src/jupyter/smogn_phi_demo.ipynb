{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load dependencies - third party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## load dependencies - internal\n",
    "from smogn.phi import phi\n",
    "from smogn.phi_ctrl_pts import phi_ctrl_pts\n",
    "from smogn.over_sampling import over_sampling\n",
    "\n",
    "# Create data frame for SMOGN generation\n",
    "D = pd.DataFrame(np.hstack((X_all_t,(dbs_off_meds_improvement.reshape(dbs_off_meds_improvement.__len__(),1)))))\n",
    "for col in D.columns:\n",
    "    D.rename(columns={col:str(col)},inplace=True)\n",
    "\n",
    "# Specify phi relevance values\n",
    "Rm = [[np.min(dbs_off_meds_improvement),  1, 0],  ## over-sample (\"minority\")\n",
    "    [np.median(dbs_off_meds_improvement), 0, 0],  ## under-sample (\"majority\")\n",
    "    ]\n",
    "\n",
    "# Conduct SMOGN\n",
    "print('Prior to SMOGN sampling, mean is',X_all_t.mean(),'standard deviation is',X_all_t.std())\n",
    "X_smogn = smogn.smoter(data = D, y = str(D.columns[-1]),rel_method = 'manual',rel_ctrl_pts_rg = Rm)\n",
    "\n",
    "## synthetic minority over-sampling technique for regression with gaussian noise \n",
    "def smoter(\n",
    "    \n",
    "    ## main arguments / inputs\n",
    "    data,                     ## training set (pandas dataframe)\n",
    "    y,                        ## response variable y by name (string)\n",
    "    k = 5,                    ## num of neighs for over-sampling (pos int)\n",
    "    pert = 0.02,              ## perturbation / noise percentage (pos real)\n",
    "    samp_method = \"balance\",  ## over / under sampling (\"balance\" or extreme\")\n",
    "    under_samp = True,        ## under sampling (bool)\n",
    "    drop_na_col = True,       ## auto drop columns with nan's (bool)\n",
    "    drop_na_row = True,       ## auto drop rows with nan's (bool)\n",
    "    replace = False,          ## sampling replacement (bool)\n",
    "    seed = None,              ## seed for random sampling (pos int or None)\n",
    "    \n",
    "    ## phi relevance function arguments / inputs\n",
    "    rel_thres = 0.5,          ## relevance threshold considered rare (pos real)\n",
    "    rel_method = \"auto\",      ## relevance method (\"auto\" or \"manual\")\n",
    "    rel_xtrm_type = \"both\",   ## distribution focus (\"high\", \"low\", \"both\")\n",
    "    rel_coef = 1.5,           ## coefficient for box plot (pos real)\n",
    "    rel_ctrl_pts_rg = None    ## input for \"manual\" rel method  (2d array)\n",
    "    \n",
    "    ):\n",
    "    \n",
    "    \"\"\"\n",
    "    the main function, designed to help solve the problem of imbalanced data \n",
    "    for regression, much the same as SMOTE for classification; SMOGN applies \n",
    "    the combintation of under-sampling the majority class (in the case of \n",
    "    regression, values commonly found near the mean of a normal distribution \n",
    "    in the response variable y) and over-sampling the minority class (rare \n",
    "    values in a normal distribution of y, typically found at the tails)\n",
    "    \n",
    "    procedure begins with a series of pre-processing steps, and to ensure no \n",
    "    missing values (nan's), sorts the values in the response variable y by\n",
    "    ascending order, and fits a function 'phi' to y, corresponding phi values \n",
    "    (between 0 and 1) are generated for each value in y, the phi values are \n",
    "    then used to determine if an observation is either normal or rare by the \n",
    "    threshold specified in the argument 'rel_thres' \n",
    "    \n",
    "    normal observations are placed into a majority class subset (normal bin) \n",
    "    and are under-sampled, while rare observations are placed in a seperate \n",
    "    minority class subset (rare bin) where they're over-sampled\n",
    "    \n",
    "    under-sampling is applied by a random sampling from the normal bin based \n",
    "    on a calculated percentage control by the argument 'samp_method', if the \n",
    "    specified input of 'samp_method' is \"balance\", less under-sampling (and \n",
    "    over-sampling) is conducted, and if \"extreme\" is specified more under-\n",
    "    sampling (and over-sampling is conducted)\n",
    "    \n",
    "    over-sampling is applied one of two ways, either synthetic minority over-\n",
    "    sampling technique for regression 'smoter' or 'smoter-gn' which applies a \n",
    "    similar interpolation method to 'smoter', but takes an additional step to\n",
    "    perturb the interpolated values with gaussian noise\n",
    "    \n",
    "    'smoter' is selected when the distance between a given observation and a \n",
    "    selected nearest neighbor is within the maximum threshold (half the median \n",
    "    distance of k nearest neighbors) 'smoter-gn' is selected when a given \n",
    "    observation and a selected nearest neighbor exceeds that same threshold\n",
    "    \n",
    "    both 'smoter' and 'smoter-gn' are only applied to numeric / continuous \n",
    "    features, synthetic values found in nominal / categorical features, are \n",
    "    generated by randomly selecting observed values found within their \n",
    "    respective feature\n",
    "    \n",
    "    procedure concludes by post-processing and returns a modified pandas data\n",
    "    frame containing under-sampled and over-sampled (synthetic) observations, \n",
    "    the distribution of the response variable y should more appropriately \n",
    "    reflect the minority class areas of interest in y that are under-\n",
    "    represented in the original training set\n",
    "    \n",
    "    ref:\n",
    "    \n",
    "    Branco, P., Torgo, L., Ribeiro, R. (2017).\n",
    "    SMOGN: A Pre-Processing Approach for Imbalanced Regression.\n",
    "    Proceedings of Machine Learning Research, 74:36-50.\n",
    "    http://proceedings.mlr.press/v74/branco17a/branco17a.pdf.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## pre-process missing values\n",
    "    if bool(drop_na_col) == True:\n",
    "        data = data.dropna(axis = 1)  ## drop columns with nan's\n",
    "    \n",
    "    if bool(drop_na_row) == True:\n",
    "        data = data.dropna(axis = 0)  ## drop rows with nan's\n",
    "    \n",
    "    ## quality check for missing values in dataframe\n",
    "    if data.isnull().values.any():\n",
    "        raise ValueError(\"cannot proceed: data cannot contain NaN values\")\n",
    "    \n",
    "    ## quality check for y\n",
    "    if isinstance(y, str) is False:\n",
    "        raise ValueError(\"cannot proceed: y must be a string\")\n",
    "    \n",
    "    if y in data.columns.values is False:\n",
    "        raise ValueError(\"cannot proceed: y must be an header name (string) \\\n",
    "               found in the dataframe\")\n",
    "    \n",
    "    ## quality check for k number specification\n",
    "    if k > len(data):\n",
    "        raise ValueError(\"cannot proceed: k is greater than number of \\\n",
    "               observations / rows contained in the dataframe\")\n",
    "    \n",
    "    ## quality check for perturbation\n",
    "    if pert > 1 or pert <= 0:\n",
    "        raise ValueError(\"pert must be a real number number: 0 < R < 1\")\n",
    "    \n",
    "    ## quality check for sampling method\n",
    "    if samp_method in [\"balance\", \"extreme\"] is False:\n",
    "        raise ValueError(\"samp_method must be either: 'balance' or 'extreme' \")\n",
    "    \n",
    "    ## quality check for relevance threshold parameter\n",
    "    if rel_thres == None:\n",
    "        raise ValueError(\"cannot proceed: relevance threshold required\")\n",
    "    \n",
    "    if rel_thres > 1 or rel_thres <= 0:\n",
    "        raise ValueError(\"rel_thres must be a real number number: 0 < R < 1\")\n",
    "    \n",
    "    ## store data dimensions\n",
    "    n = len(data)\n",
    "    d = len(data.columns)\n",
    "    \n",
    "    ## store original data types\n",
    "    feat_dtypes_orig = [None] * d\n",
    "    \n",
    "    for j in range(d):\n",
    "        feat_dtypes_orig[j] = data.iloc[:, j].dtype\n",
    "    \n",
    "    ## determine column position for response variable y\n",
    "    y_col = data.columns.get_loc(y)\n",
    "    \n",
    "    ## move response variable y to last column\n",
    "    if y_col < d - 1:\n",
    "        cols = list(range(d))\n",
    "        cols[y_col], cols[d - 1] = cols[d - 1], cols[y_col]\n",
    "        data = data[data.columns[cols]]\n",
    "    \n",
    "    ## store original feature headers and\n",
    "    ## encode feature headers to index position\n",
    "    feat_names = list(data.columns)\n",
    "    data.columns = range(d)\n",
    "    \n",
    "    ## sort response variable y by ascending order\n",
    "    y = pd.DataFrame(data[d - 1])\n",
    "    y_sort = y.sort_values(by = d - 1)\n",
    "    y_sort = y_sort[d - 1]\n",
    "    \n",
    "    ## -------------------------------- phi --------------------------------- ##\n",
    "    ## calculate parameters for phi relevance function\n",
    "    ## (see 'phi_ctrl_pts()' function for details)\n",
    "    phi_params = phi_ctrl_pts(\n",
    "        \n",
    "        y = y_sort,                ## y (ascending)\n",
    "        method = rel_method,       ## defaults \"auto\" \n",
    "        xtrm_type = rel_xtrm_type, ## defaults \"both\"\n",
    "        coef = rel_coef,           ## defaults 1.5\n",
    "        ctrl_pts = rel_ctrl_pts_rg ## user spec\n",
    "    )\n",
    "    \n",
    "    ## calculate the phi relevance function\n",
    "    ## (see 'phi()' function for details)\n",
    "    y_phi = phi(\n",
    "        \n",
    "        y = y_sort,                ## y (ascending)\n",
    "        ctrl_pts = phi_params      ## from 'phi_ctrl_pts()'\n",
    "    )\n",
    "    \n",
    "    ## phi relevance quality check\n",
    "    if all(i == 0 for i in y_phi):\n",
    "        raise ValueError(\"redefine phi relevance function: all points are 1\")\n",
    "    \n",
    "    if all(i == 1 for i in y_phi):\n",
    "        raise ValueError(\"redefine phi relevance function: all points are 0\")\n",
    "    ## ---------------------------------------------------------------------- ##\n",
    "    \n",
    "    ## determine bin (rare or normal) by bump classification\n",
    "    bumps = [0]\n",
    "    \n",
    "    for i in range(0, len(y_sort) - 1):\n",
    "        if ((y_phi[i] >= rel_thres and y_phi[i + 1] < rel_thres) or \n",
    "            (y_phi[i] < rel_thres and y_phi[i + 1] >= rel_thres)):\n",
    "                bumps.append(i + 1)\n",
    "    \n",
    "    bumps.append(n)\n",
    "    \n",
    "    ## number of bump classes\n",
    "    n_bumps = len(bumps) - 1\n",
    "    \n",
    "    ## determine indicies for each bump classification\n",
    "    b_index = {}\n",
    "    \n",
    "    for i in range(n_bumps):\n",
    "        b_index.update({i: y_sort[bumps[i]:bumps[i + 1]]})\n",
    "    \n",
    "    ## calculate over / under sampling percentage according to\n",
    "    ## bump class and user specified method (\"balance\" or \"extreme\")\n",
    "    b = round(n / n_bumps)\n",
    "    s_perc = []\n",
    "    scale = []\n",
    "    obj = []\n",
    "    \n",
    "    if samp_method == \"balance\":\n",
    "        for i in b_index:\n",
    "            s_perc.append(b / len(b_index[i]))\n",
    "            \n",
    "    if samp_method == \"extreme\":\n",
    "        for i in b_index:\n",
    "            scale.append(b ** 2 / len(b_index[i]))\n",
    "        scale = n_bumps * b / sum(scale)\n",
    "        \n",
    "        for i in b_index:\n",
    "            obj.append(round(b ** 2 / len(b_index[i]) * scale, 2))\n",
    "            s_perc.append(round(obj[i] / len(b_index[i]), 1))\n",
    "    \n",
    "    ## conduct over / under sampling and store modified training set\n",
    "    data_new = pd.DataFrame()\n",
    "    \n",
    "    for i in range(n_bumps):\n",
    "        \n",
    "        ## no sampling\n",
    "        if s_perc[i] == 1:\n",
    "            \n",
    "            ## simply return no sampling\n",
    "            ## results to modified training set\n",
    "            data_new = pd.concat([data.iloc[b_index[i].index], data_new])\n",
    "        \n",
    "        ## over-sampling\n",
    "        if s_perc[i] > 1:\n",
    "            \n",
    "            ## generate synthetic observations in training set\n",
    "            ## considered 'minority'\n",
    "            ## (see 'over_sampling()' function for details)\n",
    "            synth_obs = over_sampling(\n",
    "                data = data,\n",
    "                index = list(b_index[i].index),\n",
    "                perc = s_perc[i],\n",
    "                pert = pert,\n",
    "                k = k\n",
    "            )\n",
    "            \n",
    "            ## concatenate over-sampling\n",
    "            ## results to modified training set\n",
    "            data_new = pd.concat([synth_obs, data_new])\n",
    "        \n",
    "        ## under-sampling\n",
    "        if under_samp is True:\n",
    "            if s_perc[i] < 1:\n",
    "                \n",
    "                ## set random seed \n",
    "                if seed:\n",
    "                    np.random.seed(seed = seed)\n",
    "                \n",
    "                ## drop observations in training set\n",
    "                ## considered 'normal' (not 'rare')\n",
    "                omit_index = np.random.choice(\n",
    "                    a = list(b_index[i].index), \n",
    "                    size = int(s_perc[i] * len(b_index[i])),\n",
    "                    replace = replace\n",
    "                )\n",
    "                \n",
    "                omit_obs = data.drop(\n",
    "                    index = omit_index, \n",
    "                    axis = 0\n",
    "                )\n",
    "                \n",
    "                ## concatenate under-sampling\n",
    "                ## results to modified training set\n",
    "                data_new = pd.concat([omit_obs, data_new])\n",
    "    \n",
    "    ## rename feature headers to originals\n",
    "    data_new.columns = feat_names\n",
    "    \n",
    "    ## restore response variable y to original position\n",
    "    if y_col < d - 1:\n",
    "        cols = list(range(d))\n",
    "        cols[y_col], cols[d - 1] = cols[d - 1], cols[y_col]\n",
    "        data_new = data_new[data_new.columns[cols]]\n",
    "    \n",
    "    ## restore original data types\n",
    "    for j in range(d):\n",
    "        data_new.iloc[:, j] = data_new.iloc[:, j].astype(feat_dtypes_orig[j])\n",
    "    \n",
    "    ## return modified training set\n",
    "    return data_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
