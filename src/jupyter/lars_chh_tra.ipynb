{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from econml.sklearn_extensions.linear_model import WeightedLassoCV\n",
    "import sklearn.model_selection as sms\n",
    "import sklearn.linear_model as slm\n",
    "import sklearn.preprocessing as skp\n",
    "import sklearn.metrics as sme\n",
    "import sklearn.feature_selection as skf\n",
    "import sklearn.ensemble as ske\n",
    "import sklearn.utils as sku\n",
    "import sklearn.decomposition as skd\n",
    "import sklearn.neighbors as skn\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from celer import GroupLassoCV\n",
    "from sklearnex import patch_sklearn, config_context\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from IPython.display import HTML\n",
    "import util\n",
    "from scipy.spatial import cKDTree\n",
    "import nibabel as nib\n",
    "import os\n",
    "import pickle\n",
    "from torch import nn\n",
    "from rvae import VAE\n",
    "from rvae import train_model\n",
    "from adapt.instance_based import TrAdaBoostR2\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('''\n",
    "<style>\n",
    ".jupyter-matplotlib {\n",
    "    background-color: #000;\n",
    "}\n",
    "\n",
    ".widget-label, .jupyter-matplotlib-header{\n",
    "    color: #fff;\n",
    "}\n",
    "\n",
    ".jupyter-button {\n",
    "    background-color: #333;\n",
    "    color: #fff;\n",
    "}\n",
    "</style>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe y should also be scaled? \n",
    "# Perhaps a transform would be more effective, or scaling implemented with consistent cross-validation\n",
    "# Different scaling methods? \n",
    "#     This seems most important for noise-sensitive models like LARS. All other use StandardScaler()\n",
    "# Transformers?\n",
    "# Model-specific scaling methods?\n",
    "#     Yes, see above\n",
    "# Common cross-validation function ✓\n",
    "#     Use built-in functions wherever possible and `utils.gridsearch_pickparams()` elsewhere\n",
    "# Quantile loss\n",
    "# RANSAC\n",
    "# Data augmentation? (Mixup)\n",
    "# Data generation? (SMOGN)\n",
    "# Combine CHH dataset ✓\n",
    "# Implement CV and test ✓\n",
    "# Print selected features ✓\n",
    "# Make magnitude templates\n",
    "# Sample weights ✓\n",
    "# Look at segmentations by error ✓ (Appears to have most difference in red nucleus, which includes surrounding (white?) matter for underperforming cases)\n",
    "# Extract features from current (1:6) eroded ROIs\n",
    "# Extract features from all ROIs\n",
    "# Plot segmentation variance against error for each case across all ROIs ✓\n",
    "# Why does excluding the subthalamic nucleus increase the correlation (r=0.5 -> r=0.6)?\n",
    "# Best performance with all ROIs: cvn=6, k=1800\n",
    "# Best performance with ROIs 0:4, excluding STN: cvn=6, k=1800\n",
    "# Should the pre-operative UPDRS be appended once or to each ROI? ✓\n",
    "# Plot histogram of features for successful and unsuccessful predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get case IDs\n",
    "# case_list = open('/home/ali/RadDBS-QSM/data/docs/cases_90','r')\n",
    "# lines = case_list.read()\n",
    "# lists = np.loadtxt(case_list.name,comments=\"#\", delimiter=\",\",unpack=False,dtype=str)\n",
    "# case_id = []\n",
    "# for lines in lists:     \n",
    "#     case_id.append(lines[-9:-7])\n",
    "\n",
    "# # Load scores\n",
    "# file_dir = '/home/ali/RadDBS-QSM/data/docs/QSM anonymus- 6.22.2023-1528.csv'\n",
    "# motor_df = util.filter_scores(file_dir,'pre-dbs updrs','stim','CORNELL ID')\n",
    "# # Find cases with all required scores\n",
    "# subs,pre_imp,post_imp,pre_updrs_off = util.get_full_cases(motor_df,\n",
    "#                                                           'CORNELL ID',\n",
    "#                                                           'OFF (pre-dbs updrs)',\n",
    "#                                                           'ON (pre-dbs updrs)',\n",
    "#                                                           'OFF meds ON stim 6mo')\n",
    "# # Load extracted features\n",
    "# npy_dir = '/home/ali/RadDBS-QSM/data/npy/'\n",
    "# phi_dir = '/home/ali/RadDBS-QSM/data/phi/phi/'\n",
    "# roi_path = '/data/Ali/atlas/mcgill_pd_atlas/PD25-subcortical-labels.csv'\n",
    "# n_rois = 6\n",
    "# all_rois = False\n",
    "# Phi_all, X_all, R_all, K_all, ID_all = util.load_featstruct(phi_dir,npy_dir+'X/',npy_dir+'R/',npy_dir+'K/',n_rois,1595,all_rois)\n",
    "\n",
    "# ids = np.asarray(ID_all).astype(int)\n",
    "# # Find overlap between scored subjects and feature extraction cases\n",
    "# c_cases = np.intersect1d(np.asarray(case_id).astype(int),np.asarray(subs).astype(int))\n",
    "# # Complete case indices with respect to feature matrix\n",
    "# c_cases_idx = np.in1d(ids,c_cases)\n",
    "# X_all_c = X_all[c_cases_idx,:,:]\n",
    "# K_all_c = K_all[c_cases_idx,:,:]\n",
    "# R_all_c = R_all[c_cases_idx,:,:]\n",
    "# print(R_all_c)\n",
    "# # Re-index the scored subjects with respect to complete cases\n",
    "# s_cases_idx = np.in1d(subs,ids[c_cases_idx])\n",
    "# subsc = subs[s_cases_idx]\n",
    "# pre_imp = pre_imp[s_cases_idx]\n",
    "# post_imp = post_imp[s_cases_idx]\n",
    "# pre_updrs_off = pre_updrs_off[s_cases_idx]\n",
    "# per_change = post_imp\n",
    "# # Reshape keys and ROIs\n",
    "# if all_rois == True:\n",
    "#     K_all_cu = np.empty((K_all_c.shape[0],K_all_c.shape[1],K_all_c.shape[2]+1),dtype=object)\n",
    "#     K_all_cu[:,:,:-1] = K_all_c\n",
    "#     K_all_cu[:,:,-1] = 'pre_updrs'\n",
    "#     K = K_all_cu.reshape((K_all_cu.shape[0],K_all_cu.shape[1]*K_all_cu.shape[2]))[0]\n",
    "#     R = R_all_c.reshape((R_all_c.shape[0],R_all_c.shape[1]*R_all_c.shape[2]))\n",
    "# else:\n",
    "#     K = K_all_c.reshape((K_all_c.shape[0],K_all_c.shape[1]*K_all_c.shape[2]))[0]\n",
    "#     K = np.append(K,['pre_updrs'],0)\n",
    "#     R = R_all_c.reshape((R_all_c.shape[0],R_all_c.shape[1]*R_all_c.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment with CHH data\n",
    "X0_gt = np.load('/home/ali/RadDBS-QSM/data/npy/old/X0_gt_chh_rois.npy')\n",
    "df = pd.read_csv('/home/ali/RadDBS-QSM/data/xlxs/updrs_iii_chh.csv')\n",
    "# Patient IDs\n",
    "subject_id = np.asarray(df[df.columns[0]])[1:]\n",
    "n_rois = 6\n",
    "# Data\n",
    "s_directory = open('/home/ali/RadDBS-QSM/data/roi/roi_list','r').read().splitlines()\n",
    "# Load\n",
    "with open('/home/ali/RadDBS-QSM/data/pickles/segs_chh', \"rb\") as fp:  \n",
    "    segs = pickle.load(fp)\n",
    "    n_cases = len(segs)\n",
    "with open('/home/ali/RadDBS-QSM/data/pickles/qsms_chh', \"rb\") as fp:  \n",
    "    qsms = pickle.load(fp)\n",
    "with open('/home/ali/RadDBS-QSM/data/phi/chh/Phi_mcl_gt_roi_chh', \"rb\") as fp:  \n",
    "    Phi_gt = pickle.load(fp)\n",
    "L = int(len(X0_gt)/n_cases)\n",
    "n_features = int(L/n_rois)\n",
    "# Only extract ROI if it is present in all cases\n",
    "seg_labels_all = segs[0]\n",
    "case_number = np.zeros_like(np.asarray(s_directory))\n",
    "for i in range(n_cases):\n",
    "    case_number[i] = float(s_directory[i][-2:])\n",
    "subject_id_corr = subject_id[np.in1d(subject_id,case_number)]\n",
    "for i in range(n_cases):\n",
    "    try:\n",
    "        print('Found ROIs',str(np.unique(segs[i])),'at segmentation directory file',s_directory[i],'for case',str(subject_id_corr[i]))\n",
    "    except:\n",
    "        print('Case',subject_id[i],'quarantined')\n",
    "pre_updrs_iii_off =  np.asarray(df[df.columns[3]][1:][np.in1d(subject_id,subject_id_corr)]).astype(float)                             \n",
    "pre_updrs_iii_on =  np.asarray(df[df.columns[4]][1:][np.in1d(subject_id,subject_id_corr)]).astype(float) \n",
    "post_updrs_iii_off =  np.asarray(df[df.columns[6]][1:][np.in1d(subject_id,subject_id_corr)]).astype(float) \n",
    "\n",
    "per_change = (pre_updrs_iii_off-post_updrs_iii_off)/pre_updrs_iii_off\n",
    "pre_updrs_off = pre_updrs_iii_off\n",
    "X_all_c = X0_gt.reshape(n_cases,n_rois,n_features)\n",
    "X_all_c = X_all_c[:,0:4,:]\n",
    "lct_change = (pre_updrs_iii_off-pre_updrs_iii_on)/pre_updrs_iii_off\n",
    "pre_imp = lct_change\n",
    "subsc = subject_id_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'r2'\n",
    "results_bls = np.zeros_like(per_change)\n",
    "results_ls = np.zeros_like(per_change)\n",
    "results_gls = np.zeros_like(per_change)\n",
    "gerror = np.zeros_like(per_change)\n",
    "alphas = np.logspace(-5,-3,100)\n",
    "Ks = []\n",
    "Kstg = []\n",
    "w = []\n",
    "wg = []\n",
    "pcases = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Js = []\n",
    "err_var = np.zeros_like(per_change)\n",
    "r = np.zeros_like(per_change)\n",
    "rerror = np.zeros_like(per_change)\n",
    "kappa = []\n",
    "for j in np.arange(len(subsc)):\n",
    "    test_id = subsc[j]\n",
    "    test_index = subsc == test_id\n",
    "    train_index = subsc != test_id\n",
    "    X_train = X_all_c[train_index,:,:]\n",
    "    X_test = X_all_c[test_index,:,:]\n",
    "    y_train = per_change[train_index]\n",
    "    y_test = per_change[test_index]\n",
    "\n",
    "    idy = y_train[y_train<=0.3]\n",
    "    \n",
    "    # Cross validation\n",
    "    X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                X_train,train_index,X_test,test_index,pre_updrs_off,False,False,False)\n",
    "    cvn = len(X0_ss0-1)\n",
    "    lasso = slm.LassoLarsCV(max_iter=1000,cv=cvn,normalize=False,eps=0.1,n_jobs=1)\n",
    "\n",
    "    with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "      # Feature selection\n",
    "      warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "      sel = skf.SelectKBest(skf.r_regression,k=2925)\n",
    "      X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "      X_test_sst = sel.transform(X_test_ss0)\n",
    "      #gel = skf.RFECV(lasso,verbose=0,cv=cvn,step=100,n_jobs=-1)\n",
    "      X0_ss = X0_sst#gel.fit_transform(X0_sst,y_train)\n",
    "      kappa.append(np.linalg.cond(X0_ss0))\n",
    "      X_test_ss = X_test_sst#gel.transform(X_test_sst)\n",
    "     #Ks.append(sel.transform(K.reshape(1, -1)))\n",
    "      dx0, y_n0 = cKDTree(X0_ss).query(X_test_ss, k=1)\n",
    "\n",
    "    # LASSO\n",
    "    with warnings.catch_warnings():\n",
    "      warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "      lasso = slm.LassoLarsCV(max_iter=1000,cv=cvn,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      est_ls = lasso.fit(X0_ss,y_train)\n",
    "    # Reconstruct nearest neighbor\n",
    "    r[j] = est_ls.predict(X0_ss[y_n,:])\n",
    "    err_var[j] = np.mean(abs(est_ls.predict(X0_ss)-y_train))\n",
    "    rerror[j] = np.abs(r[j]-y_train[y_n])\n",
    "    #s.append(est_ls.score(X0_ss,y_train))\n",
    "    results_ls[j] = est_ls.predict(X_test_ss)\n",
    "    # If reconstruction error is too high, use nearest neighbor\n",
    "    if rerror[j] > 0.5:\n",
    "        print('Using nearest neighbor after high reconstruction error and CORAL failure')\n",
    "        y_n = y_n0\n",
    "        results_ls[j] = y_train[y_n]\n",
    "\n",
    "    print('Lasso predicts',str(np.round(results_ls[j],2)),\n",
    "              'for case with',str(np.round(per_change[j],2)),\n",
    "             # 'with regularization',str(est_ls.alpha_),\n",
    "              'with reconstruction error',str(np.round(rerror[j],9)),\n",
    "              'maximum error',str(np.round(err_var[j],9)),\n",
    "              'and neighbor',str(np.round(y_train[y_n],2).item()),\n",
    "              'and condition number',str(kappa[j]))\n",
    "    gerror[j] = (abs(results_ls[j]-y_test))\n",
    "        \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.eval_prediction(np.vstack((pre_imp,\n",
    "                               results_ls,\n",
    "                               )),\n",
    "                               per_change,\n",
    "                               ['LCT',\n",
    "                                'Lasso',\n",
    "                                ],(30,5))\n",
    "plt.ylim([0,2])\n",
    "plt.xlim([0,2])\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(3,2,sharex=True,sharey=True)\n",
    "# plt.style.use('dark_background')\n",
    "# plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "# ax[0,0].scatter(V[:,0]/1000,gerror)\n",
    "# ax[0,0].set_title('Right red nucleus')\n",
    "# ax[0,1].scatter(V[:,1]/1000,gerror)\n",
    "# ax[0,1].set_title('Left red nucleus')\n",
    "# ax[1,0].scatter(V[:,2]/1000,gerror)\n",
    "# ax[1,0].set_title('Right substantia nigra')\n",
    "# ax[1,1].scatter(V[:,3]/1000,gerror)\n",
    "# ax[1,1].set_title('Left substantia nigra')\n",
    "# ax[2,0].scatter(V[:,4]/1000,gerror)\n",
    "# ax[2,0].set_title('Right subthalamic nuclei')\n",
    "# ax[2,1].scatter(V[:,5]/1000,gerror)\n",
    "# ax[2,1].set_title('Left subthalamic nuclei')\n",
    "# plt.setp(ax[-1, :], xlabel='Variance');\n",
    "# plt.setp(ax[:, 0], ylabel='Error');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
