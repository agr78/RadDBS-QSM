{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import sklearn.model_selection as sms\n",
    "import sklearn.linear_model as slm\n",
    "import sklearn.preprocessing as skp\n",
    "import sklearn.feature_selection as skf\n",
    "from sklearnex import patch_sklearn, config_context\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import util\n",
    "from scipy.spatial import cKDTree\n",
    "import copy\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For samples $i \\in N$ and features $j \\in p$ with targets $y_i$, the optimal features $\\beta_{ij}$ can be determined by minimizing a loss function $l(y,f(X))$. The matrix $X$ is formed by standardizing the feature vectors such that $\\frac{1}{N}\\sum^N_{i=1}x_{ij}=0$ and $\\frac{1}{N}\\sum^N_{i=1}x_{ij}^2=1$. Then, solving for the optimal weights (least absolute shrinkage operator, LASSO) amounts to minimizing the negative likelihood of observing $(y_i,X_i)$, or\n",
    "$$\\hat{\\beta} = \\mathrm{argmin}_{\\beta} \\sum^N_{i=1} l(y_i,f(X_i)) + \\lambda \\sum^p_{j=1}\\left|\\beta_j\\right|$$\n",
    "Gradient boosting models the prediction as a weighted sum of base learners $h_i(x_i)$ such that $f(X)=\\beta_0+h_1(x_1)+...+h_p(x_p)$. The optimal learner combination is\n",
    "$$\\hat{f}^m(X) = \\hat{f}^{m-1}(X)+\\nu \\cdot \\hat{h}_{j^*}^m(x_{j^*}) \\ \\ \\ \\mathrm{s.t.} \\ \\ \\ j^* = \\mathrm{argmin}_{1 \\leq j \\leq p} \\sum^N_{i=1}\\left(\\left(-\\frac{\\partial l(y_i,f(X_i))}{\\partial f}\\right) \\bigg|_{f=f^{m-1}(X_i)}-\\hat{h}_j^m(x_{ij})\\right)^2$$\n",
    "Below, feature vectors $x$ and labels $y$ are loaded and $X \\sim \\mathcal{N}(0,1)$ is generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated arrays\n",
      "Created feature matrix\n",
      "Created ROI matrix\n",
      "Created feature label matrix\n",
      "['Left red nucleus' 'Left substantia nigra' 'Left subthalamic nucleus'\n",
      " 'Right Substantia nigra' 'Right red nucleus' 'Right subthalamic nucleus']\n"
     ]
    }
   ],
   "source": [
    "# Get case IDs\n",
    "case_list = open('/home/ali/RadDBS-QSM/data/docs/cases_90','r')\n",
    "lines = case_list.read()\n",
    "lists = np.loadtxt(case_list.name,comments=\"#\", delimiter=\",\",unpack=False,dtype=str)\n",
    "case_id = []\n",
    "for lines in lists:     \n",
    "    case_id.append(lines[-9:-7])\n",
    "\n",
    "# Load scores\n",
    "file_dir = '/home/ali/RadDBS-QSM/data/docs/QSM anonymus- 6.22.2023-1528.csv'\n",
    "motor_df = util.filter_scores(file_dir,'pre-dbs updrs','stim','CORNELL ID')\n",
    "# Find cases with all required scores\n",
    "subs,pre_imp,post_imp,pre_updrs_off = util.get_full_cases(motor_df,\n",
    "                                                          'CORNELL ID',\n",
    "                                                          'OFF (pre-dbs updrs)',\n",
    "                                                          'ON (pre-dbs updrs)',\n",
    "                                                          'OFF meds ON stim 6mo')\n",
    "# Load extracted features\n",
    "npy_dir = '/home/ali/RadDBS-QSM/data/npy/'\n",
    "phi_dir = '/home/ali/RadDBS-QSM/data/phi/phi/'\n",
    "roi_path = '/data/Ali/atlas/mcgill_pd_atlas/PD25-subcortical-labels.csv'\n",
    "n_rois = 6\n",
    "all_rois = False\n",
    "Phi_all, X_all, R_all, K_all, ID_all = util.load_featstruct(phi_dir,npy_dir+'X/',npy_dir+'R/',npy_dir+'K/',n_rois,1595,all_rois)\n",
    "ids = np.asarray(ID_all).astype(int)\n",
    "# Find overlap between scored subjects and feature extraction cases\n",
    "c_cases = np.intersect1d(np.asarray(case_id).astype(int),np.asarray(subs).astype(int))\n",
    "# Complete case indices with respect to feature matrix\n",
    "c_cases_idx = np.in1d(ids,c_cases)\n",
    "\n",
    "X_all_c, K, R, subsc, pre_imp, pre_updrs_off, per_change = util.re_index(X_all,K_all,R_all,c_cases_idx,subs,ids,all_rois,pre_imp,pre_updrs_off,post_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the low-dimension condition \n",
    "$$p < N$$\n",
    "Of the positive cone condition is imposed to ensure gradient boosting and LASSO estimators converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X_all_c.shape[0]-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S$ be a diagonal $p \\times p$ matrix with elements $s_{ij}=s_{ii} \\in \\{-1,1\\}$ and $X$ remains the feature matrix for the dataset, $N \\times p$. The the positive cone condition is met if \n",
    "$$\\left(S'X'XS\\right)^{-1}\\mathbb{I}_{p \\times 1}$$\n",
    "For all subsets of $X$ and possible combinations of $S$. A more tractable form is the diagonal dominance condition\n",
    "$$\\left|M_{jj}\\right| \\geq \\sum_{i \\neq j}\\left|M_{ij}\\right|$$\n",
    "Here, $M$ is the inverse covariance matrix of $X$, $M = (X'X)^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso predicts 0.63 and gradient boost predicts 0.64\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.6\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.62\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.63\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.67\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.65\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.59\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.57\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.61\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.64\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.65\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.61\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.64\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.67\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.62\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.62\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.64\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.64\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.56\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.65\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.59\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.63\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.61\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.58\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.59\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.62\n",
      "Lasso predicts 0.61 and gradient boost predicts 0.59\n",
      "Lasso predicts 0.6 and gradient boost predicts 0.6\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.67\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.62\n",
      "Lasso predicts 0.65 and gradient boost predicts 0.64\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.66\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.67\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.64\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.67\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.63\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.64\n",
      "Lasso predicts 0.5 and gradient boost predicts 0.6\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.63\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.67\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.55\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.62\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.68\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.65\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.64\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.56\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.61\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "scoring = 'r2'\n",
    "results_bls = np.zeros_like(per_change)\n",
    "results_ls = np.zeros_like(per_change)\n",
    "# Train\n",
    "for j in np.arange(len(subsc)):\n",
    "    test_id = subsc[j]\n",
    "    test_index = subsc == test_id\n",
    "    train_index = subsc != test_id\n",
    "    X_train = X_all_c[train_index,:,:]\n",
    "    X_test = X_all_c[test_index,:,:]\n",
    "    y_train = per_change[train_index]\n",
    "    y_test = per_change[test_index]\n",
    "    # Cross validation\n",
    "    cvn = 6\n",
    "    X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                X_train,train_index,X_test,test_index,pre_updrs_off,False)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "      # Feature selection\n",
    "      sel = skf.SelectKBest(skf.r_regression,k=p)\n",
    "      X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "      X_test_ss = sel.transform(X_test_ss0)\n",
    "      y_n = cKDTree(X0_ss).query(X_test_ss, k=1)[1]\n",
    "    \n",
    "    # LASSO\n",
    "    lasso = slm.Lasso(max_iter=1e6,alpha=1e-1)\n",
    "    est_ls = lasso.fit(X0_ss,y_train)\n",
    "    results_ls[j] = est_ls.predict(X_test_ss)\n",
    "\n",
    "    # Gradient boosting\n",
    "    gsc = sms.GridSearchCV(\n",
    "            estimator=GradientBoostingRegressor(validation_fraction=0),\n",
    "            param_grid={\"learning_rate\": [1e-2],\n",
    "                        \"max_depth\": [1],\n",
    "                        \"n_estimators\": [p]},\n",
    "                        cv=cvn, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    est_gr = gsc.fit(X0_ss, y_train)\n",
    "    results_bls[j] = est_gr.predict(X_test_ss)\n",
    "\n",
    "    # Training status\n",
    "    print('Lasso predicts',str(np.round(results_ls[j],2)),\n",
    "          'and gradient boost predicts',str(np.round(results_bls[j],2)))#,'for case',str(int(subsc[j])),'with',str(np.round(per_change[j],2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error between the LASSO estimator $f(X,\\hat{\\beta})$ and the gradient boosting regressor $\\hat{f}^{m}(X)$ is $\\mathcal{O} \\sim 10^{-3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001068049030037756"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((results_ls-results_bls)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
