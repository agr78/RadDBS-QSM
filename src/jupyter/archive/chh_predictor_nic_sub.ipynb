{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import nibabel as nib\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score, roc_curve, auc\n",
    "import SimpleITK as sitk\n",
    "import six\n",
    "from radiomics import featureextractor \n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearnex import patch_sklearn, config_context\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn import preprocessing as skp\n",
    "from sklearn import model_selection as sms\n",
    "from sklearn import feature_selection as skf\n",
    "from sklearn import linear_model as slm\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from IPython.display import HTML\n",
    "import util as util\n",
    "import nibabel as nib\n",
    "import os\n",
    "import pickle\n",
    "from torch import nn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "def remove_keymap_conflicts(new_keys_set):\n",
    "    for prop in plt.rcParams:\n",
    "        if prop.startswith('keymap.'):\n",
    "            keys = plt.rcParams[prop]\n",
    "            remove_list = set(keys) & new_keys_set\n",
    "            for key in remove_list:\n",
    "                keys.remove(key)\n",
    "\n",
    "def multi_slice_viewer(volume):\n",
    "    remove_keymap_conflicts({'j', 'k'})\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.volume = volume\n",
    "    ax.index = volume.shape[0]//2\n",
    "    ax.imshow(volume[ax.index])\n",
    "    fig.canvas.mpl_connect('key_press_event', process_key)\n",
    "\n",
    "def process_key(event):\n",
    "    fig = event.canvas.figure\n",
    "    ax = fig.axes[0]\n",
    "    if event.key == 'j':\n",
    "        previous_slice(ax)\n",
    "    elif event.key == 'k':\n",
    "        next_slice(ax)\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "\n",
    "def previous_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index-1) % volume.shape[0] \n",
    "    ax.images[0].set_array(volume[ax.index])\n",
    "\n",
    "def next_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index+1) % volume.shape[0]\n",
    "    ax.images[0].set_array(volume[ax.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set window level\n",
    "level = 0\n",
    "window = 500\n",
    "m1=level-window/2\n",
    "m2=level+window/2\n",
    "visualize = 1\n",
    "# Load data\n",
    "nrows = 256\n",
    "ncols = 256\n",
    "nslices = 160\n",
    "segs = []\n",
    "qsms = []\n",
    "laros = []\n",
    "voxel_sizes = []\n",
    "trackers = []\n",
    "q_directory = '/home/ali/RadDBS-QSM/data/nii/chh/orig/qsm/'\n",
    "q_directory = os.listdir(q_directory)\n",
    "q_directory = sorted(q_directory)\n",
    "qu_directory = '/home/ali/RadDBS-QSM/data/nii/chh/lr_roi_ft'\n",
    "qu_directory = os.listdir(qu_directory)\n",
    "qu_directory = sorted(qu_directory)\n",
    "s_directory = '/home/ali/RadDBS-QSM/data/nii/chh/orig/seg/'\n",
    "s_directory = os.listdir(s_directory)\n",
    "s_directory = sorted(s_directory)\n",
    "m_directory = '/home/ali/RadDBS-QSM/data/nii/chh/masks'\n",
    "m_directory = os.listdir(m_directory)\n",
    "m_directory = sorted(m_directory)\n",
    "case_list = []\n",
    "d_count = 0\n",
    "if visualize == 1:\n",
    "    for filename in q_directory:\n",
    "    \n",
    "        seg_filename = s_directory[d_count]\n",
    "        laro_filename = qu_directory[d_count]\n",
    "        mask_filename = m_directory[d_count]\n",
    "        seg = nib.load('/home/ali/RadDBS-QSM/data/nii/chh/orig/seg/'+seg_filename)\n",
    "        mask = nib.load('/home/ali/RadDBS-QSM/data/nii/chh/masks/'+mask_filename)\n",
    "        voxel_size = seg.header['pixdim'][0:3]\n",
    "        voxel_sizes.append(voxel_size)\n",
    "        segs.append(seg.get_fdata()[:nrows,:ncols,:nslices])\n",
    "        qsm = nib.load('/home/ali/RadDBS-QSM/data/nii/chh/orig/qsm/'+filename)\n",
    "        qsms.append(qsm.get_fdata()[:nrows,:ncols,:nslices])\n",
    "\n",
    "        laro = nib.load('/home/ali/RadDBS-QSM/data/nii/chh/lr_roi_ft/'+laro_filename)\n",
    "        laros.append(1000*laro.get_fdata()[:nrows,:ncols,:nslices])\n",
    "        print('Appending arrays with segmentation',seg_filename,'QSM,',filename,\n",
    "              'LARO,',laro_filename,'and mask',mask_filename)\n",
    "        case_list.append(filename)\n",
    "        n_cases = len(segs)\n",
    "        d_count = d_count+1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/ali/RadDBS-QSM/data/xlxs/updrs_iii_chh.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patient IDs\n",
    "subject_id = np.asarray(df[df.columns[0]])[1:]\n",
    "\n",
    "# Only extract ROI if it is present in all cases\n",
    "seg_labels_all = segs[0]\n",
    "case_number = np.zeros_like(np.asarray(s_directory))\n",
    "for i in range(n_cases):\n",
    "    case_number[i] = float(s_directory[i][:2])\n",
    "subject_id_corr = subject_id[np.in1d(subject_id,case_number)]\n",
    "for i in np.arange(n_cases):\n",
    "    try:\n",
    "        print('Found ROIs',str(np.unique(segs[i])),'at segmentation directory file',s_directory[i],'for case',str(subject_id_corr[i]))\n",
    "    except:\n",
    "        print('Case',subject_id[i],'quarantined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_updrs_iii_off =  np.asarray(df[df.columns[3]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])                                \n",
    "pre_updrs_iii_on =  np.asarray(df[df.columns[4]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])\n",
    "post_updrs_iii_off =  np.asarray(df[df.columns[6]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])\n",
    "\n",
    "per_change = (np.asarray(pre_updrs_iii_off).astype(float)-np.asarray(post_updrs_iii_off).astype(float))/(np.asarray(pre_updrs_iii_off).astype(float))\n",
    "lct_change = (np.asarray(pre_updrs_iii_off).astype(float)-(np.asarray(pre_updrs_iii_on)).astype(float))/(np.asarray(pre_updrs_iii_off).astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"radiomics\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "reextract = 0\n",
    "\n",
    "# Assume all voxel sizes are identical\n",
    "voxel_size = (0.9,0.9,0.9)\n",
    "if reextract == 1:\n",
    "    # Generate feature structure Phi from all ROIs and all cases\n",
    "    extractor = featureextractor.RadiomicsFeatureExtractor()\n",
    "    extractor.enableAllFeatures()\n",
    "    extractor.enableAllImageTypes()\n",
    "    extractor.enableFeatureClassByName('shape2D',enabled = False)\n",
    "\n",
    "    seg_labels_all = np.unique(np.asarray(segs))\n",
    "    Phi_gt = []\n",
    "    Phi_vd = []\n",
    "    Phi_lr = []\n",
    "    seg_labels = []\n",
    "    reextract = 0\n",
    "    x_row_gt = []\n",
    "    x_row_lr = []\n",
    "\n",
    "    keylib = []\n",
    "    roilib = []\n",
    "    loop_count = 1\n",
    "    n_rois = seg_labels_all[seg_labels_all>0].__len__()\n",
    "    roi_names = np.asarray(['Background','Right substantia nigra','Right subthalamic nucleus',\n",
    "                            'Left subthalamic nucleus', 'Left substantia nigra', 'Right dentate nucleus', 'Left dentate nucleus'])\n",
    "    for i in np.arange(subject_id_corr.__len__()):\n",
    "        seg_sitk = sitk.GetImageFromArray(segs[i])\n",
    "        seg_sitk.SetSpacing(voxel_size)\n",
    "        qsm_sitk_gt = sitk.GetImageFromArray(qsms[i])\n",
    "        qsm_sitk_gt.SetSpacing(voxel_size)\n",
    "        qsm_sitk_lr = sitk.GetImageFromArray(laros[i])\n",
    "        qsm_sitk_lr.SetSpacing(voxel_size)\n",
    "        # Index back since subject 12 is missing ROIs\n",
    "        for j in seg_labels_all:\n",
    "            if j>0:\n",
    "                fv_count = 0\n",
    "                featureVector_gt = extractor.execute(qsm_sitk_gt,seg_sitk,label=int(j));\n",
    "                featureVector_lr = extractor.execute(qsm_sitk_lr,seg_sitk,label=int(j));\n",
    "                Phi_gt.append(featureVector_gt)\n",
    "                Phi_lr.append(featureVector_lr)\n",
    "                for key, value in six.iteritems(featureVector_gt):\n",
    "                    if 'diagnostic' in key:\n",
    "                        next\n",
    "                    else:\n",
    "                        x_row_gt.append(featureVector_gt[key])\n",
    "                        x_row_lr.append(featureVector_lr[key])\n",
    "                        fv_count = fv_count+1\n",
    "                        keylib.append(key)\n",
    "                        roilib.append(roi_names[int(j)])\n",
    "                x_row_gt.append(pre_updrs_iii_off[i])\n",
    "                x_row_lr.append(pre_updrs_iii_off[i])\n",
    "                fv_count = fv_count+1\n",
    "        print('Extracting features for subject',subject_id_corr[i],'and appending feature matrix with vector of length',fv_count,'with UPDRS score',pre_updrs_iii_off[i])\n",
    "                \n",
    "    X0_gt = np.array(x_row_gt)\n",
    "    X0_lr = np.array(x_row_lr)\n",
    "    np.save('/home/ali/RadDBS-QSM/data/npy/rp/X0_gt_chh_rois_rp.npy',X0_gt)\n",
    "    np.save('/home/ali/RadDBS-QSM/data/npy/rp/X0_lr_chh_rois_rp.npy',X0_lr)\n",
    "\n",
    "    K = np.asarray(keylib)\n",
    "    R = np.asarray(roi_names)\n",
    "    np.save('/home/ali/RadDBS-QSM/data/npy/rp/K_chh_rp.npy',K)\n",
    "    np.save('/home/ali/RadDBS-QSM/data/npy/rp/R_chh_rp.npy',R)\n",
    "\n",
    "    print('Saving ground truth feature vector')\n",
    "    with open('/home/ali/RadDBS-QSM/data/npy/rp/Phi_mcl_gt_roi_chh_rp', 'wb') as fp:  \n",
    "        pickle.dump(Phi_gt, fp)\n",
    "    \n",
    "    print('Saving undersampled feature vector')\n",
    "    with open('/home/ali/RadDBS-QSM/data/npy/rp/Phi_mcl_lr_roi_chh_rp', 'wb') as fp:  \n",
    "        pickle.dump(Phi_lr, fp)\n",
    "\n",
    "else:\n",
    "    X0_gt = np.load('/home/ali/RadDBS-QSM/data/npy/rp/X0_gt_chh_rois_rp.npy')\n",
    "    X0_lr = np.load('/home/ali/RadDBS-QSM/data/npy/rp/X0_lr_chh_rois_rp.npy')\n",
    "    K = np.load('/home/ali/RadDBS-QSM/data/npy/rp/K_chh_rp.npy')\n",
    "    R = np.load('/home/ali/RadDBS-QSM/data/npy/rp/R_chh_rp.npy')\n",
    "    n_rois = R.shape[0]-1\n",
    "    with open('/home/ali/RadDBS-QSM/data/npy/rp/Phi_mcl_gt_roi_chh_rp', \"rb\") as fp:  \n",
    "        Phi_gt = pickle.load(fp)\n",
    "    \n",
    "    with open('/home/ali/RadDBS-QSM/data/npy/rp/Phi_mcl_lr_roi_chh_rp', \"rb\") as fp:  \n",
    "        Phi_lr = pickle.load(fp)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1596\n",
    "n_rois = 6\n",
    "X_all_c = X0_gt.reshape(n_cases,n_rois,n_features)[:,0:4,:]\n",
    "K_all_c = K.reshape(n_cases,n_rois,n_features-1)[:,0:4,:]\n",
    "K_all_c = np.char.add(K_all_c[0,:,:].reshape(-1,1),' ')\n",
    "R_all_c = np.repeat(R[1:5],n_features-1)\n",
    "K_all_c = np.char.add(np.squeeze(K_all_c),np.squeeze(R_all_c))\n",
    "K_all_c = np.append(K_all_c,['pre updrs']*5)\n",
    "R_all_c = np.append(R_all_c,['pre updrs']*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cases = len(per_change)\n",
    "n_roisc = Phi_gt.__len__()/n_cases\n",
    "L = int(len(X0_gt)/n_cases)\n",
    "n_features = int(L/n_rois)\n",
    "subsc = subject_id_corr\n",
    "pre_updrs_off = pre_updrs_iii_off\n",
    "results_ls_aug = np.zeros_like(per_change)\n",
    "\n",
    "results_lgr_aug = np.zeros_like(per_change)\n",
    "results_lgrp_aug = np.zeros_like(per_change)\n",
    "results_ls = np.zeros_like(per_change)\n",
    "results_lgr = np.zeros_like(per_change)\n",
    "results_lgrp = np.zeros_like(per_change)\n",
    "r = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise compensated LASSO\n",
    "retrain = 1\n",
    "K_nz_nc = []\n",
    "K_nz_nc_lr = []\n",
    "#R_nz_cd = []\n",
    "if retrain == 1:\n",
    "\n",
    "  aug = True\n",
    "  K_nz = []\n",
    "  c = 0\n",
    "\n",
    "  for j in np.arange(len(subsc)):\n",
    "      test_id = subsc[j]\n",
    "      test_index = subsc == test_id\n",
    "      train_index = subsc != test_id\n",
    "      X_train = X_all_c[train_index,:,:]\n",
    "      X_test = X_all_c[test_index,:,:]\n",
    "      y_train = per_change[train_index]\n",
    "      y_test = per_change[test_index]\n",
    "\n",
    "      y_cat = y_train <= 0.3\n",
    "      idy = np.where(y_cat==1)\n",
    "      # Cross validation\n",
    "                                            \n",
    "      X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                  X_train,train_index,X_test,\n",
    "                                                  test_index,pre_updrs_off,None,None,None,None,None,None,None,None,None,False,False,False)\n",
    "      cvn = 5\n",
    "      cv_scores = np.zeros((cvn,1))\n",
    "      cv_lgr_scores = np.zeros((cvn,1))\n",
    "      rs = 1\n",
    "      rcfs = 1000\n",
    "      (mu, sigma) = stats.norm.fit(y_train)\n",
    "      kappa = stats.skew(y_train)\n",
    "      print('Label distribution of:',mu,sigma,kappa)\n",
    "      for jj in np.arange(2,cvn):\n",
    "        # Resample to avoid stratification errors\n",
    "        while np.sum(y_cat) < cvn:\n",
    "          np.random.seed(rs)\n",
    "          idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "          X0_ss0 = np.append(X0_ss0,X0_ss0[idyr,:].reshape(1,-1),axis=0)\n",
    "          y_train = np.append(y_train,y_train[idyr])\n",
    "          y_cat = y_train <= 0.3\n",
    "          rs = rs+1\n",
    "          print('Resampled to size',y_train.shape)\n",
    "        if aug == True:\n",
    "          y_train_n = y_train+(1.96*sigma)*np.random.normal(0,1,1)\n",
    "          y_train = np.hstack((y_train,y_train_n))\n",
    "          y_cat = y_train <= 0.3\n",
    "          X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "      \n",
    "      for jj in np.arange(2,cvn):\n",
    "        skf_g = sms.StratifiedKFold(n_splits=jj,shuffle=True,random_state=0)\n",
    "        skf_gen = skf_g.split(X0_ss0,y_cat)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None,penalty='l1',solver='liblinear',random_state=0)\n",
    "        with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "          # Feature selection\n",
    "          warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "          sel = skf.RFECV(lasso,step=rcfs,cv=skf_gen,n_jobs=1)\n",
    "          # Stratifies classifiers automatically\n",
    "          sel_lr = skf.RFECV(lgr,step=rcfs,cv=jj,n_jobs=1)\n",
    "          X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "          X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "          est_ls = lasso.fit(X0_ss,y_train)\n",
    "          est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "          cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "          cv_lgr_scores[jj] = est_lgr.score(X0_ssl,y_cat)\n",
    "          print('LassoCV score for',jj,'is',cv_scores[jj],'from dataset of size',X0_ss.shape)\n",
    "          \n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "        best_cv = np.argmax(cv_scores)\n",
    "        best_cv_lgr = np.argmax(cv_lgr_scores)\n",
    "\n",
    "        # Break any ties\n",
    "        if np.sum(cv_scores == best_cv) > 1:\n",
    "          cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "          cv_lgr_scores_tb = np.zeros((np.sum(cv_scores == best_cv_lgr),1))\n",
    "          for jjj in (cv_scores == cv_scores[best_cv]):\n",
    "            if jjj > 0:\n",
    "              print('Breaking tie')\n",
    "              skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "              skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "              X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "              X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "              lasso = slm.LassoLarsCV(max_iter=1000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "              lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None,penalty='l1',solver='liblinear',random_state=1)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "              cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "              cv_lgr_scores_tb[jj] = est_lgr.score(sel_lr.fit_transform(X0_ss0,y_cat),y_cat)\n",
    "          best_cv = np.argmax(cv_scores_tb)\n",
    "          best_cv_lgr = np.argmax(cv_lgr_scores_tb)\n",
    "        \n",
    "        # Fit whole dataset with optimal cv\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=best_cv_lgr,class_weight=None,penalty='l1',solver='liblinear',random_state=1)\n",
    "        sel = skf.RFECV(lasso,step=rcfs,cv=best_cv)\n",
    "        sel_lr = skf.RFECV(lgr,step=rcfs,cv=best_cv)\n",
    "        X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "        X_test_ss = sel.transform(X_test_ss0)\n",
    "        X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "        X_test_ssl = sel_lr.transform(X_test_ss0)\n",
    "        K_ss = sel.transform(K_all_c.reshape(1,-1))\n",
    "        K_ss_lr = sel_lr.transform(K_all_c.reshape(1,-1))\n",
    "        # R_ss = sel.transform(R_all_c.reshape(1,-1))\n",
    "\n",
    "\n",
    "      # LASSO\n",
    "      with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=best_cv_lgr,class_weight=None,penalty='l1',solver='liblinear',random_state=1)\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "        est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "      results_lgr_aug[c] = est_lgr.predict(X_test_ssl)\n",
    "      results_lgrp_aug[c] = est_lgr.predict_proba(X_test_ssl)[0][0]\n",
    "      results_ls_aug[c] = est_ls.predict(X_test_ss)\n",
    "      print('Lasso predicts',str(np.round(results_ls_aug[c],4)),'and logistic regression predicts',results_lgr_aug[c],\n",
    "                'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv,'and',sum(y_cat),'minority cases')\n",
    "      K_nz_nc.append(np.squeeze(K_ss)[abs(est_ls.coef_)>0])\n",
    "      K_nz_nc_lr.append(np.squeeze(K_ss_lr)[np.squeeze(abs(est_lgr.coef_)>0)])\n",
    "      c=c+1\n",
    "\n",
    "  np.save('results_ls_aug_d.npy',results_ls_aug)\n",
    "  np.save('results_lgr_aug_d.npy',results_lgr_aug)\n",
    "  np.save('results_lgrp_aug_d.npy',results_lgrp_aug)\n",
    "  np.save('results_K_nz_nc.npy',K_nz_nc)\n",
    "  np.save('results_K_nz_nc_lr.npy',K_nz_nc_lr)\n",
    "  np.save('beta_nz_nc.npy',est_ls.coef_[abs(est_ls.coef_)>0])\n",
    "  np.save('beta_nz_nc_lr.npy',est_lgr.coef_[np.squeeze(abs(est_lgr.coef_)>0)])\n",
    "else:\n",
    "  results_ls_aug = np.load('results_ls_aug_d.npy')\n",
    "  results_lgr_aug = np.load('results_lgr_aug_d.npy')\n",
    "  results_lgrp_aug = np.load('results_lgrp_aug_d.npy')\n",
    "  K_nz_nc = np.load('results_K_nz_nc.npy',allow_pickle=True)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(abs(est_lgr.coef_)>0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Lasso\n",
    "Js = []\n",
    "aug = False\n",
    "K_nz_vl = []\n",
    "K_nz_vl_lr = []\n",
    "err_var = np.zeros_like(per_change)\n",
    "rerror = np.zeros_like(per_change)\n",
    "kappa = []\n",
    "K_nz = []\n",
    "E_nz = []\n",
    "c = 0\n",
    "\n",
    "for j in np.arange(len(subsc)):\n",
    "    test_id = subsc[j]\n",
    "    test_index = subsc == test_id\n",
    "    train_index = subsc != test_id\n",
    "    X_train = X_all_c[train_index,:,:]\n",
    "    X_test = X_all_c[test_index,:,:]\n",
    "    y_train = per_change[train_index]\n",
    "    y_test = per_change[test_index]\n",
    "\n",
    "    y_cat = y_train <= 0.3\n",
    "    idy = np.where(y_cat==1)\n",
    "    # Cross validation\n",
    "                                          \n",
    "    X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                X_train,train_index,X_test,\n",
    "                                                test_index,pre_updrs_off,None,None,None,None,None,None,None,None,None,False,False,False)\n",
    "    cvn = 5\n",
    "    cv_scores = np.zeros((cvn,1))\n",
    "    cv_lgr_scores = np.zeros((cvn,1))\n",
    "    rs = 1\n",
    "    rcfs = 1000\n",
    "    (mu, sigma) = stats.norm.fit(y_train)\n",
    "    kappa = stats.skew(y_train)\n",
    "    print('Label distribution of:',mu,sigma,kappa)\n",
    "    for jj in np.arange(2,cvn):\n",
    "      # Resample to avoid stratification errors\n",
    "      while np.sum(y_cat) < cvn:\n",
    "        np.random.seed(rs)\n",
    "        idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "        X0_ss0 = np.append(X0_ss0,X0_ss0[idyr,:].reshape(1,-1),axis=0)\n",
    "        y_train = np.append(y_train,y_train[idyr])\n",
    "        y_cat = y_train <= 0.3\n",
    "        rs = rs+1\n",
    "        print('Resampled to size',y_train.shape)\n",
    "      if aug == True:\n",
    "        y_train_n = y_train+(1.96*sigma)*np.random.normal(0,1,1)\n",
    "        y_train = np.hstack((y_train,y_train_n))\n",
    "        y_cat = y_train <= 0.3\n",
    "        X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "    \n",
    "    for jj in np.arange(2,cvn):\n",
    "      skf_g = sms.StratifiedKFold(n_splits=jj,shuffle=True,random_state=0)\n",
    "      skf_gen = skf_g.split(X0_ss0,y_cat)\n",
    "      lasso = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None,penalty='l1',solver='liblinear',random_state=0)\n",
    "        print('Found minority samples:',sum(y_cat))\n",
    "        # Feature selection\n",
    "        sel = skf.RFECV(lasso,step=rcfs,cv=skf_gen,n_jobs=1)\n",
    "        # Stratifies classifiers automatically\n",
    "        sel_lr = skf.RFECV(lgr,step=rcfs,cv=jj,n_jobs=1)\n",
    "        X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "        X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "        est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "        cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "        cv_lgr_scores[jj] = est_lgr.score(X0_ssl,y_cat)\n",
    "        print('LogisticRegressionCV score for',jj,'is',cv_lgr_scores[jj],'from dataset of size',X0_ss.shape)\n",
    "        \n",
    "    with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "      best_cv = np.argmax(cv_scores)\n",
    "      best_cv_lgr = np.argmax(cv_lgr_scores)\n",
    "\n",
    "      # Break any ties\n",
    "      if np.sum(cv_scores == best_cv) > 1:\n",
    "        cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "        cv_lgr_scores_tb = np.zeros((np.sum(cv_scores == best_cv_lgr),1))\n",
    "        for jjj in (cv_scores == cv_scores[best_cv]):\n",
    "          if jjj > 0:\n",
    "            print('Breaking tie')\n",
    "            skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "            skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "            X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "            X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "            est_ls = lasso.fit(X0_ss,y_train)\n",
    "            est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "            lasso = slm.LassoLarsCV(max_iter=1000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "            lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None,penalty='l1',solver='liblinear',random_state=1)\n",
    "            est_ls = lasso.fit(X0_ss,y_train)\n",
    "            est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "            cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "            cv_lgr_scores_tb[jj] = est_lgr.score(sel_lr.fit_transform(X0_ss0,y_cat),y_cat)\n",
    "        best_cv = np.argmax(cv_scores_tb)\n",
    "        best_cv_lgr = np.argmax(cv_lgr_scores_tb)\n",
    "      \n",
    "      # Fit whole dataset with optimal cv\n",
    "      lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None,penalty='l1',solver='liblinear',random_state=0)\n",
    "      sel = skf.RFECV(lasso,step=rcfs,cv=best_cv)\n",
    "      sel_lr = skf.RFECV(lgr,step=rcfs,cv=best_cv)\n",
    "      X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "      X_test_ss = sel.transform(X_test_ss0)\n",
    "      X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "      X_test_ssl = sel_lr.transform(X_test_ss0)\n",
    "      K_ss = sel.transform(K_all_c.reshape(1,-1))\n",
    "      K_ss_lr = sel_lr.transform(K_all_c.reshape(1,-1))\n",
    "\n",
    "    # LASSO\n",
    "    with warnings.catch_warnings():\n",
    "      warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "      lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=best_cv_lgr,class_weight=None,penalty='l1',solver='liblinear',random_state=0)\n",
    "      est_ls = lasso.fit(X0_ss,y_train)\n",
    "      est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "    results_lgr[c] = est_lgr.predict(X_test_ssl)\n",
    "    results_lgrp[c] = est_lgr.predict_proba(X_test_ssl)[0][0]\n",
    "    results_ls[c] = est_ls.predict(X_test_ss)\n",
    "    print('Lasso predicts',str(np.round(results_ls[c],4)),'and logistic regression predicts',results_lgr[c],\n",
    "              'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv,'and',sum(y_cat),'minority cases')\n",
    "    K_nz_vl.append(np.squeeze(K_ss)[abs(est_ls.coef_)>0])\n",
    "    K_nz_vl_lr.append(np.squeeze(K_ss_lr)[np.squeeze(abs(est_lgr.coef_)>0)])\n",
    "    c=c+1\n",
    "\n",
    "np.save('results_ls_d.npy',results_ls)\n",
    "np.save('results_lgr_d.npy',results_lgr)\n",
    "np.save('results_lgrp_d.npy',results_lgrp)\n",
    "np.save('results_K_nz_vl.npy',K_nz_vl)\n",
    "np.save('results_K_nz_vl_lr.npy',K_nz_vl_lr)\n",
    "np.save('beta_nz_vl.npy',est_ls.coef_[abs(est_ls.coef_)>0])\n",
    "np.save('beta_nz_vl_lr.npy',est_lgr.coef_[np.squeeze(abs(est_lgr.coef_)>0)])\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOGN LASSO\n",
    "if retrain == 1:\n",
    "  results_ls_smogn = np.zeros_like(per_change)\n",
    "  aug = False\n",
    "  c = 0\n",
    "  K_nz_smogn = []\n",
    "  #R_nz_smogn = []\n",
    "  for j in np.arange(c,len(subsc)):\n",
    "      test_id = subsc[j]\n",
    "      test_index = subsc == test_id\n",
    "      train_index = subsc != test_id\n",
    "      X_train = X_all_c[train_index,:,:]\n",
    "      X_test = X_all_c[test_index,:,:]\n",
    "      y_train = per_change[train_index]\n",
    "      y_test = per_change[test_index]\n",
    "\n",
    "      y_cat = y_train <= 0.3\n",
    "      idy = np.where(y_cat==1)\n",
    "      # Cross validation\n",
    "\n",
    "    \n",
    "                                            \n",
    "      X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                  X_train,train_index,X_test,\n",
    "                                                  test_index,pre_updrs_off,None,None,None,None,None,None,None,None,None,False,False,False)\n",
    "      X_smogn,y_smogn,idx_kept,sscaler = util.rad_smogn(X0_ss0,y_train,np.amax(y_train),np.amin(y_train),1,0,0.05,0.02)\n",
    "      X0_ss00 = X0_ss0\n",
    "      X0_ss0 = np.vstack((X0_ss0,X_smogn))\n",
    "      y_train = np.hstack((y_train,y_smogn))\n",
    "      y_train_0 = y_train\n",
    "      cvn = 5\n",
    "\n",
    "      cv_scores = np.zeros((cvn,1))\n",
    "      rs = 1\n",
    "      rcfs = 1000\n",
    "      (mu, sigma) = stats.norm.fit(y_train)\n",
    "      kappa = stats.skew(y_train)\n",
    "      print('Label distribution of:',mu,sigma,kappa)\n",
    "      for jj in np.arange(2,cvn):\n",
    "        # Resample to avoid stratification errors\n",
    "        while np.sum(y_cat) < cvn:\n",
    "          np.random.seed(rs)\n",
    "          idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "          X0_ss0 = np.append(X0_ss0,X0_ss0[idyr,:].reshape(1,-1),axis=0)\n",
    "          y_train = np.append(y_train,y_train[idyr])\n",
    "          y_cat = y_train <= 0.3\n",
    "          rs = rs+1\n",
    "          print('Resampled to size',y_train.shape)\n",
    "\n",
    "      for jj in np.arange(2,cvn):\n",
    "        skf_g = sms.StratifiedKFold(n_splits=jj,shuffle=True,random_state=0)\n",
    "        skf_gen = skf_g.split(X0_ss0,y_cat)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "          # Feature selection\n",
    "          warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "          sel = skf.RFECV(lasso,step=rcfs,cv=skf_gen)\n",
    "          X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "          X0_ss = X0_sst\n",
    "          est_ls = lasso.fit(X0_ss,y_train)\n",
    "          cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "          print('LassoCV score for',jj,'is',cv_scores[jj],'from dataset of size',X0_ss.shape)\n",
    "          \n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "        best_cv = np.argmax(cv_scores)\n",
    "        # Break any ties\n",
    "        if np.sum(cv_scores == best_cv) > 1:\n",
    "          cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "          for jjj in (cv_scores == cv_scores(best_cv)):\n",
    "            if jjj > 0:\n",
    "              skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "              skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "              X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "              X0_ss = X0_sst\n",
    "              lasso = slm.LassoLarsCV(max_iter=1000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "          best_cv = np.argmax(cv_scores_tb)\n",
    "        \n",
    "        # Fit whole dataset with optimal cv\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        sel = skf.RFECV(lasso,step=rcfs,cv=best_cv)\n",
    "        X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "        X_test_ss = sel.transform(X_test_ss0)\n",
    "        K_ss = sel.transform(K_all_c.reshape(1,-1))\n",
    "        #R_ss = sel.transform(R_all_c.reshape(1,-1))\n",
    "        \n",
    "      # LASSO\n",
    "      with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "        results_ls_smogn[c] = est_ls.predict(X_test_ss)\n",
    "        if results_ls_smogn[c] < 0:\n",
    "            dx, y_n = cKDTree(X0_ss00.reshape(X0_ss00.shape[0],-1)).query(X_test_ss0.reshape(1,-1),k=15)\n",
    "            results_ls_smogn[c] = np.mean((y_train_0[y_n]))\n",
    "            print('Using nearest neighbor')\n",
    "        print('Lasso predicts',str(np.round(results_ls_smogn[c],4)),\n",
    "              'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv)\n",
    "        K_nz_smogn.append(np.squeeze(K_ss)[abs(est_ls.coef_)>0])\n",
    "        #R_nz_smogn.append(np.squeeze(R_ss)[est_ls.coef_>0])\n",
    "        c=c+1\n",
    "\n",
    "\n",
    "  np.save('results_ls_smogn_d.npy',results_ls_smogn)\n",
    "  np.save('results_K_nz_smogn.npy',K_nz_smogn)\n",
    "  np.save('beta_nz_smogn.npy',est_ls.coef_[abs(est_ls.coef_)>0])\n",
    "\n",
    "else:\n",
    "  results_ls_smogn = np.load('results_ls_smogn_d.npy')\n",
    "  K_nz_smogn = np.load('results_K_nz_smogn.npy',allow_pickle=True)\n",
    "  \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define probability of an unsucessful procedure as $P = 1-u$ where $u$ is the UPDRS percent improvement prediction between $0$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wild bootstrap LASSO\n",
    "if retrain == 1:\n",
    "  aug = False\n",
    "  results_ls_wbs = np.zeros_like(per_change)\n",
    "  K_nz_wbs = []\n",
    "  #R_nz_wbs = []\n",
    "  c = 0\n",
    "  for j in np.arange(c,len(subsc)):\n",
    "      test_id = subsc[j]\n",
    "      test_index = subsc == test_id\n",
    "      train_index = subsc != test_id\n",
    "      X_train = X_all_c[train_index,:,:]\n",
    "      X_test = X_all_c[test_index,:,:]\n",
    "      y_train = per_change[train_index]\n",
    "      y_test = per_change[test_index]\n",
    "\n",
    "      y_cat = y_train <= 0.3\n",
    "      idy = np.where(y_cat==1)\n",
    "      # Cross validation\n",
    "                                            \n",
    "      X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                  X_train,train_index,X_test,\n",
    "                                                  test_index,pre_updrs_off,None,None,None,None,None,None,None,None,None,False,False,False)\n",
    "      X0_ss00 = X0_ss0\n",
    "      y_train_0 = y_train\n",
    "      cvn = 5\n",
    "\n",
    "      cv_scores = np.zeros((cvn,1))\n",
    "      rs = 1\n",
    "      rcfs = 1000\n",
    "      (mu, sigma) = stats.norm.fit(y_train)\n",
    "      kappa = stats.skew(y_train)\n",
    "      print('Label distribution of:',mu,sigma,kappa)\n",
    "      for jj in np.arange(2,cvn):\n",
    "        # Resample to avoid stratification errors\n",
    "        while np.sum(y_cat) < cvn:\n",
    "          np.random.seed(rs)\n",
    "          idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "          X0_ss0 = np.append(X0_ss0,X0_ss0[idyr,:].reshape(1,-1),axis=0)\n",
    "          y_train = np.append(y_train,y_train[idyr])\n",
    "          y_cat = y_train <= 0.3\n",
    "          rs = rs+1\n",
    "          print('Resampled to size',y_train.shape)\n",
    "          ls0 = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "          est0 = ls0.fit(X0_ss0,y_train)\n",
    "          eps = y_train-ls0.predict(X0_ss0)\n",
    "          eps_v = eps*np.random.normal(0,1,1)\n",
    "          y_train_0 = y_train\n",
    "        if aug == True:\n",
    "          y_train_n = y_train+(1.96*sigma)*np.random.normal(0,1,1)\n",
    "          y_train = np.hstack((y_train,y_train_n))\n",
    "          y_cat = y_train <= 0.3\n",
    "          X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "        else: # Control for different training sample sizes\n",
    "          while len(eps_v) < len(y_train):\n",
    "            eps_v = np.hstack((eps_v,eps*np.random.normal(0,1,1)))\n",
    "          y_train_n = y_train+eps_v\n",
    "          y_train = np.hstack((y_train,y_train_n))\n",
    "          y_cat = y_train <= 0.3\n",
    "          X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "\n",
    "      for jj in np.arange(2,cvn):\n",
    "        skf_g = sms.StratifiedKFold(n_splits=jj,shuffle=True,random_state=0)\n",
    "        skf_gen = skf_g.split(X0_ss0,y_cat)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "          # Feature selection\n",
    "          warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "          sel = skf.RFECV(lasso,step=rcfs,cv=skf_gen)\n",
    "          X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "          X0_ss = X0_sst\n",
    "          est_ls = lasso.fit(X0_ss,y_train)\n",
    "          cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "          print('LassoCV score for',jj,'is',cv_scores[jj],'from dataset of size',X0_ss.shape)\n",
    "          \n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "        best_cv = np.argmax(cv_scores)\n",
    "        # Break any ties\n",
    "        if np.sum(cv_scores == best_cv) > 1:\n",
    "          cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "          for jjj in (cv_scores == cv_scores(best_cv)):\n",
    "            if jjj > 0:\n",
    "              skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "              skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "              X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "              X0_ss = X0_sst\n",
    "              lasso = slm.LassoLarsCV(max_iter=1000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "          best_cv = np.argmax(cv_scores_tb)\n",
    "        \n",
    "        # Fit whole dataset with optimal cv\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        sel = skf.RFECV(lasso,step=rcfs,cv=best_cv)\n",
    "        X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "        X_test_ss = sel.transform(X_test_ss0)\n",
    "        K_ss = sel.transform(K_all_c.reshape(1,-1))\n",
    "        #R_ss = sel.transform(R_all_c.reshape(1,-1))\n",
    "        \n",
    "      # LASSO\n",
    "      with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "        results_ls_wbs[c] = est_ls.predict(X_test_ss)\n",
    "        if results_ls_wbs[c] < 0:\n",
    "            dx, y_n = cKDTree(X0_ss00.reshape(X0_ss00.shape[0],-1)).query(X_test_ss0.reshape(1,-1),k=15)\n",
    "            results_ls_wbs[c] = np.mean((y_train_0[y_n]))\n",
    "            print('Using nearest neighbor')\n",
    "        print('Lasso predicts',str(np.round(results_ls_wbs[c],4)),\n",
    "              'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv)\n",
    "        K_nz_wbs.append(np.squeeze(K_ss)[abs(est_ls.coef_)>0])\n",
    "        #R_nz_wbs.append(np.squeeze(R_ss)[est_ls.coef_>0])\n",
    "        c=c+1\n",
    "\n",
    "  np.save('results_ls_wbs_d.npy',results_ls_wbs)\n",
    "  np.save('results_K_nz_wbs.npy',K_nz_wbs)\n",
    "  np.save('beta_nz_wbs.npy',est_ls.coef_[abs(est_ls.coef_)>0])\n",
    "else:\n",
    "  results_ls_wbs = np.load('results_ls_wbs_d.npy')\n",
    "  K_nz_wbs = np.load('results_K_nz_wbs.npy',allow_pickle=True)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
    "    data1     = np.asarray(data1)\n",
    "    data2     = np.asarray(data2)\n",
    "    mean      = np.mean([data1, data2], axis=0)\n",
    "    diff      = data1-data2                   # Difference between datasets\n",
    "    md        = np.mean(diff)                 # Mean of the difference\n",
    "    sd        = np.std(diff, axis=0)          # Standard deviation of the difference\n",
    "\n",
    "    plt.scatter(mean, diff, *args, **kwargs)\n",
    "    plt.axhline(md,           color='gray', linestyle='--')\n",
    "    plt.text(0.1, 2*md, r'$\\mu$ = '+str(np.round(md,2)), horizontalalignment='center',verticalalignment='center')\n",
    "    plt.text(0.5, 0.4, r'$\\mu + 1.96 \\sigma$ = '+str(np.round(md + 1.96*sd,2)), horizontalalignment='center',verticalalignment='center')\n",
    "    plt.text(0.5, -0.5, r'$\\mu - 1.96 \\sigma$ = '+str(np.round(md - 1.96*sd,2)), horizontalalignment='center',verticalalignment='center')\n",
    "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
    "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
    "    plt.ylim([-4*1.96*sd,4*1.96*sd])\n",
    "    diff_in = np.logical_and(diff < md + 1.96*sd,diff > md - 1.96*sd)\n",
    "    plt.scatter(mean[diff_in], diff[diff_in], *args, **kwargs)\n",
    "    plt.xlabel(r'$\\frac{y_{true}+y_{predicted}}{2}$',fontsize=16)\n",
    "    plt.ylabel(r'$y_{predicted}-y_{true}$',fontsize=12)\n",
    "    return diff,diff_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d,din = bland_altman_plot(results_ls_aug,per_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = 1.25\n",
    "fs = 32\n",
    "ofx = 0.25\n",
    "ofy = 0.75\n",
    "pre_imp = np.repeat((np.asarray(pre_updrs_iii_off,dtype=float)-np.asarray(pre_updrs_iii_on,dtype=float))/np.asarray(pre_updrs_iii_off,dtype=float),r)\n",
    "per_change = np.repeat(per_change,r)\n",
    "plt.rcParams['figure.figsize'] = [40, 10]\n",
    "fig,ax = plt.subplots(1,5,sharex=True,sharey=True)\n",
    "#ax[0].scatter(pre_imp,per_change)\n",
    "col = np.where(per_change <= 0.3,'orange','blue')\n",
    "ax[0].scatter(pre_imp,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(pre_imp,per_change)\n",
    "y_model = pre_imp*lr.slope+lr.intercept\n",
    "ax[0].plot(pre_imp,y_model,color='r')\n",
    "ax[0].text(ofx,ofy,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.3f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[0].transAxes,fontsize=fs)  \n",
    "ax[0].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[0].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[0].set_title('LCT',fontsize=fs)\n",
    "ax[0].set_xlabel('Prediction',fontsize=fs)\n",
    "ax[0].xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[0].set_ylabel('True UPDRS-III improvement',fontsize=fs)\n",
    "ax[0].yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "col = np.where(per_change <= 0.3,'orange','blue')\n",
    "\n",
    "ax[1].scatter(results_ls,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(np.squeeze(results_ls),per_change)\n",
    "y_model = results_ls*lr.slope+lr.intercept\n",
    "ax[1].plot(results_ls,y_model,color='r')\n",
    "ax[1].text(ofx,ofy,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.7f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[1].transAxes,fontsize=fs) \n",
    "ax[1].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[1].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[1].set_title('Lasso',fontsize=fs)\n",
    "ax[1].set_xlabel('Prediction',fontsize=fs)\n",
    "ax[1].xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "# ax[1].set_ylabel('True Improvement',fontsize=fs)\n",
    "ax[1].yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "col = np.where(per_change <= 0.3,'orange','blue')\n",
    "\n",
    "\n",
    "ax[2].scatter(results_ls_smogn,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(np.squeeze(results_ls_smogn),per_change)\n",
    "y_model = results_ls_smogn*lr.slope+lr.intercept\n",
    "ax[2].plot(results_ls_smogn,y_model,color='r')\n",
    "ax[2].text(ofx,ofy,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.7f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[2].transAxes,fontsize=fs) \n",
    "ax[2].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[2].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[2].set_xlabel('Prediction',fontsize=fs)\n",
    "ax[2].xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "# ax[2].set_ylabel('True Improvement',fontsize=fs)\n",
    "ax[2].yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[2].set_title('SMOGN Lasso',fontsize=fs)\n",
    "\n",
    "ax[3].scatter(results_ls_wbs,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(np.squeeze(results_ls_wbs),per_change)\n",
    "y_model = results_ls_wbs*lr.slope+lr.intercept\n",
    "ax[3].plot(results_ls_wbs,y_model,color='r')\n",
    "ax[3].text(ofx,ofy,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.7f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[3].transAxes,fontsize=fs) \n",
    "ax[3].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[3].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[3].set_xlabel('Prediction',fontsize=fs)\n",
    "ax[3].xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "# ax[3].set_ylabel('True Improvement',fontsize=fs)\n",
    "ax[3].yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[3].set_title('Wild Boostrap Lasso',fontsize=fs)\n",
    "\n",
    "ax[4].scatter(results_ls_aug,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(np.squeeze(results_ls_aug),per_change)\n",
    "y_model = results_ls_aug*lr.slope+lr.intercept\n",
    "ax[4].plot(results_ls_aug,y_model,color='r')\n",
    "ax[4].text(ofx,ofy,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.7f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[4].transAxes,fontsize=fs) \n",
    "ax[4].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[4].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[4].set_xlabel('Prediction',fontsize=fs)\n",
    "ax[4].xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "# ax[4].set_ylabel('True Improvement',fontsize=fs)\n",
    "ax[4].yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[4].set_title('Noise Compensated Lasso',fontsize=fs)\n",
    "plt.ylim([0,ylim])\n",
    "plt.xlim([0,ylim])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (30,15)\n",
    "fig, axes = plt.subplots(1,4,sharey=True)\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.2, \n",
    "                    hspace=0.2)\n",
    "\n",
    "R = [item for sublist in K_nz_vl for item in sublist]\n",
    "letter_counts = Counter(R)\n",
    "lc = { x: count for x, count in letter_counts.items() if count > 30 }\n",
    "df = pd.DataFrame.from_dict(lc, orient='index')\n",
    "df.sort_values(0, ascending=False, inplace=True)\n",
    "df.plot(ax=axes[0], y=0, kind='bar', legend=False, fontsize=fs,stacked=True, width=1, edgecolor='black',color='tab:orange')\n",
    "axes[0].set_title('Lasso',fontsize=fs)\n",
    "R = [item for sublist in K_nz_smogn for item in sublist]\n",
    "letter_counts = Counter(R)\n",
    "lc = { x: count for x, count in letter_counts.items() if count > 23 }\n",
    "df = pd.DataFrame.from_dict(lc, orient='index')\n",
    "df.sort_values(0, ascending=False, inplace=True)\n",
    "df.plot(ax=axes[1], y=0, kind='bar', legend=False, fontsize=fs,stacked=True, width=1, edgecolor='black',color='tab:red')\n",
    "axes[1].set_title('SMOGN Lasso',fontsize=fs)\n",
    "R = [item for sublist in K_nz_wbs for item in sublist]\n",
    "letter_counts = Counter(R)\n",
    "lc = { x: count for x, count in letter_counts.items() if count > 28 }\n",
    "df = pd.DataFrame.from_dict(lc, orient='index')\n",
    "df.sort_values(0, ascending=False, inplace=True)\n",
    "df.plot(ax=axes[2], y=0, kind='bar', legend=False, fontsize=fs,stacked=True, width=1, edgecolor='black',color='tab:purple')\n",
    "axes[2].set_title('Wild Bootstrap Lasso',fontsize=fs)\n",
    "R = [item for sublist in K_nz_nc for item in sublist]\n",
    "letter_counts = Counter(R)\n",
    "lc = { x: count for x, count in letter_counts.items() if count > 25 }\n",
    "df = pd.DataFrame.from_dict(lc, orient='index')\n",
    "df.sort_values(0, ascending=False, inplace=True)\n",
    "df.plot(ax=axes[3], y=0, kind='bar', legend=False, fontsize=fs,stacked=True, width=1, edgecolor='black',color='tab:green')\n",
    "axes[3].set_title('Noise Compensated Lasso',fontsize=fs)\n",
    "plt.suptitle('Predictive radiomic features',fontsize=fs)\n",
    "plt.ylabel('Frequency',fontsize=16)\n",
    "\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (30,15)\n",
    "fig,ax = plt.subplots(1,2,sharex=True,sharey=True)\n",
    "t = 0.3\n",
    "y_bin = per_change<t\n",
    "\n",
    "y_predicted_ls = 1-results_ls\n",
    "y_predicted_ls_aug = 1-results_ls_aug\n",
    "y_predicted_lct = 1-pre_imp\n",
    "y_predicted_aug = 1-results_lgrp_aug\n",
    "y_predicted_lr = 1-results_lgrp\n",
    "y_predicted_smogn = 1-results_ls_smogn\n",
    "y_predicted_wbs = 1-results_ls_wbs\n",
    "\n",
    "fprlsa, tprlsa, _ = roc_curve(y_bin,  y_predicted_ls_aug)\n",
    "fprls, tprls, _ = roc_curve(y_bin,  y_predicted_ls)\n",
    "fprl, tprl, _ = roc_curve(y_bin,  y_predicted_lct)\n",
    "fpra, tpra, _ = roc_curve(y_bin, y_predicted_aug)\n",
    "fprlr, tprlr, _ = roc_curve(y_bin, y_predicted_lr)\n",
    "fprlss, tprlss, _ = roc_curve(y_bin, y_predicted_smogn)\n",
    "fprlwb, tprlwb, _ = roc_curve(y_bin, y_predicted_wbs)\n",
    "\n",
    "roc_auc_lct = auc(fprl, tprl)\n",
    "roc_auc_lr = auc(fprlr, tprlr)\n",
    "roc_auc_lr_aug = auc(fpra, tpra)\n",
    "roc_auc_ls = auc(fprls, tprls)\n",
    "roc_auc_ls_aug = auc(fprlsa, tprlsa)\n",
    "roc_auc_ls_smogn = auc(fprlss, tprlss)\n",
    "roc_auc_ls_wbs = auc(fprlwb, tprlwb)\n",
    "\n",
    "plt.rcParams['legend.loc']='lower right'\n",
    "ax[0].plot(fprl, tprl, label = 'LCT AUC = %0.2f' % roc_auc_lct, linewidth=5)\n",
    "ax[0].plot(fprlr, tprlr, label = 'LR AUC = %0.2f' % roc_auc_lr, linewidth=5)\n",
    "ax[0].plot(fpra, tpra, label = 'LR + noise AUC = %0.2f' % roc_auc_lr_aug, linewidth=5)\n",
    "# plt.plot(fprls, tprls, label = 'LASSO AUC = %0.2f' % roc_auc_ls)\n",
    "# plt.plot(fprlsa, tprlsa, label = 'LASSO + noise AUC = %0.2f' % roc_auc_ls_aug)\n",
    "ax[0].set_title('Classifier performance',fontsize=fs)\n",
    "ax[0].set_xlabel('False positive rate \\n $(1-specificity$)',fontsize=fs)\n",
    "ax[0].set_ylabel('True positive rate \\n $(sensitivity$)',fontsize=fs)\n",
    "ax[0].xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[0].yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[0].legend(fontsize=fs//1.5)\n",
    "plt.rcParams['legend.loc']='lower right'\n",
    "ax[1].plot(fprl, tprl, label = 'LCT AUC = %0.2f' % roc_auc_lct, linewidth=5)\n",
    "ax[1].plot(fprls, tprls, label = 'LASSO AUC = %0.2f' % roc_auc_ls, linewidth=5)\n",
    "ax[1].plot(fprlsa, tprlsa, label = 'LASSO + noise AUC = %0.2f' % roc_auc_ls_aug, linewidth=5)\n",
    "ax[1].plot(fprlss, tprlss, label = 'LASSO + SMOGN AUC = %0.2f' % roc_auc_ls_smogn, linewidth=5)\n",
    "ax[1].plot(fprlwb, tprlwb, label = 'LASSO + wild bootstrap AUC = %0.2f' % roc_auc_ls_wbs, linewidth=5)\n",
    "# plt.plot(fprls, tprls, label = 'LASSO AUC = %0.2f' % roc_auc_ls)\n",
    "# plt.plot(fprlsa, tprlsa, label = 'LASSO + noise AUC = %0.2f' % roc_auc_ls_aug)\n",
    "ax[1].set_title('Binarized regressor performance',fontsize=fs)\n",
    "ax[1].set_xlabel('False positive rate \\n $(1-specificity$)',fontsize=fs)\n",
    "ax[1].set_ylabel('True positive rate \\n $(sensitivity$)',fontsize=fs)\n",
    "ax[1].xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[1].yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[1].legend(fontsize=fs//1.5)\n",
    "plt.style.use('default')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lgrp_aug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14732b5bb7ad6abfe54a083b8d194ae3941adfb1b18321b588b21cb8f420fced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
