{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import nibabel as nib\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score, roc_curve, auc\n",
    "import SimpleITK as sitk\n",
    "import six\n",
    "from radiomics import featureextractor \n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import logging\n",
    "import scipy\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearnex import patch_sklearn, config_context\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn import preprocessing as skp\n",
    "from sklearn import model_selection as sms\n",
    "from sklearn import feature_selection as skf\n",
    "from sklearn import linear_model as slm\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from IPython.display import HTML\n",
    "import util as util\n",
    "import nibabel as nib\n",
    "import os\n",
    "import pickle\n",
    "from torch import nn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "def remove_keymap_conflicts(new_keys_set):\n",
    "    for prop in plt.rcParams:\n",
    "        if prop.startswith('keymap.'):\n",
    "            keys = plt.rcParams[prop]\n",
    "            remove_list = set(keys) & new_keys_set\n",
    "            for key in remove_list:\n",
    "                keys.remove(key)\n",
    "\n",
    "def multi_slice_viewer(volume):\n",
    "    remove_keymap_conflicts({'j', 'k'})\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.volume = volume\n",
    "    ax.index = volume.shape[0]//2\n",
    "    ax.imshow(volume[ax.index])\n",
    "    fig.canvas.mpl_connect('key_press_event', process_key)\n",
    "\n",
    "def process_key(event):\n",
    "    fig = event.canvas.figure\n",
    "    ax = fig.axes[0]\n",
    "    if event.key == 'j':\n",
    "        previous_slice(ax)\n",
    "    elif event.key == 'k':\n",
    "        next_slice(ax)\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "\n",
    "def previous_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index-1) % volume.shape[0] \n",
    "    ax.images[0].set_array(volume[ax.index])\n",
    "\n",
    "def next_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index+1) % volume.shape[0]\n",
    "    ax.images[0].set_array(volume[ax.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending arrays with segmentation 01_roi_combined.nii QSM, 00001_qsm.nii.gz LARO, QSM_lrr_01.nii and mask mask01.nii\n",
      "Appending arrays with segmentation 02_roi_combined.nii QSM, 00002_qsm.nii.gz LARO, QSM_lrr_02.nii and mask mask02.nii\n",
      "Appending arrays with segmentation 03_roi_combined.nii QSM, 00003_qsm.nii.gz LARO, QSM_lrr_03.nii and mask mask03.nii\n",
      "Appending arrays with segmentation 04_roi_combined.nii QSM, 00004_qsm.nii.gz LARO, QSM_lrr_04.nii and mask mask04.nii\n",
      "Appending arrays with segmentation 05_roi_combined.nii QSM, 00005_qsm.nii.gz LARO, QSM_lrr_05.nii and mask mask05.nii\n",
      "Appending arrays with segmentation 06_roi_combined.nii QSM, 00006_qsm.nii.gz LARO, QSM_lrr_06.nii and mask mask06.nii\n",
      "Appending arrays with segmentation 07_roi_combined.nii QSM, 00007_qsm.nii.gz LARO, QSM_lrr_07.nii and mask mask07.nii\n",
      "Appending arrays with segmentation 08_roi_combined.nii QSM, 00008_qsm.nii.gz LARO, QSM_lrr_08.nii and mask mask08.nii\n",
      "Appending arrays with segmentation 09_roi_combined.nii QSM, 00009_qsm.nii.gz LARO, QSM_lrr_09.nii and mask mask09.nii\n",
      "Appending arrays with segmentation 10_roi_combined.nii QSM, 00010_qsm.nii.gz LARO, QSM_lrr_10.nii and mask mask10.nii\n",
      "Appending arrays with segmentation 11_roi_combined.nii QSM, 00011_qsm.nii.gz LARO, QSM_lrr_11.nii and mask mask11.nii\n",
      "Appending arrays with segmentation 13_roi_combined.nii QSM, 00013_qsm.nii.gz LARO, QSM_lrr_13.nii and mask mask13.nii\n",
      "Appending arrays with segmentation 14_roi_combined.nii QSM, 00014_qsm.nii.gz LARO, QSM_lrr_14.nii and mask mask14.nii\n",
      "Appending arrays with segmentation 16_roi_combined.nii QSM, 00016_qsm.nii.gz LARO, QSM_lrr_16.nii and mask mask16.nii\n",
      "Appending arrays with segmentation 18_roi_combined.nii QSM, 00018_qsm.nii.gz LARO, QSM_lrr_18.nii and mask mask18.nii\n",
      "Appending arrays with segmentation 19_roi_combined.nii QSM, 00019_qsm.nii.gz LARO, QSM_lrr_19.nii and mask mask19.nii\n",
      "Appending arrays with segmentation 20_roi_combined.nii QSM, 00020_qsm.nii.gz LARO, QSM_lrr_20.nii and mask mask20.nii\n",
      "Appending arrays with segmentation 21_roi_combined.nii QSM, 00021_qsm.nii.gz LARO, QSM_lrr_21.nii and mask mask21.nii\n",
      "Appending arrays with segmentation 22_roi_combined.nii QSM, 00022_qsm.nii.gz LARO, QSM_lrr_22.nii and mask mask22.nii\n",
      "Appending arrays with segmentation 23_roi_combined.nii QSM, 00023_qsm.nii.gz LARO, QSM_lrr_23.nii and mask mask23.nii\n",
      "Appending arrays with segmentation 24_roi_combined.nii QSM, 00024_qsm.nii.gz LARO, QSM_lrr_24.nii and mask mask24.nii\n",
      "Appending arrays with segmentation 25_roi_combined.nii QSM, 00025_qsm.nii.gz LARO, QSM_lrr_25.nii and mask mask25.nii\n",
      "Appending arrays with segmentation 26_roi_combined.nii QSM, 00026_qsm.nii.gz LARO, QSM_lrr_26.nii and mask mask26.nii\n",
      "Appending arrays with segmentation 27_roi_combined.nii QSM, 00027_qsm.nii.gz LARO, QSM_lrr_27.nii and mask mask27.nii\n",
      "Appending arrays with segmentation 28_roi_combined.nii QSM, 00028_qsm.nii.gz LARO, QSM_lrr_28.nii and mask mask28.nii\n",
      "Appending arrays with segmentation 29_roi_combined.nii QSM, 00029_qsm.nii.gz LARO, QSM_lrr_29.nii and mask mask29.nii\n",
      "Appending arrays with segmentation 30_roi_combined.nii QSM, 00030_qsm.nii.gz LARO, QSM_lrr_30.nii and mask mask30.nii\n",
      "Appending arrays with segmentation 31_roi_combined.nii QSM, 00031_qsm.nii.gz LARO, QSM_lrr_31.nii and mask mask31.nii\n",
      "Appending arrays with segmentation 32_roi_combined.nii QSM, 00032_qsm.nii.gz LARO, QSM_lrr_32.nii and mask mask32.nii\n",
      "Appending arrays with segmentation 33_roi_combined.nii QSM, 00033_qsm.nii.gz LARO, QSM_lrr_33.nii and mask mask33.nii\n",
      "Appending arrays with segmentation 34_roi_combined.nii QSM, 00034_qsm.nii.gz LARO, QSM_lrr_34.nii and mask mask34.nii\n",
      "Appending arrays with segmentation 35_roi_combined.nii QSM, 00035_qsm.nii.gz LARO, QSM_lrr_35.nii and mask mask35.nii\n",
      "Appending arrays with segmentation 36_roi_combined.nii QSM, 00036_qsm.nii.gz LARO, QSM_lrr_36.nii and mask mask36.nii\n",
      "Appending arrays with segmentation 37_roi_combined.nii QSM, 00037_qsm.nii.gz LARO, QSM_lrr_37.nii and mask mask37.nii\n",
      "Appending arrays with segmentation 38_roi_combined.nii QSM, 00038_qsm.nii.gz LARO, QSM_lrr_38.nii and mask mask38.nii\n",
      "Appending arrays with segmentation 39_roi_combined.nii QSM, 00039_qsm.nii.gz LARO, QSM_lrr_39.nii and mask mask39.nii\n",
      "Appending arrays with segmentation 40_roi_combined.nii QSM, 00040_qsm.nii.gz LARO, QSM_lrr_40.nii and mask mask40.nii\n"
     ]
    }
   ],
   "source": [
    "# Set window level\n",
    "level = 0\n",
    "window = 500\n",
    "m1=level-window/2\n",
    "m2=level+window/2\n",
    "visualize = 1\n",
    "# Load data\n",
    "nrows = 256\n",
    "ncols = 256\n",
    "nslices = 160\n",
    "segs = []\n",
    "qsms = []\n",
    "laros = []\n",
    "voxel_sizes = []\n",
    "trackers = []\n",
    "q_directory = '/home/ali/RadDBS-QSM/data/nii/chh/orig/qsm/'\n",
    "q_directory = os.listdir(q_directory)\n",
    "q_directory = sorted(q_directory)\n",
    "qu_directory = '/home/ali/RadDBS-QSM/data/nii/chh/lr_roi_ft'\n",
    "qu_directory = os.listdir(qu_directory)\n",
    "qu_directory = sorted(qu_directory)\n",
    "s_directory = '/home/ali/RadDBS-QSM/data/nii/chh/orig/seg/'\n",
    "s_directory = os.listdir(s_directory)\n",
    "s_directory = sorted(s_directory)\n",
    "m_directory = '/home/ali/RadDBS-QSM/data/nii/chh/masks'\n",
    "m_directory = os.listdir(m_directory)\n",
    "m_directory = sorted(m_directory)\n",
    "case_list = []\n",
    "d_count = 0\n",
    "if visualize == 1:\n",
    "    for filename in q_directory:\n",
    "    \n",
    "        seg_filename = s_directory[d_count]\n",
    "        laro_filename = qu_directory[d_count]\n",
    "        mask_filename = m_directory[d_count]\n",
    "        seg = nib.load('/home/ali/RadDBS-QSM/data/nii/chh/orig/seg/'+seg_filename)\n",
    "        mask = nib.load('/home/ali/RadDBS-QSM/data/nii/chh/masks/'+mask_filename)\n",
    "        voxel_size = seg.header['pixdim'][0:3]\n",
    "        voxel_sizes.append(voxel_size)\n",
    "        segs.append(seg.get_fdata()[:nrows,:ncols,:nslices])\n",
    "        qsm = nib.load('/home/ali/RadDBS-QSM/data/nii/chh/orig/qsm/'+filename)\n",
    "        qsms.append(qsm.get_fdata()[:nrows,:ncols,:nslices])\n",
    "\n",
    "        laro = nib.load('/home/ali/RadDBS-QSM/data/nii/chh/lr_roi_ft/'+laro_filename)\n",
    "        laros.append(1000*laro.get_fdata()[:nrows,:ncols,:nslices])\n",
    "        print('Appending arrays with segmentation',seg_filename,'QSM,',filename,\n",
    "              'LARO,',laro_filename,'and mask',mask_filename)\n",
    "        case_list.append(filename)\n",
    "        n_cases = len(segs)\n",
    "        d_count = d_count+1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjects</th>\n",
       "      <th>LCT</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>UPDRSⅢ</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Preoperative Off-Medicine</td>\n",
       "      <td>Preoperative On-Medicine</td>\n",
       "      <td>Postoperative  Off-Medicine &amp;Off-Stimulation\\n</td>\n",
       "      <td>Postoperative  Off-Medicine &amp;On-Stimulation</td>\n",
       "      <td>Postoperative  On-Medicine &amp;On-Stimulation</td>\n",
       "      <td>age</td>\n",
       "      <td>Gender(male=0,female=1)</td>\n",
       "      <td>Disease duration(years)</td>\n",
       "      <td>LEDD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>62.26%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>75.00%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>856.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>51.72%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>28</td>\n",
       "      <td>52</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>22.58%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62</td>\n",
       "      <td>48</td>\n",
       "      <td>61</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>55.88%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>751.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>75.44%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57</td>\n",
       "      <td>14</td>\n",
       "      <td>55</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>84.00%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>68</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1087.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>41.86%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43</td>\n",
       "      <td>25</td>\n",
       "      <td>48</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>72.58%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62</td>\n",
       "      <td>17</td>\n",
       "      <td>56</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>81.25%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>70.21%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94</td>\n",
       "      <td>28</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>43</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>63.95%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>89.55%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>856.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.0</td>\n",
       "      <td>88.46%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>62.50%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>27</td>\n",
       "      <td>72</td>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.0</td>\n",
       "      <td>48.00%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.0</td>\n",
       "      <td>14.29%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>84</td>\n",
       "      <td>86</td>\n",
       "      <td>43</td>\n",
       "      <td>34</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.0</td>\n",
       "      <td>36.62%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>47</td>\n",
       "      <td>46</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.0</td>\n",
       "      <td>54.90%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.0</td>\n",
       "      <td>81.63%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.0</td>\n",
       "      <td>71.43%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1125.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.0</td>\n",
       "      <td>90.32%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.0</td>\n",
       "      <td>65.85%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "      <td>43</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>862.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24.0</td>\n",
       "      <td>74.29%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>341.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>83.61%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26.0</td>\n",
       "      <td>58.97%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.0</td>\n",
       "      <td>5.88%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28.0</td>\n",
       "      <td>76.67%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>68</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29.0</td>\n",
       "      <td>81.01%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79</td>\n",
       "      <td>15</td>\n",
       "      <td>79</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30.0</td>\n",
       "      <td>62.35%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>32</td>\n",
       "      <td>92</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31.0</td>\n",
       "      <td>27.91%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32.0</td>\n",
       "      <td>31.48%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>37</td>\n",
       "      <td>64</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33.0</td>\n",
       "      <td>80.33%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34.0</td>\n",
       "      <td>40.74%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>36</td>\n",
       "      <td>29</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>656.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35.0</td>\n",
       "      <td>16.13%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93</td>\n",
       "      <td>78</td>\n",
       "      <td>69</td>\n",
       "      <td>47</td>\n",
       "      <td>44</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36.0</td>\n",
       "      <td>59.26%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>22</td>\n",
       "      <td>57</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37.0</td>\n",
       "      <td>61.40%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57</td>\n",
       "      <td>22</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38.0</td>\n",
       "      <td>95.92%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39.0</td>\n",
       "      <td>79.35%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40.0</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>27</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subjects    LCT   Unnamed: 2                     UPDRSⅢ  \\\n",
       "0        NaN     NaN         NaN  Preoperative Off-Medicine   \n",
       "1        1.0  62.26%         NaN                         53   \n",
       "2        2.0  75.00%         NaN                         32   \n",
       "3        3.0  51.72%         NaN                         58   \n",
       "4        4.0  22.58%         NaN                         62   \n",
       "5        5.0  55.88%         NaN                         34   \n",
       "6        6.0  75.44%         NaN                         57   \n",
       "7        7.0  84.00%         NaN                        100   \n",
       "8        8.0  41.86%         NaN                         43   \n",
       "9        9.0  72.58%         NaN                         62   \n",
       "10      10.0  81.25%         NaN                         64   \n",
       "11      11.0  70.21%         NaN                         94   \n",
       "12      12.0  63.95%         NaN                         86   \n",
       "13      13.0  89.55%         NaN                         67   \n",
       "14      14.0  88.46%         NaN                         26   \n",
       "15      15.0  62.50%         NaN                         72   \n",
       "16      16.0  48.00%         NaN                         25   \n",
       "17      17.0  14.29%         NaN                         98   \n",
       "18      18.0  36.62%         NaN                         71   \n",
       "19      19.0  54.90%         NaN                         51   \n",
       "20      20.0  81.63%         NaN                         49   \n",
       "21      21.0  71.43%         NaN                         49   \n",
       "22      22.0  90.32%         NaN                         31   \n",
       "23      23.0  65.85%         NaN                         41   \n",
       "24      24.0  74.29%         NaN                         35   \n",
       "25      25.0  83.61%         NaN                         61   \n",
       "26      26.0  58.97%         NaN                         78   \n",
       "27      27.0   5.88%         NaN                         51   \n",
       "28      28.0  76.67%         NaN                         60   \n",
       "29      29.0  81.01%         NaN                         79   \n",
       "30      30.0  62.35%         NaN                         85   \n",
       "31      31.0  27.91%         NaN                         43   \n",
       "32      32.0  31.48%         NaN                         54   \n",
       "33      33.0  80.33%         NaN                         61   \n",
       "34      34.0  40.74%         NaN                         54   \n",
       "35      35.0  16.13%         NaN                         93   \n",
       "36      36.0  59.26%         NaN                         54   \n",
       "37      37.0  61.40%         NaN                         57   \n",
       "38      38.0  95.92%         NaN                         49   \n",
       "39      39.0  79.35%         NaN                         92   \n",
       "40      40.0  50.00%         NaN                         54   \n",
       "\n",
       "                  Unnamed: 4                                       Unnamed: 5  \\\n",
       "0   Preoperative On-Medicine  Postoperative  Off-Medicine &Off-Stimulation\\n    \n",
       "1                         20                                               40   \n",
       "2                          8                                               26   \n",
       "3                         28                                               52   \n",
       "4                         48                                               61   \n",
       "5                         15                                               44   \n",
       "6                         14                                               55   \n",
       "7                         16                                               68   \n",
       "8                         25                                               48   \n",
       "9                         17                                               56   \n",
       "10                        12                                               33   \n",
       "11                        28                                               31   \n",
       "12                        31                                               28   \n",
       "13                         7                                               64   \n",
       "14                         3                                               31   \n",
       "15                        27                                               72   \n",
       "16                        13                                               17   \n",
       "17                        84                                               86   \n",
       "18                        45                                               49   \n",
       "19                        23                                               37   \n",
       "20                         9                                               20   \n",
       "21                        14                                               42   \n",
       "22                         3                                               46   \n",
       "23                        14                                               43   \n",
       "24                         9                                               21   \n",
       "25                        10                                               60   \n",
       "26                        32                                               64   \n",
       "27                        48                                               45   \n",
       "28                        14                                               68   \n",
       "29                        15                                               79   \n",
       "30                        32                                               92   \n",
       "31                        31                                               31   \n",
       "32                        37                                               64   \n",
       "33                        12                                               53   \n",
       "34                        32                                               64   \n",
       "35                        78                                               69   \n",
       "36                        22                                               57   \n",
       "37                        22                                               56   \n",
       "38                         2                                               58   \n",
       "39                        19                                              NaN   \n",
       "40                        27                                               34   \n",
       "\n",
       "                                     Unnamed: 6  \\\n",
       "0   Postoperative  Off-Medicine &On-Stimulation   \n",
       "1                                            15   \n",
       "2                                            17   \n",
       "3                                            18   \n",
       "4                                            36   \n",
       "5                                            29   \n",
       "6                                            32   \n",
       "7                                            12   \n",
       "8                                            26   \n",
       "9                                            25   \n",
       "10                                           12   \n",
       "11                                           34   \n",
       "12                                           27   \n",
       "13                                           19   \n",
       "14                                           18   \n",
       "15                                           44   \n",
       "16                                            8   \n",
       "17                                           43   \n",
       "18                                           47   \n",
       "19                                           21   \n",
       "20                                            3   \n",
       "21                                            8   \n",
       "22                                           28   \n",
       "23                                           24   \n",
       "24                                           15   \n",
       "25                                           21   \n",
       "26                                           29   \n",
       "27                                           27   \n",
       "28                                           43   \n",
       "29                                           32   \n",
       "30                                           40   \n",
       "31                                            6   \n",
       "32                                           28   \n",
       "33                                           21   \n",
       "34                                           36   \n",
       "35                                           47   \n",
       "36                                           12   \n",
       "37                                           11   \n",
       "38                                           14   \n",
       "39                                           63   \n",
       "40                                            9   \n",
       "\n",
       "                                    Unnamed: 7 Unnamed: 8  \\\n",
       "0   Postoperative  On-Medicine &On-Stimulation        age   \n",
       "1                                            7         66   \n",
       "2                                           10         64   \n",
       "3                                           20         67   \n",
       "4                                           28         69   \n",
       "5                                           22         63   \n",
       "6                                           16         67   \n",
       "7                                          NaN         58   \n",
       "8                                           26         74   \n",
       "9                                           11         57   \n",
       "10                                         NaN         52   \n",
       "11                                          43         68   \n",
       "12                                          18         56   \n",
       "13                                         NaN         60   \n",
       "14                                          17         62   \n",
       "15                                          38         56   \n",
       "16                                           8         66   \n",
       "17                                          34         73   \n",
       "18                                          46         52   \n",
       "19                                          18         68   \n",
       "20                                         NaN         59   \n",
       "21                                         NaN         62   \n",
       "22                                         NaN         65   \n",
       "23                                          24         75   \n",
       "24                                         NaN         47   \n",
       "25                                          17         63   \n",
       "26                                          21         70   \n",
       "27                                          20         59   \n",
       "28                                         NaN         67   \n",
       "29                                          28         66   \n",
       "30                                         NaN         72   \n",
       "31                                         NaN         71   \n",
       "32                                          25         51   \n",
       "33                                          15         54   \n",
       "34                                          29         66   \n",
       "35                                          44         70   \n",
       "36                                         NaN         49   \n",
       "37                                         NaN         76   \n",
       "38                                          12         67   \n",
       "39                                         NaN         56   \n",
       "40                                           8         69   \n",
       "\n",
       "                 Unnamed: 9              Unnamed: 10 Unnamed: 11  \n",
       "0   Gender(male=0,female=1)  Disease duration(years)        LEDD  \n",
       "1                         1                        9         825  \n",
       "2                         1                        6      856.25  \n",
       "3                         1                       15        1350  \n",
       "4                         1                        5         350  \n",
       "5                         1                       10      751.25  \n",
       "6                         1                        5         525  \n",
       "7                         0                       10      1087.5  \n",
       "8                         1                       10         550  \n",
       "9                         1                       10         500  \n",
       "10                        0                        6         600  \n",
       "11                        1                       11        1450  \n",
       "12                        1                       13         600  \n",
       "13                        1                       11      856.25  \n",
       "14                        0                        5         375  \n",
       "15                        0                       12         400  \n",
       "16                        0                        8         850  \n",
       "17                        1                       15         300  \n",
       "18                        1                        7        1200  \n",
       "19                        1                       13         575  \n",
       "20                        0                       11         700  \n",
       "21                        0                        9     1125.75  \n",
       "22                        1                        7         875  \n",
       "23                        0                       10       862.5  \n",
       "24                        1                        7      341.67  \n",
       "25                        0                        9         455  \n",
       "26                        0                        5         850  \n",
       "27                        0                        6         525  \n",
       "28                        0                        7        1025  \n",
       "29                        0                        7         750  \n",
       "30                        0                        7        1075  \n",
       "31                        0                       12        1025  \n",
       "32                        0                        5         450  \n",
       "33                        0                        7         675  \n",
       "34                        0                        9      656.25  \n",
       "35                        0                        5         NaN  \n",
       "36                        0                        6         575  \n",
       "37                        1                        4         375  \n",
       "38                        0                        7         510  \n",
       "39                        1                       10        1100  \n",
       "40                        0                        7         450  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/ali/RadDBS-QSM/data/xlxs/updrs_iii_chh_cvs.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 01_roi_combined.nii for case 1.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 02_roi_combined.nii for case 2.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 03_roi_combined.nii for case 3.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 04_roi_combined.nii for case 4.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 05_roi_combined.nii for case 5.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 06_roi_combined.nii for case 6.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 07_roi_combined.nii for case 7.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 08_roi_combined.nii for case 8.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 09_roi_combined.nii for case 9.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 10_roi_combined.nii for case 10.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 11_roi_combined.nii for case 11.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 13_roi_combined.nii for case 13.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 14_roi_combined.nii for case 14.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 16_roi_combined.nii for case 16.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 18_roi_combined.nii for case 18.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 19_roi_combined.nii for case 19.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 20_roi_combined.nii for case 20.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 21_roi_combined.nii for case 21.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 22_roi_combined.nii for case 22.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 23_roi_combined.nii for case 23.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 24_roi_combined.nii for case 24.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 25_roi_combined.nii for case 25.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 26_roi_combined.nii for case 26.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 27_roi_combined.nii for case 27.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 28_roi_combined.nii for case 28.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 29_roi_combined.nii for case 29.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 30_roi_combined.nii for case 30.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 31_roi_combined.nii for case 31.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 32_roi_combined.nii for case 32.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 33_roi_combined.nii for case 33.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 34_roi_combined.nii for case 34.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 35_roi_combined.nii for case 35.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 36_roi_combined.nii for case 36.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 37_roi_combined.nii for case 37.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 38_roi_combined.nii for case 38.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 39_roi_combined.nii for case 39.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 40_roi_combined.nii for case 40.0\n"
     ]
    }
   ],
   "source": [
    "# Patient IDs\n",
    "subject_id = np.asarray(df[df.columns[0]])[1:]\n",
    "\n",
    "# Only extract ROI if it is present in all cases\n",
    "seg_labels_all = segs[0]\n",
    "case_number = np.zeros_like(np.asarray(s_directory))\n",
    "for i in range(n_cases):\n",
    "    case_number[i] = float(s_directory[i][:2])\n",
    "subject_id_corr = subject_id[np.in1d(subject_id,case_number)]\n",
    "age = np.nan_to_num(np.asarray(df[df.columns[-4]])[1:][np.in1d(subject_id,case_number)].astype(float))\n",
    "sex = np.nan_to_num(np.asarray(df[df.columns[-3]])[1:][np.in1d(subject_id,case_number)].astype(float))\n",
    "dd = np.nan_to_num(np.asarray(df[df.columns[-2]])[1:][np.in1d(subject_id,case_number)].astype(float))\n",
    "ledd = np.nan_to_num(np.asarray(df[df.columns[-1]])[1:][np.in1d(subject_id,case_number)].astype(float))\n",
    "\n",
    "for i in np.arange(n_cases):\n",
    "    try:\n",
    "        print('Found ROIs',str(np.unique(segs[i])),'at segmentation directory file',s_directory[i],'for case',str(subject_id_corr[i]))\n",
    "    except:\n",
    "        print('Case',subject_id[i],'quarantined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_updrs_iii_off =  np.asarray(df[df.columns[3]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])                                \n",
    "pre_updrs_iii_on =  np.asarray(df[df.columns[4]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])\n",
    "post_updrs_iii_off =  np.asarray(df[df.columns[6]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])\n",
    "\n",
    "per_change = (np.asarray(pre_updrs_iii_off).astype(float)-np.asarray(post_updrs_iii_off).astype(float))/(np.asarray(pre_updrs_iii_off).astype(float))\n",
    "lct_change = (np.asarray(pre_updrs_iii_off).astype(float)-(np.asarray(pre_updrs_iii_on)).astype(float))/(np.asarray(pre_updrs_iii_off).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"radiomics\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "reextract = 0\n",
    "\n",
    "# Assume all voxel sizes are identical\n",
    "voxel_size = (0.9,0.9,0.9)\n",
    "if reextract == 1:\n",
    "    # Generate feature structure Phi from all ROIs and all cases\n",
    "    extractor = featureextractor.RadiomicsFeatureExtractor()\n",
    "    extractor.enableAllFeatures()\n",
    "    extractor.enableAllImageTypes()\n",
    "    extractor.enableFeatureClassByName('shape2D',enabled = False)\n",
    "\n",
    "    seg_labels_all = np.unique(np.asarray(segs))\n",
    "    Phi_gt = []\n",
    "    Phi_vd = []\n",
    "    Phi_lr = []\n",
    "    seg_labels = []\n",
    "    reextract = 0\n",
    "    x_row_gt = []\n",
    "    x_row_lr = []\n",
    "\n",
    "    keylib = []\n",
    "    roilib = []\n",
    "    loop_count = 1\n",
    "    n_rois = seg_labels_all[seg_labels_all>0].__len__()\n",
    "    roi_names = np.asarray(['Background','Right substantia nigra','Right subthalamic nucleus',\n",
    "                            'Left subthalamic nucleus', 'Left substantia nigra', 'Right dentate nucleus', 'Left dentate nucleus'])\n",
    "    for i in np.arange(subject_id_corr.__len__()):\n",
    "        seg_sitk = sitk.GetImageFromArray(segs[i])\n",
    "        seg_sitk.SetSpacing(voxel_size)\n",
    "        qsm_sitk_gt = sitk.GetImageFromArray(qsms[i])\n",
    "        qsm_sitk_gt.SetSpacing(voxel_size)\n",
    "        qsm_sitk_lr = sitk.GetImageFromArray(laros[i])\n",
    "        qsm_sitk_lr.SetSpacing(voxel_size)\n",
    "        # Index back since subject 12 is missing ROIs\n",
    "        for j in seg_labels_all:\n",
    "            if j>0:\n",
    "                fv_count = 0\n",
    "                featureVector_gt = extractor.execute(qsm_sitk_gt,seg_sitk,label=int(j));\n",
    "                featureVector_lr = extractor.execute(qsm_sitk_lr,seg_sitk,label=int(j));\n",
    "                Phi_gt.append(featureVector_gt)\n",
    "                Phi_lr.append(featureVector_lr)\n",
    "                for key, value in six.iteritems(featureVector_gt):\n",
    "                    if 'diagnostic' in key:\n",
    "                        next\n",
    "                    else:\n",
    "                        x_row_gt.append(featureVector_gt[key])\n",
    "                        x_row_lr.append(featureVector_lr[key])\n",
    "                        fv_count = fv_count+1\n",
    "                        keylib.append(key)\n",
    "                        roilib.append(roi_names[int(j)])\n",
    "                x_row_gt.append(pre_updrs_iii_off[i])\n",
    "                x_row_lr.append(pre_updrs_iii_off[i])\n",
    "                fv_count = fv_count+1\n",
    "        print('Extracting features for subject',subject_id_corr[i],'and appending feature matrix with vector of length',fv_count,'with UPDRS score',pre_updrs_iii_off[i])\n",
    "                \n",
    "    X0_gt = np.array(x_row_gt)\n",
    "    X0_lr = np.array(x_row_lr)\n",
    "    np.save('/home/ali/RadDBS-QSM/data/npy/rp/X0_gt_chh_rois_rp.npy',X0_gt)\n",
    "    np.save('/home/ali/RadDBS-QSM/data/npy/rp/X0_lr_chh_rois_rp.npy',X0_lr)\n",
    "\n",
    "    K = np.asarray(keylib)\n",
    "    R = np.asarray(roi_names)\n",
    "    np.save('/home/ali/RadDBS-QSM/data/npy/rp/K_chh_rp.npy',K)\n",
    "    np.save('/home/ali/RadDBS-QSM/data/npy/rp/R_chh_rp.npy',R)\n",
    "\n",
    "    print('Saving ground truth feature vector')\n",
    "    with open('/home/ali/RadDBS-QSM/data/npy/rp/Phi_mcl_gt_roi_chh_rp', 'wb') as fp:  \n",
    "        pickle.dump(Phi_gt, fp)\n",
    "    \n",
    "    print('Saving undersampled feature vector')\n",
    "    with open('/home/ali/RadDBS-QSM/data/npy/rp/Phi_mcl_lr_roi_chh_rp', 'wb') as fp:  \n",
    "        pickle.dump(Phi_lr, fp)\n",
    "\n",
    "else:\n",
    "    X0_gt = np.load('/home/ali/RadDBS-QSM/data/npy/rp/X0_gt_chh_rois_rp.npy')\n",
    "    X0_lr = np.load('/home/ali/RadDBS-QSM/data/npy/rp/X0_lr_chh_rois_rp.npy')\n",
    "    K = np.load('/home/ali/RadDBS-QSM/data/npy/rp/K_chh_rp.npy')\n",
    "    R = np.load('/home/ali/RadDBS-QSM/data/npy/rp/R_chh_rp.npy')\n",
    "    n_rois = R.shape[0]-1\n",
    "    with open('/home/ali/RadDBS-QSM/data/npy/rp/Phi_mcl_gt_roi_chh_rp', \"rb\") as fp:  \n",
    "        Phi_gt = pickle.load(fp)\n",
    "    \n",
    "    with open('/home/ali/RadDBS-QSM/data/npy/rp/Phi_mcl_lr_roi_chh_rp', \"rb\") as fp:  \n",
    "        Phi_lr = pickle.load(fp)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1596\n",
    "n_rois = 6\n",
    "X_all_c = X0_gt.reshape(n_cases,n_rois,n_features)[:,0:4,:]\n",
    "K_all_c = K.reshape(n_cases,n_rois,n_features-1)[:,0:4,:]\n",
    "K_all_c = np.char.add(K_all_c[0,:,:].reshape(-1,1),' ')\n",
    "R_all_c = np.repeat(R[1:5],n_features-1)\n",
    "K_all_c = np.char.add(np.squeeze(K_all_c),np.squeeze(R_all_c))\n",
    "K_all_c = np.append(K_all_c,['pre updrs']*5)\n",
    "R_all_c = np.append(R_all_c,['pre updrs']*5)\n",
    "K_all_c = np.append(K_all_c,['age'])\n",
    "R_all_c = np.append(R_all_c,['age'])\n",
    "K_all_c = np.append(K_all_c,['disease duration'])\n",
    "R_all_c = np.append(R_all_c,['disease duration'])\n",
    "K_all_c = np.append(K_all_c,['sex'])\n",
    "R_all_c = np.append(R_all_c,['sex'])\n",
    "# K_all_c = np.append(K_all_c,['ledd'])\n",
    "# R_all_c = np.append(R_all_c,['ledd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cases = len(per_change)\n",
    "n_roisc = Phi_gt.__len__()/n_cases\n",
    "L = int(len(X0_gt)/n_cases)\n",
    "n_features = int(L/n_rois)\n",
    "subsc = subject_id_corr\n",
    "pre_updrs_off = pre_updrs_iii_off\n",
    "results_ls_aug = np.zeros_like(per_change)\n",
    "\n",
    "results_lgr_aug = np.zeros_like(per_change)\n",
    "results_lgrp_aug = np.zeros_like(per_change)\n",
    "results_ls = np.zeros_like(per_change)\n",
    "results_lgr = np.zeros_like(per_change)\n",
    "results_lgrp = np.zeros_like(per_change)\n",
    "r = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution of: 0.5669595264848581 0.20858808082526412 -0.24474920851943865\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53533552] from dataset of size (228, 3388)\n",
      "LassoCV score for 3 is [0.53533552] from dataset of size (228, 388)\n",
      "LassoCV score for 4 is [0.5350341] from dataset of size (228, 6388)\n",
      "LassoCV score for 5 is [0.52731496] from dataset of size (228, 2388)\n",
      "Lasso predicts 0.5941 and logistic regression predicts 0.0 for case with 0.72 and selected CV 3 and 72 minority cases\n",
      "Label distribution of: 0.5738548357091768 0.20932917254901903 -0.3279210191356042\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53648174] from dataset of size (228, 6388)\n",
      "LassoCV score for 3 is [0.53648174] from dataset of size (228, 6388)\n",
      "LassoCV score for 4 is [0.53587512] from dataset of size (228, 6388)\n",
      "LassoCV score for 5 is [0.52858127] from dataset of size (228, 1388)\n",
      "Lasso predicts 0.3965 and logistic regression predicts 0.0 for case with 0.47 and selected CV 2 and 69 minority cases\n",
      "Label distribution of: 0.5677185809199048 0.20908232478622193 -0.24926099868801613\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53527112] from dataset of size (228, 5388)\n",
      "LassoCV score for 3 is [0.53527112] from dataset of size (228, 2388)\n",
      "LassoCV score for 4 is [0.53485128] from dataset of size (228, 5388)\n",
      "LassoCV score for 5 is [0.52947576] from dataset of size (228, 1388)\n",
      "Lasso predicts 0.7361 and logistic regression predicts 0.0 for case with 0.69 and selected CV 3 and 72 minority cases\n",
      "Label distribution of: 0.5752269235227969 0.2084721065940771 -0.34398027688109245\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53718551] from dataset of size (228, 388)\n",
      "LassoCV score for 3 is [0.53718551] from dataset of size (228, 5388)\n",
      "LassoCV score for 4 is [0.53667589] from dataset of size (228, 2388)\n",
      "LassoCV score for 5 is [0.52958503] from dataset of size (228, 1388)\n",
      "Lasso predicts 0.5768 and logistic regression predicts 0.0 for case with 0.42 and selected CV 2 and 69 minority cases\n",
      "Label distribution of: 0.5827907017222488 0.19744836811095323 -0.25025976222641483\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "Resampled to size (39,)\n",
      "LassoCV score for 2 is [0.64284055] from dataset of size (234, 1388)\n",
      "LassoCV score for 3 is [0.64284055] from dataset of size (234, 1388)\n",
      "LassoCV score for 4 is [0.64267455] from dataset of size (234, 1388)\n",
      "LassoCV score for 5 is [0.63363152] from dataset of size (234, 1388)\n",
      "Lasso predicts 0.3676 and logistic regression predicts 0.0 for case with 0.15 and selected CV 2 and 57 minority cases\n",
      "Label distribution of: 0.5746924331750638 0.20884605172212908 -0.33822865445502964\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53689135] from dataset of size (228, 1388)\n",
      "LassoCV score for 3 is [0.53689135] from dataset of size (228, 5388)\n",
      "LassoCV score for 4 is [0.53513706] from dataset of size (228, 1388)\n",
      "LassoCV score for 5 is [0.52556374] from dataset of size (228, 1388)\n",
      "Lasso predicts 0.3942 and logistic regression predicts 0.0 for case with 0.44 and selected CV 2 and 69 minority cases\n",
      "Label distribution of: 0.5624312245980657 0.20344924934212058 -0.28703446866429055\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53677467] from dataset of size (228, 5388)\n",
      "LassoCV score for 3 is [0.53672815] from dataset of size (228, 388)\n",
      "LassoCV score for 4 is [0.53532263] from dataset of size (228, 3388)\n",
      "LassoCV score for 5 is [0.53055988] from dataset of size (228, 2388)\n",
      "Lasso predicts 0.6627 and logistic regression predicts 0.0 for case with 0.88 and selected CV 2 and 70 minority cases\n",
      "Label distribution of: 0.5758937568978073 0.20793336707043802 -0.35000008336051325\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53758905] from dataset of size (228, 3388)\n",
      "LassoCV score for 3 is [0.53758938] from dataset of size (228, 6388)\n",
      "LassoCV score for 4 is [0.53627779] from dataset of size (228, 3388)\n",
      "LassoCV score for 5 is [0.52879922] from dataset of size (228, 2388)\n",
      "Lasso predicts 0.3141 and logistic regression predicts 0.0 for case with 0.4 and selected CV 3 and 69 minority cases\n",
      "Label distribution of: 0.5702986081106105 0.20999601422390549 -0.27751686488556404\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53540525] from dataset of size (228, 1388)\n",
      "LassoCV score for 3 is [0.53540525] from dataset of size (228, 1388)\n",
      "LassoCV score for 4 is [0.53434282] from dataset of size (228, 3388)\n",
      "LassoCV score for 5 is [0.5250312] from dataset of size (228, 1388)\n",
      "Lasso predicts 0.4788 and logistic regression predicts 0.0 for case with 0.6 and selected CV 2 and 71 minority cases\n",
      "Label distribution of: 0.5643062245980658 0.2060398064467874 -0.25159784666703033\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53594917] from dataset of size (228, 388)\n",
      "LassoCV score for 3 is [0.53594917] from dataset of size (228, 5388)\n",
      "LassoCV score for 4 is [0.53482255] from dataset of size (228, 6388)\n",
      "LassoCV score for 5 is [0.53216865] from dataset of size (228, 388)\n",
      "Lasso predicts 0.4032 and logistic regression predicts 1.0 for case with 0.81 and selected CV 2 and 72 minority cases\n",
      "Label distribution of: 0.5691451725886095 0.20973322323580731 -0.2629533781637346\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53527844] from dataset of size (228, 6388)\n",
      "LassoCV score for 3 is [0.53527844] from dataset of size (228, 2388)\n",
      "LassoCV score for 4 is [0.53433383] from dataset of size (228, 388)\n",
      "LassoCV score for 5 is [0.53357486] from dataset of size (228, 1388)\n",
      "Lasso predicts 0.6984 and logistic regression predicts 0.0 for case with 0.64 and selected CV 2 and 71 minority cases\n",
      "Label distribution of: 0.5669751715300725 0.2085993111100245 -0.24481874200363993\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.5353337] from dataset of size (228, 3388)\n",
      "LassoCV score for 3 is [0.53489845] from dataset of size (228, 1388)\n",
      "LassoCV score for 4 is [0.53456945] from dataset of size (228, 4388)\n",
      "LassoCV score for 5 is [0.528488] from dataset of size (228, 1388)\n",
      "Lasso predicts 0.5315 and logistic regression predicts 0.0 for case with 0.72 and selected CV 2 and 72 minority cases\n",
      "Label distribution of: 0.5783286604955017 0.20527469945818264 -0.35630765675696446\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53943507] from dataset of size (228, 388)\n",
      "LassoCV score for 3 is [0.53943507] from dataset of size (228, 1388)\n",
      "LassoCV score for 4 is [0.5389054] from dataset of size (228, 6388)\n",
      "LassoCV score for 5 is [0.53356193] from dataset of size (228, 3388)\n",
      "Lasso predicts 0.5297 and logistic regression predicts 0.0 for case with 0.31 and selected CV 2 and 68 minority cases\n",
      "Label distribution of: 0.5679867801536214 0.20923231991769523 -0.25136542100761594\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53525979] from dataset of size (228, 6388)\n",
      "LassoCV score for 3 is [0.53525979] from dataset of size (228, 388)\n",
      "LassoCV score for 4 is [0.53376979] from dataset of size (228, 3388)\n",
      "LassoCV score for 5 is [0.52244534] from dataset of size (228, 1388)\n",
      "Lasso predicts 0.4156 and logistic regression predicts 1.0 for case with 0.68 and selected CV 2 and 72 minority cases\n",
      "Label distribution of: 0.5774859976810078 0.2063190239320207 -0.3574795129443696\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53872749] from dataset of size (228, 6388)\n",
      "LassoCV score for 3 is [0.53872749] from dataset of size (228, 6388)\n",
      "LassoCV score for 4 is [0.53834482] from dataset of size (228, 6388)\n",
      "LassoCV score for 5 is [0.53253513] from dataset of size (228, 388)\n",
      "Lasso predicts 0.2701 and logistic regression predicts 1.0 for case with 0.34 and selected CV 2 and 69 minority cases\n",
      "Label distribution of: 0.5705357997614644 0.21002096073970725 -0.2807677568009367\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53544468] from dataset of size (228, 5388)\n",
      "LassoCV score for 3 is [0.53544468] from dataset of size (228, 4388)\n",
      "LassoCV score for 4 is [0.53437934] from dataset of size (228, 3388)\n",
      "LassoCV score for 5 is [0.52717187] from dataset of size (228, 1388)\n",
      "Lasso predicts 0.3534 and logistic regression predicts 0.0 for case with 0.59 and selected CV 2 and 71 minority cases\n",
      "Label distribution of: 0.5607985715368413 0.2006390097129064 -0.34802680409786524\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53779034] from dataset of size (228, 5388)\n",
      "LassoCV score for 3 is [0.53779034] from dataset of size (228, 3388)\n",
      "LassoCV score for 4 is [0.53557543] from dataset of size (228, 2388)\n",
      "LassoCV score for 5 is [0.52943829] from dataset of size (228, 1388)\n",
      "Lasso predicts 0.5696 and logistic regression predicts 0.0 for case with 0.94 and selected CV 2 and 69 minority cases\n",
      "Label distribution of: 0.5636330386570226 0.20518643494540414 -0.26076044933979126\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.5362061] from dataset of size (228, 5388)\n",
      "LassoCV score for 3 is [0.5362061] from dataset of size (228, 2388)\n",
      "LassoCV score for 4 is [0.53563626] from dataset of size (228, 4388)\n",
      "LassoCV score for 5 is [0.52978271] from dataset of size (228, 2388)\n",
      "Lasso predicts 0.5884 and logistic regression predicts 0.0 for case with 0.84 and selected CV 2 and 72 minority cases\n",
      "Label distribution of: 0.5841874969994993 0.19415564179220104 -0.16306203646880674\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "Resampled to size (39,)\n",
      "LassoCV score for 2 is [0.63448547] from dataset of size (234, 4388)\n",
      "LassoCV score for 3 is [0.63448547] from dataset of size (234, 4388)\n",
      "LassoCV score for 4 is [0.63435365] from dataset of size (234, 2388)\n",
      "LassoCV score for 5 is [0.62956165] from dataset of size (234, 3388)\n",
      "Lasso predicts 0.2208 and logistic regression predicts 0.0 for case with 0.1 and selected CV 3 and 57 minority cases\n",
      "Label distribution of: 0.5753580538663584 0.20837251231034953 -0.34527291849868297\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.53726166] from dataset of size (228, 5388)\n",
      "LassoCV score for 3 is [0.53726166] from dataset of size (228, 3388)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Noise compensated LASSO\n",
    "K_nz_nc = []\n",
    "K_nz_nc_lr = []\n",
    "E_nz_nc = []\n",
    "E_nz_nc_lr = []\n",
    "R_nz_cd = []\n",
    "if retrain == 1:\n",
    "\n",
    "  aug = True\n",
    "  K_nz = []\n",
    "  c = 0\n",
    "\n",
    "  for j in np.arange(c,len(subsc)):\n",
    "      test_id = subsc[j]\n",
    "      test_index = subsc == test_id\n",
    "      train_index = subsc != test_id\n",
    "      X_train = X_all_c[train_index,:,:]\n",
    "      X_test = X_all_c[test_index,:,:]\n",
    "      y_train0 = per_change[train_index]\n",
    "      y_test = per_change[test_index]\n",
    "\n",
    "      y_cat = y_train0 <= 0.3\n",
    "      idy = np.where(y_cat==1)\n",
    "      # Cross validation\n",
    "                                            \n",
    "      X0_ss00,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                  X_train,train_index,X_test,\n",
    "                                                  test_index,pre_updrs_iii_on,age,dd,sex,None,\n",
    "                                                  None,None,None,None,None,False,False,False)\n",
    "      cvn = 5\n",
    "      cv_scores = np.zeros((cvn+1,1))\n",
    "      cv_lgr_scores = np.zeros((cvn+1,1))\n",
    "      rs = 1\n",
    "      rcfs = 1000\n",
    "      (mu, sigma) = stats.norm.fit(y_train0)\n",
    "      kappa = stats.skew(y_train0)\n",
    "      print('Label distribution of:',mu,sigma,kappa)\n",
    "      Q = 5\n",
    "      for jj in np.arange(Q):\n",
    "        # Resample to avoid stratification errors\n",
    "        while np.sum(y_cat) < cvn:\n",
    "          np.random.seed(rs)\n",
    "          idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "          X0_ss00 = np.append(X0_ss00,X0_ss00[idyr,:].reshape(1,-1),axis=0)\n",
    "          y_train0 = np.append(y_train0,y_train0[idyr])\n",
    "          y_cat = y_train0 <= 0.3\n",
    "          rs = rs+1\n",
    "          print('Resampled to size',y_train0.shape)\n",
    "          y_train_n = y_train0\n",
    "          X0_ss0_n = X0_ss00\n",
    "        if aug == True:\n",
    "          y_train_n = np.append(y_train_n,y_train0+(1.96*sigma)*np.random.normal(0,1,1))\n",
    "          y_cat = y_train_n <= 0.3\n",
    "          X0_ss0_n = np.append(X0_ss0_n,X0_ss00,axis=0)\n",
    "\n",
    "      y_train = y_train_n\n",
    "      X0_ss0 = X0_ss0_n\n",
    "      \n",
    "      for jj in np.arange(2,cvn+1):\n",
    "        skf_g = sms.StratifiedKFold(n_splits=jj,shuffle=True,random_state=0)\n",
    "        skf_gen = skf_g.split(X0_ss0,y_cat)\n",
    "        lasso = slm.LassoLarsCV(max_iter=10000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None,penalty='l1',solver='liblinear',random_state=0)\n",
    "        with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "          # Feature selection\n",
    "          warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "          sel = skf.RFECV(lasso,step=rcfs,cv=skf_gen,n_jobs=-1)\n",
    "          # Stratifies classifiers automatically\n",
    "          sel_lr = skf.RFECV(lgr,step=rcfs,cv=jj,n_jobs=-1)\n",
    "          X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "          X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "          est_ls = lasso.fit(X0_ss,y_train)\n",
    "          est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "          cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "          cv_lgr_scores[jj] = est_lgr.score(X0_ssl,y_cat)\n",
    "          print('LassoCV score for',jj,'is',cv_scores[jj],'from dataset of size',X0_ss.shape)\n",
    "          \n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "        best_cv = np.argmax(cv_scores)\n",
    "        best_cv_lgr = np.argmax(cv_lgr_scores)\n",
    "\n",
    "        # Break any ties\n",
    "        if np.sum(cv_scores == best_cv) > 1:\n",
    "          cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "          cv_lgr_scores_tb = np.zeros((np.sum(cv_scores == best_cv_lgr),1))\n",
    "          for jjj in (cv_scores == cv_scores[best_cv]):\n",
    "            if jjj > 0:\n",
    "              print('Breaking tie')\n",
    "              skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "              skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "              X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "              X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "              lasso = slm.LassoLarsCV(max_iter=10000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "              lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None,penalty='l1',solver='liblinear',random_state=1)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "              cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "              cv_lgr_scores_tb[jj] = est_lgr.score(sel_lr.fit_transform(X0_ss0,y_cat),y_cat)\n",
    "          best_cv = np.argmax(cv_scores_tb)\n",
    "          best_cv_lgr = np.argmax(cv_lgr_scores_tb)\n",
    "        \n",
    "        # Fit whole dataset with optimal cv\n",
    "        lasso = slm.LassoLarsCV(max_iter=10000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=best_cv_lgr,class_weight=None,penalty='l1',solver='liblinear',random_state=0)\n",
    "        sel = skf.RFECV(lasso,step=rcfs,cv=best_cv)\n",
    "        sel_lr = skf.RFECV(lgr,step=rcfs,cv=best_cv)\n",
    "        X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "        X_test_ss = sel.transform(X_test_ss0)\n",
    "        X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "        X_test_ssl = sel_lr.transform(X_test_ss0)\n",
    "        K_ss = sel.transform(K_all_c.reshape(1,-1))\n",
    "        K_ss_lr = sel_lr.transform(K_all_c.reshape(1,-1))\n",
    "        R_ss = sel.transform(R_all_c.reshape(1,-1))\n",
    "\n",
    "\n",
    "      # LASSO\n",
    "      with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        lasso = slm.LassoLarsCV(max_iter=10000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=best_cv_lgr,class_weight=None,penalty='l1',solver='liblinear',random_state=0)\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "        est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "      results_lgr_aug[c] = est_lgr.predict(X_test_ssl)\n",
    "      results_lgrp_aug[c] = est_lgr.predict_proba(X_test_ssl)[0][0]\n",
    "      results_ls_aug[c] = est_ls.predict(X_test_ss)\n",
    "      print('Lasso predicts',str(np.round(results_ls_aug[c],4)),'and logistic regression predicts',results_lgr_aug[c],\n",
    "                'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv,'and',sum(y_cat),'minority cases')\n",
    "      K_nz_nc.append(np.squeeze(K_ss)[abs(est_ls.coef_)>0])\n",
    "      E_nz_nc.append(est_ls.coef_[abs(est_ls.coef_)>0])\n",
    "      K_nz_nc_lr.append(np.squeeze(K_ss_lr)[np.squeeze(abs(est_lgr.coef_)>0)])\n",
    "      E_nz_nc_lr.append(est_lgr.coef_[abs(est_lgr.coef_)>0])\n",
    "      c=c+1\n",
    "\n",
    "  np.save('results_ls_aug_d_cvs.npy',results_ls_aug)\n",
    "  np.save('results_lgr_aug_d_cvs.npy',results_lgr_aug)\n",
    "  np.save('results_lgrp_aug_d_cvs.npy',results_lgrp_aug)\n",
    "  np.save('results_K_nz_nc_cvs_all.npy',K_nz_nc)\n",
    "  np.save('results_K_nz_nc_lr_cvs.npy',K_nz_nc_lr)\n",
    "  np.save('beta_nz_nc_cvs.npy',E_nz_nc)\n",
    "  np.save('beta_nz_nc_lr_cvs.npy',E_nz_nc_lr)\n",
    "else:\n",
    "  results_ls_aug = np.load('results_ls_aug_d_cvs.npy')\n",
    "  results_lgr_aug = np.load('results_lgr_aug_d_cvs.npy')\n",
    "  results_lgrp_aug = np.load('results_lgrp_aug_d_cvs.npy')\n",
    "  K_nz_nc = np.load('results_K_nz_nc_cvs_all.npy',allow_pickle=True)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Lasso\n",
    "if retrain == 1:\n",
    "  Js = []\n",
    "  aug = False\n",
    "  K_nz_vl = []\n",
    "  K_nz_vl_lr = []\n",
    "  err_var = np.zeros_like(per_change)\n",
    "  rerror = np.zeros_like(per_change)\n",
    "  kappa = []\n",
    "  K_nz = []\n",
    "  E_nz_vl = []\n",
    "  E_nz_vl_lr = []\n",
    "  c = 0\n",
    "\n",
    "  for j in np.arange(len(subsc)):\n",
    "      test_id = subsc[j]\n",
    "      test_index = subsc == test_id\n",
    "      train_index = subsc != test_id\n",
    "      X_train = X_all_c[train_index,:,:]\n",
    "      X_test = X_all_c[test_index,:,:]\n",
    "      y_train = per_change[train_index]\n",
    "      y_test = per_change[test_index]\n",
    "\n",
    "      y_cat = y_train <= 0.3\n",
    "      idy = np.where(y_cat==1)\n",
    "      # Cross validation\n",
    "      X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                  X_train,train_index,X_test,\n",
    "                                                  test_index,pre_updrs_iii_on,age,dd,sex,None,None,None,None,None,None,False,False,False)\n",
    "      cvn = 5\n",
    "      cv_scores = np.zeros((cvn+1,1))\n",
    "      cv_lgr_scores = np.zeros((cvn+1,1))\n",
    "      rs = 1\n",
    "      rcfs = 1000\n",
    "      (mu, sigma) = stats.norm.fit(y_train)\n",
    "      kappa = stats.skew(y_train)\n",
    "      print('Label distribution of:',mu,sigma,kappa)\n",
    "      for jj in np.arange(2,cvn):\n",
    "        # Resample to avoid stratification errors\n",
    "        while np.sum(y_cat) < cvn:\n",
    "          np.random.seed(rs)\n",
    "          idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "          X0_ss0 = np.append(X0_ss0,X0_ss0[idyr,:].reshape(1,-1),axis=0)\n",
    "          y_train = np.append(y_train,y_train[idyr])\n",
    "          y_cat = y_train <= 0.3\n",
    "          rs = rs+1\n",
    "          print('Resampled to size',y_train.shape)\n",
    "        if aug == True:\n",
    "          y_train_n = y_train+(1.96*sigma)*np.random.normal(0,1,1)\n",
    "          y_train = np.hstack((y_train,y_train_n))\n",
    "          y_cat = y_train <= 0.3\n",
    "          X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "        else:\n",
    "          y_train_n = y_train\n",
    "          y_train = np.hstack((y_train,y_train_n))\n",
    "          y_cat = y_train <= 0.3\n",
    "          X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "          \n",
    "      \n",
    "      for jj in np.arange(2,cvn+1):\n",
    "        skf_g = sms.StratifiedKFold(n_splits=jj,shuffle=True,random_state=0)\n",
    "        skf_gen = skf_g.split(X0_ss0,y_cat)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "          warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "          warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "          lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None,penalty='l1',solver='liblinear',random_state=0)\n",
    "          print('Found minority samples:',sum(y_cat))\n",
    "          # Feature selection\n",
    "          sel = skf.RFECV(lasso,step=rcfs,cv=skf_gen,n_jobs=1)\n",
    "          # Stratifies classifiers automatically\n",
    "          sel_lr = skf.RFECV(lgr,step=rcfs,cv=jj,n_jobs=1)\n",
    "          X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "          X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "          est_ls = lasso.fit(X0_ss,y_train)\n",
    "          est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "          cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "          cv_lgr_scores[jj] = est_lgr.score(X0_ssl,y_cat)\n",
    "          print('LogisticRegressionCV score for',jj,'is',cv_lgr_scores[jj],'from dataset of size',X0_ss.shape)\n",
    "          \n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "        best_cv = np.argmax(cv_scores)\n",
    "        best_cv_lgr = np.argmax(cv_lgr_scores)\n",
    "\n",
    "        # Break any ties\n",
    "        if np.sum(cv_scores == best_cv) > 1:\n",
    "          cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "          cv_lgr_scores_tb = np.zeros((np.sum(cv_scores == best_cv_lgr),1))\n",
    "          for jjj in (cv_scores == cv_scores[best_cv]):\n",
    "            if jjj > 0:\n",
    "              print('Breaking tie')\n",
    "              skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "              skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "              X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "              X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "              lasso = slm.LassoLarsCV(max_iter=1000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "              lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None,penalty='l1',solver='liblinear',random_state=1)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "              cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "              cv_lgr_scores_tb[jj] = est_lgr.score(sel_lr.fit_transform(X0_ss0,y_cat),y_cat)\n",
    "          best_cv = np.argmax(cv_scores_tb)\n",
    "          best_cv_lgr = np.argmax(cv_lgr_scores_tb)\n",
    "        \n",
    "        # Fit whole dataset with optimal cv\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None,penalty='l1',solver='liblinear',random_state=0)\n",
    "        sel = skf.RFECV(lasso,step=rcfs,cv=best_cv)\n",
    "        sel_lr = skf.RFECV(lgr,step=rcfs,cv=best_cv)\n",
    "        X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "        X_test_ss = sel.transform(X_test_ss0)\n",
    "        X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "        X_test_ssl = sel_lr.transform(X_test_ss0)\n",
    "        K_ss = sel.transform(K_all_c.reshape(1,-1))\n",
    "        K_ss_lr = sel_lr.transform(K_all_c.reshape(1,-1))\n",
    "\n",
    "      # LASSO\n",
    "      with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=best_cv_lgr,class_weight=None,penalty='l1',solver='liblinear',random_state=0)\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "        est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "      results_lgr[c] = est_lgr.predict(X_test_ssl)\n",
    "      results_lgrp[c] = est_lgr.predict_proba(X_test_ssl)[0][0]\n",
    "      results_ls[c] = est_ls.predict(X_test_ss)\n",
    "      print('Lasso predicts',str(np.round(results_ls[c],4)),'and logistic regression predicts',results_lgr[c],\n",
    "                'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv_lgr,'and',sum(y_cat),'minority cases')\n",
    "      K_nz_vl.append(np.squeeze(K_ss)[abs(est_ls.coef_)>0])\n",
    "      E_nz_vl.append(est_ls.coef_[abs(est_ls.coef_)>0])\n",
    "      K_nz_vl_lr.append(np.squeeze(K_ss_lr)[np.squeeze((est_lgr.coef_)>0)])\n",
    "      E_nz_vl_lr.append(est_lgr.coef_[abs(est_lgr.coef_)>0])\n",
    "      c=c+1\n",
    "\n",
    "  np.save('results_ls_d_cvs.npy',results_ls)\n",
    "  np.save('results_lgr_d_cvs.npy',results_lgr)\n",
    "  np.save('results_lgrp_d_cvs.npy',results_lgrp)\n",
    "  np.save('results_K_nz_vl_cvs_all.npy',K_nz_vl)\n",
    "  np.save('results_K_nz_vl_lr_cvs.npy',K_nz_vl_lr)\n",
    "  np.save('beta_nz_vl_cvs.npy',E_nz_vl)\n",
    "  np.save('beta_nz_vl_lr_cvs.npy',E_nz_vl_lr)\n",
    "\n",
    "    \n",
    "else:\n",
    "  results_ls = np.load('results_ls_d_cvs.npy')\n",
    "  results_lgr = np.load('results_lgr_d_cvs.npy')\n",
    "  results_lgrp = np.load('results_lgrp_d_cvs.npy')\n",
    "  K_nz_vl = np.load('results_K_nz_vl_cvs_all.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Lasso\n",
    "results_lsw = np.zeros_like(per_change)\n",
    "results_lgrw = np.zeros_like(per_change)\n",
    "results_lgrwp = np.zeros_like(per_change)\n",
    "\n",
    "if retrain == 1:\n",
    "  Js = []\n",
    "  aug = False\n",
    "  K_nz_vlw = []\n",
    "  K_nz_vlw_lr = []\n",
    "  err_var = np.zeros_like(per_change)\n",
    "  rerror = np.zeros_like(per_change)\n",
    "  kappa = []\n",
    "  K_nzw = []\n",
    "  E_nz_vlw = []\n",
    "  E_nz_vlw_lr = []\n",
    "  c = 0\n",
    "\n",
    "  for j in np.arange(len(subsc)):\n",
    "      test_id = subsc[j]\n",
    "      test_index = subsc == test_id\n",
    "      train_index = subsc != test_id\n",
    "      X_train = X_all_c[train_index,:,:]\n",
    "      X_test = X_all_c[test_index,:,:]\n",
    "      y_train = per_change[train_index]\n",
    "      y_test = per_change[test_index]\n",
    "\n",
    "      y_cat = y_train <= 0.3\n",
    "      idy = np.where(y_cat==1)\n",
    "\n",
    "      # Cross validation\n",
    "      X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                  X_train,train_index,X_test,\n",
    "                                                  test_index,pre_updrs_iii_on,age,dd,sex,None,None,None,None,None,None,False,False,False)\n",
    "      cvn = 5\n",
    "      cv_scores = np.zeros((cvn+1,1))\n",
    "      cv_lgr_scores = np.zeros((cvn+1,1))\n",
    "      rs = 1\n",
    "      rcfs = 1000\n",
    "      (mu, sigma) = stats.norm.fit(y_train)\n",
    "      kappa = stats.skew(y_train)\n",
    "      print('Label distribution of:',mu,sigma,kappa)\n",
    "\n",
    "           # Resample to avoid stratification errors\n",
    "      while np.sum(y_cat) < cvn:\n",
    "          np.random.seed(rs)\n",
    "          idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "          X0_ss0 = np.append(X0_ss0,X0_ss0[idyr,:].reshape(1,-1),axis=0)\n",
    "          y_train = np.append(y_train,y_train[idyr])\n",
    "          y_cat = y_train <= 0.3\n",
    "          rs = rs+1\n",
    "          print('Resampled to size',y_train.shape)\n",
    "          \n",
    "      for jj in np.arange(2,cvn+1):\n",
    "\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "          warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "          warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "          lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None,penalty='l1',solver='liblinear',random_state=0)\n",
    "          print('Found minority samples:',sum(y_cat))\n",
    "          # Feature selection\n",
    "          sel = skf.RFECV(lasso,step=rcfs,cv=jj,n_jobs=1)\n",
    "          # Stratifies classifiers automatically\n",
    "          X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "          est_ls = lasso.fit(X0_ss,y_train)\n",
    "          cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "          print('Lasso score for',jj,'is',cv_scores[jj],'from dataset of size',X0_ss.shape)\n",
    "          \n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "        best_cv = np.argmax(cv_scores)\n",
    "        best_cv_lgr = np.argmax(cv_lgr_scores)\n",
    "\n",
    "        # Break any ties\n",
    "        if np.sum(cv_scores == best_cv) > 1:\n",
    "          cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "          cv_lgr_scores_tb = np.zeros((np.sum(cv_scores == best_cv_lgr),1))\n",
    "          for jjj in (cv_scores == cv_scores[best_cv]):\n",
    "            if jjj > 0:\n",
    "              print('Breaking tie')\n",
    "              skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "              skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "              X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "              X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "              lasso = slm.LassoLarsCV(max_iter=1000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "              lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None,penalty='l1',solver='liblinear',random_state=1)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "              cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "              cv_lgr_scores_tb[jj] = est_lgr.score(sel_lr.fit_transform(X0_ss0,y_cat),y_cat)\n",
    "          best_cv = np.argmax(cv_scores_tb)\n",
    "          best_cv_lgr = np.argmax(cv_lgr_scores_tb)\n",
    "        \n",
    "        # Fit whole dataset with optimal cv\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        sel = skf.RFECV(lasso,step=rcfs,cv=best_cv)\n",
    "        X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "        X_test_ss = sel.transform(X_test_ss0)\n",
    "        K_ss = sel.transform(K_all_c.reshape(1,-1))\n",
    "\n",
    "\n",
    "      # LASSO\n",
    "      with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "  \n",
    "      results_lsw[c] = est_ls.predict(X_test_ss)\n",
    "      print('Lasso predicts',str(np.round(results_lsw[c],4)),'and logistic regression predicts',results_lgrw[c],\n",
    "                'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv,'and',sum(y_cat),'minority cases')\n",
    "      try:\n",
    "        K_nz_vlw.append(np.squeeze(K_ss)[abs(est_ls.coef_)>0])\n",
    "      except:\n",
    "        K_nz_vlw.append(K_ss[abs(est_ls.coef_)>0])\n",
    "      c=c+1\n",
    "\n",
    "  np.save('results_lsw_d_cvs.npy',results_lsw)\n",
    "  np.save('results_K_nz_vlw_cvs_all.npy',K_nz_vlw)\n",
    "\n",
    "    \n",
    "else:\n",
    "  results_lsw = np.load('results_lsw_d_cvs.npy')\n",
    "  K_nz_vlw = np.load('results_K_nz_vlw_cvs_all.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOGN LASSO\n",
    "if retrain == 1:\n",
    "  results_ls_smogn = np.zeros_like(per_change)\n",
    "  aug = False\n",
    "  c = 0\n",
    "  K_nz_smogn = []\n",
    "  #R_nz_smogn = []\n",
    "  for j in np.arange(c,len(subsc)):\n",
    "      test_id = subsc[j]\n",
    "      test_index = subsc == test_id\n",
    "      train_index = subsc != test_id\n",
    "      X_train = X_all_c[train_index,:,:]\n",
    "      X_test = X_all_c[test_index,:,:]\n",
    "      y_train = per_change[train_index]\n",
    "      y_test = per_change[test_index]\n",
    "\n",
    "      y_cat = y_train <= 0.3\n",
    "      idy = np.where(y_cat==1)\n",
    "      # Cross validation\n",
    "      X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                  X_train,train_index,X_test,\n",
    "                                                  test_index,pre_updrs_iii_on,age,dd,sex,None,None,None,None,None,None,False,False,False)\n",
    "      X_smogn,y_smogn,idx_kept,sscaler = util.rad_smogn(X0_ss0,y_train,np.amax(y_train),np.amin(y_train),1,0,0.05,0.02,1)\n",
    "      X0_ss00 = X0_ss0\n",
    "      X0_ss0 = np.vstack((X0_ss0,X_smogn))\n",
    "      y_train = np.hstack((y_train,y_smogn))\n",
    "      y_train_0 = y_train\n",
    "      cvn = 5\n",
    "\n",
    "      cv_scores = np.zeros((cvn+1,1))\n",
    "      rs = 1\n",
    "      rcfs = 1000\n",
    "      (mu, sigma) = stats.norm.fit(y_train)\n",
    "      kappa = stats.skew(y_train)\n",
    "      print('Label distribution of:',mu,sigma,kappa)\n",
    "      for jj in np.arange(2,cvn):\n",
    "        # Resample to avoid stratification errors\n",
    "        while np.sum(y_cat) < cvn:\n",
    "          np.random.seed(rs)\n",
    "          idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "          X0_ss0 = np.append(X0_ss0,X0_ss0[idyr,:].reshape(1,-1),axis=0)\n",
    "          y_train = np.append(y_train,y_train[idyr])\n",
    "          y_cat = y_train <= 0.3\n",
    "          rs = rs+1\n",
    "          print('Resampled to size',y_train.shape)\n",
    "\n",
    "      for jj in np.arange(2,cvn+1):\n",
    "        skf_g = sms.StratifiedKFold(n_splits=jj,shuffle=True,random_state=0)\n",
    "        skf_gen = skf_g.split(X0_ss0,y_cat)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "          # Feature selection\n",
    "          warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "          sel = skf.RFECV(lasso,step=rcfs,cv=skf_gen)\n",
    "          X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "          X0_ss = X0_sst\n",
    "          est_ls = lasso.fit(X0_ss,y_train)\n",
    "          cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "          print('LassoCV score for',jj,'is',cv_scores[jj],'from dataset of size',X0_ss.shape)\n",
    "          \n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "        best_cv = np.argmax(cv_scores)\n",
    "        # Break any ties\n",
    "        if np.sum(cv_scores == best_cv) > 1:\n",
    "          cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "          for jjj in (cv_scores == cv_scores(best_cv)):\n",
    "            if jjj > 0:\n",
    "              skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "              skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "              X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "              X0_ss = X0_sst\n",
    "              lasso = slm.LassoLarsCV(max_iter=1000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "          best_cv = np.argmax(cv_scores_tb)\n",
    "        \n",
    "        # Fit whole dataset with optimal cv\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        sel = skf.RFECV(lasso,step=rcfs,cv=best_cv)\n",
    "        X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "        X_test_ss = sel.transform(X_test_ss0)\n",
    "        K_ss = sel.transform(K_all_c.reshape(1,-1))\n",
    "        \n",
    "      # LASSO\n",
    "      with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "        results_ls_smogn[c] = est_ls.predict(X_test_ss)\n",
    "        if results_ls_smogn[c] < 0:\n",
    "            dx, y_n = cKDTree(X0_ss00.reshape(X0_ss00.shape[0],-1)).query(X_test_ss0.reshape(1,-1),k=15)\n",
    "            results_ls_smogn[c] = np.mean((y_train_0[y_n]))\n",
    "            print('Using nearest neighbor')\n",
    "        print('Lasso predicts',str(np.round(results_ls_smogn[c],4)),\n",
    "              'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv)\n",
    "      try:\n",
    "        K_nz_smogn.append(np.squeeze(K_ss)[abs(est_ls.coef_)>0])\n",
    "      except:\n",
    "        next\n",
    "      #   K_nz_smogn.append(K_ss[abs(est_ls.coef_)>0])\n",
    "      c=c+1\n",
    "\n",
    "\n",
    "  np.save('results_ls_smogn_d_cvs.npy',results_ls_smogn)\n",
    "  np.save('results_K_nz_smogn_cvs_all.npy',K_nz_smogn)\n",
    "  np.save('beta_nz_smogn_cvs.npy',est_ls.coef_[abs(est_ls.coef_)>0])\n",
    "\n",
    "else:\n",
    "  results_ls_smogn = np.load('results_ls_smogn_d_cvs.npy')\n",
    "  K_nz_smogn = np.load('results_K_nz_smogn_cvs_all.npy',allow_pickle=True)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define probability of an unsucessful procedure as $P = 1-u$ where $u$ is the UPDRS percent improvement prediction between $0$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wild bootstrap LASSO\n",
    "if retrain == 1:\n",
    "  aug = False\n",
    "  results_ls_wbs = np.zeros_like(per_change)\n",
    "  K_nz_wbs = []\n",
    "  #R_nz_wbs = []\n",
    "  c = 0\n",
    "  for j in np.arange(c,len(subsc)):\n",
    "      test_id = subsc[j]\n",
    "      test_index = subsc == test_id\n",
    "      train_index = subsc != test_id\n",
    "      X_train = X_all_c[train_index,:,:]\n",
    "      X_test = X_all_c[test_index,:,:]\n",
    "      y_train = per_change[train_index]\n",
    "      y_test = per_change[test_index]\n",
    "\n",
    "      y_cat = y_train <= 0.3\n",
    "      idy = np.where(y_cat==1)\n",
    "      # Cross validation\n",
    "                                            \n",
    "      X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                  X_train,train_index,X_test,\n",
    "                                                  test_index,pre_updrs_iii_on,age,dd,sex,None,None,None,None,None,None,False,False,False)\n",
    "      X0_ss00 = X0_ss0\n",
    "      y_train_0 = y_train\n",
    "      cvn = 5\n",
    "\n",
    "      cv_scores = np.zeros((cvn+1,1))\n",
    "      rs = 1\n",
    "      rcfs = 1000\n",
    "      (mu, sigma) = stats.norm.fit(y_train)\n",
    "      kappa = stats.skew(y_train)\n",
    "      print('Label distribution of:',mu,sigma,kappa)\n",
    "      for jj in np.arange(2,cvn):\n",
    "        # Resample to avoid stratification errors\n",
    "        while np.sum(y_cat) < cvn:\n",
    "          np.random.seed(rs)\n",
    "          idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "          X0_ss0 = np.append(X0_ss0,X0_ss0[idyr,:].reshape(1,-1),axis=0)\n",
    "          y_train = np.append(y_train,y_train[idyr])\n",
    "          y_cat = y_train <= 0.3\n",
    "          rs = rs+1\n",
    "          print('Resampled to size',y_train.shape)\n",
    "          ls0 = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "          est0 = ls0.fit(X0_ss0,y_train)\n",
    "          eps = y_train-ls0.predict(X0_ss0)\n",
    "          eps_v = eps*np.random.normal(0,1,1)\n",
    "          y_train_0 = y_train\n",
    "        if aug == True:\n",
    "          y_train_n = y_train+(1.96*sigma)*np.random.normal(0,1,1)\n",
    "          y_train = np.hstack((y_train,y_train_n))\n",
    "          y_cat = y_train <= 0.3\n",
    "          X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "        else: # Control for different training sample sizes\n",
    "          while len(eps_v) < len(y_train):\n",
    "            eps_v = np.hstack((eps_v,eps*np.random.normal(0,1,1)))\n",
    "          y_train_n = y_train+eps_v\n",
    "          y_train = np.hstack((y_train,y_train_n))\n",
    "          y_cat = y_train <= 0.3\n",
    "          X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "\n",
    "      for jj in np.arange(2,cvn+1):\n",
    "        skf_g = sms.StratifiedKFold(n_splits=jj,shuffle=True,random_state=0)\n",
    "        skf_gen = skf_g.split(X0_ss0,y_cat)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "          # Feature selection\n",
    "          warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "          sel = skf.RFECV(lasso,step=rcfs,cv=skf_gen)\n",
    "          X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "          X0_ss = X0_sst\n",
    "          est_ls = lasso.fit(X0_ss,y_train)\n",
    "          cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "          print('LassoCV score for',jj,'is',cv_scores[jj],'from dataset of size',X0_ss.shape)\n",
    "          \n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "        best_cv = np.argmax(cv_scores)\n",
    "        # Break any ties\n",
    "        if np.sum(cv_scores == best_cv) > 1:\n",
    "          cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "          for jjj in (cv_scores == cv_scores(best_cv)):\n",
    "            if jjj > 0:\n",
    "              skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "              skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "              X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "              X0_ss = X0_sst\n",
    "              lasso = slm.LassoLarsCV(max_iter=1000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "          best_cv = np.argmax(cv_scores_tb)\n",
    "        \n",
    "        # Fit whole dataset with optimal cv\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        sel = skf.RFECV(lasso,step=rcfs,cv=best_cv)\n",
    "        X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "        X_test_ss = sel.transform(X_test_ss0)\n",
    "        K_ss = sel.transform(K_all_c.reshape(1,-1))\n",
    "        \n",
    "      # LASSO\n",
    "      with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "        results_ls_wbs[c] = est_ls.predict(X_test_ss)\n",
    "        if results_ls_wbs[c] < 0:\n",
    "            dx, y_n = cKDTree(X0_ss00.reshape(X0_ss00.shape[0],-1)).query(X_test_ss0.reshape(1,-1),k=15)\n",
    "            results_ls_wbs[c] = np.mean((y_train_0[y_n]))\n",
    "            print('Using nearest neighbor')\n",
    "        print('Lasso predicts',str(np.round(results_ls_wbs[c],4)),\n",
    "              'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv)\n",
    "        K_nz_wbs.append(np.squeeze(K_ss)[abs(est_ls.coef_)>0])\n",
    "        c=c+1\n",
    "\n",
    "  np.save('results_ls_wbs_d_cvs.npy',results_ls_wbs)\n",
    "  np.save('results_K_nz_wbs_cvs_all.npy',K_nz_wbs)\n",
    "  np.save('beta_nz_wbs_cvs.npy',est_ls.coef_[abs(est_ls.coef_)>0])\n",
    "else:\n",
    "  results_ls_wbs = np.load('results_ls_wbs_d_cvs.npy')\n",
    "  K_nz_wbs = np.load('results_K_nz_wbs_cvs_all.npy',allow_pickle=True)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
    "    data1     = np.asarray(data1)\n",
    "    data2     = np.asarray(data2)\n",
    "    mean      = np.mean([data1, data2], axis=0)\n",
    "    diff      = data1-data2                   # Difference between datasets\n",
    "    md        = np.mean(diff)                 # Mean of the difference\n",
    "    sd        = np.std(diff, axis=0)          # Standard deviation of the difference\n",
    "\n",
    "    plt.scatter(mean, diff, *args, **kwargs)\n",
    "    plt.axhline(md,           color='gray', linestyle='--')\n",
    "    plt.text(0.1, 2*md, r'$\\mu$ = '+str(np.round(md,2)), horizontalalignment='center',verticalalignment='center')\n",
    "    plt.text(0.5, 0.4, r'$\\mu + 1.96 \\sigma$ = '+str(np.round(md + 1.96*sd,2)), horizontalalignment='center',verticalalignment='center')\n",
    "    plt.text(0.5, -0.5, r'$\\mu - 1.96 \\sigma$ = '+str(np.round(md - 1.96*sd,2)), horizontalalignment='center',verticalalignment='center')\n",
    "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
    "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
    "    plt.ylim([-4*1.96*sd,4*1.96*sd])\n",
    "    diff_in = np.logical_and(diff < md + 1.96*sd,diff > md - 1.96*sd)\n",
    "    plt.scatter(mean[diff_in], diff[diff_in], *args, **kwargs)\n",
    "    plt.xlabel(r'$\\frac{y_{true}+y_{predicted}}{2}$',fontsize=16)\n",
    "    plt.ylabel(r'$y_{predicted}-y_{true}$',fontsize=12)\n",
    "    return diff,diff_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d,din = bland_altman_plot(results_ls_aug,per_change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = 1.25\n",
    "fs = 32\n",
    "ofx = 0.25\n",
    "ofy = 0.75\n",
    "pre_imp = np.repeat((np.asarray(pre_updrs_iii_off,dtype=float)-np.asarray(pre_updrs_iii_on,dtype=float))/np.asarray(pre_updrs_iii_off,dtype=float),r)\n",
    "per_change = np.repeat(per_change,r)\n",
    "plt.rcParams['figure.figsize'] = [20, 30]\n",
    "fig,ax = plt.subplots(3,2,sharex=True,sharey=True)\n",
    "col = np.where(per_change <= 0.3,'orangered','blue')\n",
    "ax[0,0].scatter(pre_imp,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(pre_imp,per_change)\n",
    "y_model = pre_imp*lr.slope+lr.intercept\n",
    "ax[0,0].plot(pre_imp,y_model,color='r')\n",
    "ax[0,0].text(ofx,ofy,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.3f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[0,0].transAxes,fontsize=fs,\n",
    "                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round'))  \n",
    "ci,pi,x_line,y_line = util.confidence_interval(pre_imp,per_change,ylim)\n",
    "ax[0,0].fill_between(x_line,y_line+ci,y_line-ci,color = 'r',label = '95% confidence interval',alpha=0.1)\n",
    "ax[0,0].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[0,0].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[0,0].set_title('LCT',fontsize=fs)\n",
    "ax[0,0].xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[0,0].set_ylabel('True UPDRS-III improvement',fontsize=fs)\n",
    "ax[0,0].yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "\n",
    "ax[1,1].scatter(results_ls,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(np.squeeze(results_ls),per_change)\n",
    "y_model = results_ls*lr.slope+lr.intercept\n",
    "ax[1,1].plot(results_ls,y_model,color='r')\n",
    "ax[1,1].text(ofx,ofy,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.7f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[1,1].transAxes,fontsize=fs,\n",
    "                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round')) \n",
    "ci,pi,x_line,y_line = util.confidence_interval(results_ls,per_change,ylim)\n",
    "ax[1,1].fill_between(x_line,y_line+ci,y_line-ci,color = 'r',label = '95% confidence interval',alpha=0.1)\n",
    "ax[1,1].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[1,1].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[1,1].set_title('Bootstrap Lasso',fontsize=fs)\n",
    "ax[1,1].xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[1,1].yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "\n",
    "ax[2,1].scatter(results_ls_smogn,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(np.squeeze(results_ls_smogn),per_change)\n",
    "y_model = results_ls_smogn*lr.slope+lr.intercept\n",
    "ax[2,1].plot(results_ls_smogn,y_model,color='r')\n",
    "ax[2,1].text(ofx,ofy,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.7f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[2,1].transAxes,fontsize=fs,\n",
    "                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round')) \n",
    "ci,pi,x_line,y_line = util.confidence_interval(results_ls_smogn,per_change,ylim)\n",
    "ax[2,1].fill_between(x_line,y_line+ci,y_line-ci,color = 'r',label = '95% confidence interval',alpha=0.1)\n",
    "ax[2,1].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[2,1].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[2,1].set_xlabel('Prediction',fontsize=fs)\n",
    "ax[2,1].xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[2,1].yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[2,1].set_title('SMOGN Lasso',fontsize=fs)\n",
    "\n",
    "ax[2,0].scatter(results_ls_wbs,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(np.squeeze(results_ls_wbs),per_change)\n",
    "y_model = results_ls_wbs*lr.slope+lr.intercept\n",
    "ax[2,0].plot(results_ls_wbs,y_model,color='r')\n",
    "ax[2,0].text(ofx,ofy,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.7f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[2,0].transAxes,fontsize=fs,\n",
    "                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round')) \n",
    "ci,pi,x_line,y_line = util.confidence_interval(results_ls_wbs,per_change,ylim)\n",
    "ax[2,0].fill_between(x_line,y_line+ci,y_line-ci,color = 'r',label = '95% confidence interval',alpha=0.1)\n",
    "ax[2,0].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[2,0].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[2,0].set_xlabel('Prediction',fontsize=fs)\n",
    "ax[2,0].xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[2,0].set_ylabel('True UPDRS-III Improvement',fontsize=fs)\n",
    "ax[2,0].yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[2,0].set_title('Wild Boostrap Lasso',fontsize=fs)\n",
    "\n",
    "ax[0,1].scatter(results_ls_aug,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(np.squeeze(results_ls_aug),per_change)\n",
    "y_model = results_ls_aug*lr.slope+lr.intercept\n",
    "ax[0,1].plot(results_ls_aug,y_model,color='r')\n",
    "ax[0,1].text(ofx,ofy,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.7f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[0,1].transAxes,fontsize=fs,\n",
    "                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round')) \n",
    "ci,pi,x_line,y_line = util.confidence_interval(results_ls_aug,per_change,ylim)\n",
    "ax[0,1].fill_between(x_line,y_line+ci,y_line-ci,color = 'r',label = '95% confidence interval',alpha=0.1)\n",
    "ax[0,1].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[0,1].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[0,1].xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[0,1].yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[0,1].set_title('Noise Compensated Lasso',fontsize=fs)\n",
    "plt.ylim([0,ylim])\n",
    "plt.xlim([0,ylim])\n",
    "\n",
    "ax[1,0].scatter(results_lsw,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(np.squeeze(results_lsw),per_change)\n",
    "y_model = results_lsw*lr.slope+lr.intercept\n",
    "ax[1,0].plot(results_lsw,y_model,color='r')\n",
    "ax[1,0].text(ofx,ofy,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.7f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[1,0].transAxes,fontsize=fs,\n",
    "                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round')) \n",
    "ci,pi,x_line,y_line = util.confidence_interval(results_lsw,per_change,ylim)\n",
    "ax[1,0].fill_between(x_line,y_line+ci,y_line-ci,color = 'r',label = '95% confidence interval',alpha=0.1)\n",
    "ax[1,0].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[1,0].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[1,0].set_title('Lasso',fontsize=fs)\n",
    "ax[1,0].xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax[1,0].set_ylabel('True UPDRS-III Improvement',fontsize=fs)\n",
    "ax[1,0].yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "\n",
    "\n",
    "plt.savefig('plots_auto.png')\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_nz_wbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "E_nz_nc = np.load('beta_nz_nc_cvs.npy',allow_pickle=True)\n",
    "plt.rcParams[\"figure.figsize\"] = (30,10)\n",
    "fig, axes = plt.subplots(1,1,sharey=True)\n",
    "\n",
    "R = [item for sublist in K_nz_nc for item in sublist]\n",
    "letter_counts = Counter(R)\n",
    "lc = {x: count for x, count in letter_counts.items() if count >= 35}\n",
    "df_lc = pd.DataFrame.from_dict(lc, orient='index')\n",
    "df_lc.sort_values(0, ascending=False, inplace=True)\n",
    "\n",
    "R = [item for sublist in K_nz_vl for item in sublist]\n",
    "letter_counts = Counter(R)\n",
    "lcv = {x: count for x, count in letter_counts.items() if x in lc.keys()}\n",
    "df_lcv = pd.DataFrame.from_dict(lcv, orient='index')\n",
    "df_lcv.sort_values(0, ascending=False, inplace=True)\n",
    "\n",
    "R = [item for sublist in K_nz_wbs for item in sublist]\n",
    "letter_counts = Counter(R)\n",
    "lwbs = {x: count for x, count in letter_counts.items() if x in lc.keys()}\n",
    "df_lwbs = pd.DataFrame.from_dict(lwbs, orient='index')\n",
    "df_lwbs.sort_values(0, ascending=False, inplace=True)\n",
    "\n",
    "R = [item for sublist in K_nz_smogn for item in sublist]\n",
    "letter_counts = Counter(R)\n",
    "lsm = {x: count for x, count in letter_counts.items() if x in lc.keys()}\n",
    "df_lsm = pd.DataFrame.from_dict(lsm, orient='index')\n",
    "df_lsm.sort_values(0, ascending=False, inplace=True) \n",
    "\n",
    "df = pd.concat([df_lc, df_lcv, df_lwbs, df_lsm],axis=1)\n",
    "df.columns = [\"Bootstrap\",\"Noise Compensated\", \"Wild Bootstrap\", \"SMOGN\"]\n",
    "df.index = df.index.str.replace('_', ' ')\n",
    "df.index = df.index.str.replace(' L', '\\n L')\n",
    "df.index = df.index.str.replace(' R', '\\n R')\n",
    "df.plot(ax=axes, kind='bar', legend=True, fontsize=fs//1.25, width=0.75, edgecolor='black')\n",
    "\n",
    "plt.legend(loc='lower center', framealpha=1,fontsize=fs//1.25, ncol=4, bbox_to_anchor=(0.5, -0.02175))\n",
    "plt.xticks(rotation=45,ha='right',wrap=True)\n",
    "plt.suptitle('Predictive Radiomic Features \\nin Lasso Models',fontsize=fs//1.25)\n",
    "plt.ylabel('Frequency',fontsize=fs//1.25)\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.savefig('feats_auto.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.zeros((len(per_change),len(lc.keys())))\n",
    "X_r =  np.zeros((3,len(lc.keys())))\n",
    "X_nr =  np.zeros((3,len(lc.keys())))\n",
    "for j in np.arange(0,len(K_nz_nc)):\n",
    "    c = 0\n",
    "    for k in np.arange(0,len(K_nz_nc[j])):\n",
    "        if K_nz_nc[j][k] in lc.keys():\n",
    "            beta[j,c] = E_nz_nc[j][k]\n",
    "    \n",
    "            X_r[:,c] = (X_all_c.reshape(len(per_change),-1)[per_change>0.85,np.where(K_all_c == K_nz_nc[j][k])]).astype(float)\n",
    "            X_nr[:,c] = (X_all_c.reshape(len(per_change),-1)[per_change<0.3,np.where(K_all_c == K_nz_nc[j][k])]).astype(float)\n",
    "\n",
    "            c = c+1\n",
    "\n",
    "for i in np.arange(len(lc.keys())):\n",
    "    res = stats.wilcoxon(X_r[:,i].ravel(),X_nr[:,i].ravel())\n",
    "    if res.pvalue <= 0.05:\n",
    "        print(res.statistic, res.pvalue)     \n",
    "        print('Responder median:',np.median(X_r[:,i]/np.amax(X_r[:,i])),'and non-responder median',np.median(X_nr[:,i]/np.amax(X_nr[:,i])))  \n",
    "\n",
    "beta_r = (beta[per_change>0.85,:])\n",
    "beta_nr = (beta[per_change<0.3,:])\n",
    "res = stats.wilcoxon(beta_r.ravel(),beta_nr.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, recall_score, accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "fig,ax = plt.subplots(1,1,sharex=True,sharey=True)\n",
    "t = 0.3\n",
    "y_bin = per_change<t\n",
    "\n",
    "y_predicted_ls = results_ls < 0.3\n",
    "y_predicted_ls_aug = results_ls_aug < 0.3\n",
    "y_predicted_lct = pre_imp < 0.3\n",
    "y_predicted_aug = 1-results_lgrp_aug\n",
    "y_predicted_lr = 1-results_lgrp\n",
    "y_predicted_smogn = results_ls_smogn < 0.3\n",
    "y_predicted_wbs = results_ls_wbs < 0.3\n",
    "\n",
    "estimators = np.vstack((lct_change < 0.3,results_lgr,results_lgr_aug,results_ls < 0.3,results_ls_aug < 0.3,results_ls_smogn < 0.3,results_ls_wbs < 0.3))\n",
    "\n",
    "\n",
    "fprlsa, tprlsa, _ = roc_curve(y_bin,  y_predicted_ls_aug)\n",
    "fprls, tprls, _ = roc_curve(y_bin,  y_predicted_ls)\n",
    "fprl, tprl, _ = roc_curve(y_bin,  y_predicted_lct)\n",
    "fpra, tpra, ts_aug = roc_curve(y_bin, y_predicted_aug)\n",
    "fprlr, tprlr, ts = roc_curve(y_bin, y_predicted_lr)\n",
    "pra,rca,ta = precision_recall_curve(y_bin,y_predicted_aug)\n",
    "prlr,rclr,tlr = precision_recall_curve(y_bin,y_predicted_lr)\n",
    "\n",
    "scores = np.zeros((7,4))\n",
    "\n",
    "for j in np.arange(estimators.shape[0]):\n",
    "    spf = recall_score(y_bin,estimators[j,:],pos_label=0)\n",
    "    scores[j,0] = spf\n",
    "    sns = recall_score(y_bin,estimators[j,:])\n",
    "    scores[j,1] = sns\n",
    "    acc = accuracy_score(y_bin,estimators[j,:])\n",
    "    scores[j,2] = acc\n",
    "    prec = precision_score(y_bin,estimators[j,:])\n",
    "    scores[j,3] = prec\n",
    "\n",
    "fprlss, tprlss, _ = roc_curve(y_bin, y_predicted_smogn)\n",
    "fprlwb, tprlwb, _ = roc_curve(y_bin, y_predicted_wbs)\n",
    "roc_auc_lr = auc(fprlr, tprlr)\n",
    "roc_auc_lr_aug = auc(fpra, tpra)\n",
    "roc_auc_ls = auc(fprls, tprls)\n",
    "roc_auc_lct = auc(fprl, tprl)\n",
    "roc_auc_ls_aug = auc(fprlsa, tprlsa)\n",
    "roc_auc_ls_smogn = auc(fprlss, tprlss)\n",
    "roc_auc_ls_wbs = auc(fprlwb, tprlwb)\n",
    "\n",
    "ax.plot(fprlr, tprlr, label = 'Logistic Regression %0.2f' % roc_auc_lr, linewidth=5)\n",
    "ax.plot(fpra, tpra, label = 'Noise Compensated \\nLogistic Regression %0.2f' % roc_auc_lr_aug, linewidth=5)\n",
    "\n",
    "ax.plot(np.linspace(0,1,len(fpra)),np.linspace(0,1,len(fpra)),linewidth=5,color='black',linestyle='dashed')\n",
    "ax.set_title('Receiver Operating Characteristic',fontsize=fs)\n",
    "ax.set_xlabel('False positive rate \\n $(1-specificity$)',fontsize=fs)\n",
    "ax.set_ylabel('True positive rate \\n $(sensitivity$)',fontsize=fs)\n",
    "ax.xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax.yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax.legend(bbox_to_anchor=(0.38,0.2),fontsize=fs//1.5)\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec(res,pc):\n",
    "        y_true = pc\n",
    "        y_pred = res\n",
    "        epsilon_0 = 0.0\n",
    "        epsilon_max = 1.0\n",
    "        d_epsilon = 0.01\n",
    "        y = []\n",
    "        x = np.arange(epsilon_0,epsilon_max,d_epsilon)\n",
    "        epsilon = np.abs(y_true-y_pred) \n",
    "        for j in np.arange(len(x)):\n",
    "            c = 0\n",
    "            for k in np.arange(len(epsilon)):\n",
    "                if epsilon[k] < x[j]:\n",
    "                    c = c+1\n",
    "            y.append(c/len(y_true))\n",
    "        auc_rec = scipy.integrate.simps(y,x)/epsilon_max\n",
    "        return x, np.array(y), auc_rec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "fig,ax = plt.subplots(1,1,sharex=True,sharey=True)\n",
    "x_lct,y_lct,eta = rec(pre_imp,per_change)\n",
    "plt.plot(x_lct,y_lct,linewidth=5,label='LCT %0.2f' % eta, color='gray')\n",
    "# x_ls,y_ls,a,eta = rec(results_ls,per_change)\n",
    "# plt.plot(x_ls,y_ls,label='Lasso %0.2f' % eta)\n",
    "x_ls,y_ls,eta = rec(results_lsw,per_change)\n",
    "plt.plot(x_ls,y_ls,linewidth=5,label='Bootstrap Lasso %0.3f' % eta, color='tab:blue')\n",
    "x_smogn,y_smogn,eta = rec(results_ls_smogn,per_change)\n",
    "plt.plot(x_smogn,y_smogn,linewidth=5,label='SMOGN Lasso %0.3f' % eta, color='tab:red')\n",
    "x_lsa,y_lsa,eta = rec(results_ls_aug,per_change)\n",
    "plt.plot(x_lsa,y_lsa,linewidth=5,label='Noise Compensated Lasso %0.3f' % eta, color='tab:orange')\n",
    "x_wbs,y_wbs,eta = rec(results_ls_wbs,per_change)\n",
    "plt.plot(x_wbs,y_wbs,linewidth=5,label='Wild Bootstrap Lasso %0.3f' % eta, color=\"tab:green\")\n",
    "x_n,y_n,eta = rec(np.mean(per_change)*np.ones_like(per_change),per_change)\n",
    "plt.plot(x_n,y_n,linestyle='dashed',color='k',linewidth=5,label='Mean %0.3f' % eta)\n",
    "plt.xlabel('Absolute deviation',fontsize=fs)\n",
    "plt.ylabel('Accuracy',fontsize=fs)\n",
    "plt.xlim([0,1])\n",
    "plt.title('Regression Error Characteristic',fontsize=fs)\n",
    "ax.xaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax.yaxis.set_tick_params(labelleft=True,labelsize=fs//1.5)\n",
    "ax.legend(bbox_to_anchor=(0.215,0.375),fontsize=fs//1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_lasso, _, coefs_lasso = slm.lars_path(X0_ss0[:,0:20], y_train, method=\"lasso\", verbose=True)\n",
    "from itertools import cycle\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "plt.figure(1)\n",
    "coefs_lasso[coefs_lasso > 2.5] = 2.5\n",
    "coefs_lasso[coefs_lasso < -2.5] = -2.5\n",
    "for coef_l in zip(coefs_lasso):\n",
    "    l1 = plt.semilogx(alphas_lasso, np.squeeze(coef_l),linewidth=2.5)\n",
    "plt.xticks(fontsize=fs//1.5)\n",
    "plt.yticks(fontsize=fs//1.5)\n",
    "\n",
    "plt.xlabel(r\"Regularization $log(\\lambda)$\",fontsize=fs)\n",
    "plt.ylabel(r\"Weights $w(\\lambda)$\",fontsize=fs)\n",
    "plt.axis(\"tight\")\n",
    "plt.style.use('default')\n",
    "plt.xlim([1e-6,0.1])\n",
    "plt.ylim([-3,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14732b5bb7ad6abfe54a083b8d194ae3941adfb1b18321b588b21cb8f420fced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
