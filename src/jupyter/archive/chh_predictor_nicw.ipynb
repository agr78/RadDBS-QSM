{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import nibabel as nib\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score, roc_curve, auc\n",
    "import SimpleITK as sitk\n",
    "import six\n",
    "from radiomics import featureextractor \n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearnex import patch_sklearn, config_context\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn import preprocessing as skp\n",
    "from sklearn import model_selection as sms\n",
    "from sklearn import feature_selection as skf\n",
    "from sklearn import linear_model as slm\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from IPython.display import HTML\n",
    "import util as util\n",
    "import nibabel as nib\n",
    "import os\n",
    "import pickle\n",
    "from torch import nn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "def remove_keymap_conflicts(new_keys_set):\n",
    "    for prop in plt.rcParams:\n",
    "        if prop.startswith('keymap.'):\n",
    "            keys = plt.rcParams[prop]\n",
    "            remove_list = set(keys) & new_keys_set\n",
    "            for key in remove_list:\n",
    "                keys.remove(key)\n",
    "\n",
    "def multi_slice_viewer(volume):\n",
    "    remove_keymap_conflicts({'j', 'k'})\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.volume = volume\n",
    "    ax.index = volume.shape[0]//2\n",
    "    ax.imshow(volume[ax.index])\n",
    "    fig.canvas.mpl_connect('key_press_event', process_key)\n",
    "\n",
    "def process_key(event):\n",
    "    fig = event.canvas.figure\n",
    "    ax = fig.axes[0]\n",
    "    if event.key == 'j':\n",
    "        previous_slice(ax)\n",
    "    elif event.key == 'k':\n",
    "        next_slice(ax)\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "\n",
    "def previous_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index-1) % volume.shape[0] \n",
    "    ax.images[0].set_array(volume[ax.index])\n",
    "\n",
    "def next_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index+1) % volume.shape[0]\n",
    "    ax.images[0].set_array(volume[ax.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending arrays with segmentation 01_roi_combined.nii QSM, 00001_qsm.nii.gz LARO, QSM_lrr_01.nii and mask mask01.nii\n",
      "Appending arrays with segmentation 02_roi_combined.nii QSM, 00002_qsm.nii.gz LARO, QSM_lrr_02.nii and mask mask02.nii\n",
      "Appending arrays with segmentation 03_roi_combined.nii QSM, 00003_qsm.nii.gz LARO, QSM_lrr_03.nii and mask mask03.nii\n",
      "Appending arrays with segmentation 04_roi_combined.nii QSM, 00004_qsm.nii.gz LARO, QSM_lrr_04.nii and mask mask04.nii\n",
      "Appending arrays with segmentation 05_roi_combined.nii QSM, 00005_qsm.nii.gz LARO, QSM_lrr_05.nii and mask mask05.nii\n",
      "Appending arrays with segmentation 06_roi_combined.nii QSM, 00006_qsm.nii.gz LARO, QSM_lrr_06.nii and mask mask06.nii\n",
      "Appending arrays with segmentation 07_roi_combined.nii QSM, 00007_qsm.nii.gz LARO, QSM_lrr_07.nii and mask mask07.nii\n",
      "Appending arrays with segmentation 08_roi_combined.nii QSM, 00008_qsm.nii.gz LARO, QSM_lrr_08.nii and mask mask08.nii\n",
      "Appending arrays with segmentation 09_roi_combined.nii QSM, 00009_qsm.nii.gz LARO, QSM_lrr_09.nii and mask mask09.nii\n",
      "Appending arrays with segmentation 10_roi_combined.nii QSM, 00010_qsm.nii.gz LARO, QSM_lrr_10.nii and mask mask10.nii\n",
      "Appending arrays with segmentation 11_roi_combined.nii QSM, 00011_qsm.nii.gz LARO, QSM_lrr_11.nii and mask mask11.nii\n",
      "Appending arrays with segmentation 13_roi_combined.nii QSM, 00013_qsm.nii.gz LARO, QSM_lrr_13.nii and mask mask13.nii\n",
      "Appending arrays with segmentation 14_roi_combined.nii QSM, 00014_qsm.nii.gz LARO, QSM_lrr_14.nii and mask mask14.nii\n",
      "Appending arrays with segmentation 16_roi_combined.nii QSM, 00016_qsm.nii.gz LARO, QSM_lrr_16.nii and mask mask16.nii\n",
      "Appending arrays with segmentation 18_roi_combined.nii QSM, 00018_qsm.nii.gz LARO, QSM_lrr_18.nii and mask mask18.nii\n",
      "Appending arrays with segmentation 19_roi_combined.nii QSM, 00019_qsm.nii.gz LARO, QSM_lrr_19.nii and mask mask19.nii\n",
      "Appending arrays with segmentation 20_roi_combined.nii QSM, 00020_qsm.nii.gz LARO, QSM_lrr_20.nii and mask mask20.nii\n",
      "Appending arrays with segmentation 21_roi_combined.nii QSM, 00021_qsm.nii.gz LARO, QSM_lrr_21.nii and mask mask21.nii\n",
      "Appending arrays with segmentation 22_roi_combined.nii QSM, 00022_qsm.nii.gz LARO, QSM_lrr_22.nii and mask mask22.nii\n",
      "Appending arrays with segmentation 23_roi_combined.nii QSM, 00023_qsm.nii.gz LARO, QSM_lrr_23.nii and mask mask23.nii\n",
      "Appending arrays with segmentation 24_roi_combined.nii QSM, 00024_qsm.nii.gz LARO, QSM_lrr_24.nii and mask mask24.nii\n",
      "Appending arrays with segmentation 25_roi_combined.nii QSM, 00025_qsm.nii.gz LARO, QSM_lrr_25.nii and mask mask25.nii\n",
      "Appending arrays with segmentation 26_roi_combined.nii QSM, 00026_qsm.nii.gz LARO, QSM_lrr_26.nii and mask mask26.nii\n",
      "Appending arrays with segmentation 27_roi_combined.nii QSM, 00027_qsm.nii.gz LARO, QSM_lrr_27.nii and mask mask27.nii\n",
      "Appending arrays with segmentation 28_roi_combined.nii QSM, 00028_qsm.nii.gz LARO, QSM_lrr_28.nii and mask mask28.nii\n",
      "Appending arrays with segmentation 29_roi_combined.nii QSM, 00029_qsm.nii.gz LARO, QSM_lrr_29.nii and mask mask29.nii\n",
      "Appending arrays with segmentation 30_roi_combined.nii QSM, 00030_qsm.nii.gz LARO, QSM_lrr_30.nii and mask mask30.nii\n",
      "Appending arrays with segmentation 31_roi_combined.nii QSM, 00031_qsm.nii.gz LARO, QSM_lrr_31.nii and mask mask31.nii\n",
      "Appending arrays with segmentation 32_roi_combined.nii QSM, 00032_qsm.nii.gz LARO, QSM_lrr_32.nii and mask mask32.nii\n",
      "Appending arrays with segmentation 33_roi_combined.nii QSM, 00033_qsm.nii.gz LARO, QSM_lrr_33.nii and mask mask33.nii\n",
      "Appending arrays with segmentation 34_roi_combined.nii QSM, 00034_qsm.nii.gz LARO, QSM_lrr_34.nii and mask mask34.nii\n",
      "Appending arrays with segmentation 35_roi_combined.nii QSM, 00035_qsm.nii.gz LARO, QSM_lrr_35.nii and mask mask35.nii\n",
      "Appending arrays with segmentation 36_roi_combined.nii QSM, 00036_qsm.nii.gz LARO, QSM_lrr_36.nii and mask mask36.nii\n",
      "Appending arrays with segmentation 37_roi_combined.nii QSM, 00037_qsm.nii.gz LARO, QSM_lrr_37.nii and mask mask37.nii\n",
      "Appending arrays with segmentation 38_roi_combined.nii QSM, 00038_qsm.nii.gz LARO, QSM_lrr_38.nii and mask mask38.nii\n",
      "Appending arrays with segmentation 39_roi_combined.nii QSM, 00039_qsm.nii.gz LARO, QSM_lrr_39.nii and mask mask39.nii\n",
      "Appending arrays with segmentation 40_roi_combined.nii QSM, 00040_qsm.nii.gz LARO, QSM_lrr_40.nii and mask mask40.nii\n"
     ]
    }
   ],
   "source": [
    "# Set window level\n",
    "level = 0\n",
    "window = 500\n",
    "m1=level-window/2\n",
    "m2=level+window/2\n",
    "visualize = 1\n",
    "# Load data\n",
    "nrows = 256\n",
    "ncols = 256\n",
    "nslices = 160\n",
    "segs = []\n",
    "qsms = []\n",
    "laros = []\n",
    "voxel_sizes = []\n",
    "trackers = []\n",
    "q_directory = '/home/ali/RadDBS-QSM/data/nii/chh/orig/qsm/'\n",
    "q_directory = os.listdir(q_directory)\n",
    "q_directory = sorted(q_directory)\n",
    "qu_directory = '/home/ali/RadDBS-QSM/data/nii/chh/lr_roi_ft'\n",
    "qu_directory = os.listdir(qu_directory)\n",
    "qu_directory = sorted(qu_directory)\n",
    "s_directory = '/home/ali/RadDBS-QSM/data/nii/chh/orig/seg/'\n",
    "s_directory = os.listdir(s_directory)\n",
    "s_directory = sorted(s_directory)\n",
    "m_directory = '/home/ali/RadDBS-QSM/data/nii/chh/masks'\n",
    "m_directory = os.listdir(m_directory)\n",
    "m_directory = sorted(m_directory)\n",
    "case_list = []\n",
    "d_count = 0\n",
    "if visualize == 1:\n",
    "    for filename in q_directory:\n",
    "    \n",
    "        seg_filename = s_directory[d_count]\n",
    "        laro_filename = qu_directory[d_count]\n",
    "        mask_filename = m_directory[d_count]\n",
    "        seg = nib.load('/home/ali/RadDBS-QSM/data/nii/chh/orig/seg/'+seg_filename)\n",
    "        mask = nib.load('/home/ali/RadDBS-QSM/data/nii/chh/masks/'+mask_filename)\n",
    "        voxel_size = seg.header['pixdim'][0:3]\n",
    "        voxel_sizes.append(voxel_size)\n",
    "        segs.append(seg.get_fdata()[:nrows,:ncols,:nslices])\n",
    "        qsm = nib.load('/home/ali/RadDBS-QSM/data/nii/chh/orig/qsm/'+filename)\n",
    "        qsms.append(qsm.get_fdata()[:nrows,:ncols,:nslices])\n",
    "\n",
    "        laro = nib.load('/home/ali/RadDBS-QSM/data/nii/chh/lr_roi_ft/'+laro_filename)\n",
    "        laros.append(1000*laro.get_fdata()[:nrows,:ncols,:nslices])\n",
    "        print('Appending arrays with segmentation',seg_filename,'QSM,',filename,\n",
    "              'LARO,',laro_filename,'and mask',mask_filename)\n",
    "        case_list.append(filename)\n",
    "        n_cases = len(segs)\n",
    "        d_count = d_count+1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjects</th>\n",
       "      <th>LCT</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>UPDRSⅢ</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Preoperative Off-Medicine</td>\n",
       "      <td>Preoperative On-Medicine</td>\n",
       "      <td>Postoperative  Off-Medicine &amp;Off-Stimulation\\n</td>\n",
       "      <td>Postoperative  Off-Medicine &amp;On-Stimulation</td>\n",
       "      <td>Postoperative  On-Medicine &amp;On-Stimulation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>62.26%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>75.00%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>51.72%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>28</td>\n",
       "      <td>52</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>22.58%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62</td>\n",
       "      <td>48</td>\n",
       "      <td>61</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>55.88%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>75.44%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57</td>\n",
       "      <td>14</td>\n",
       "      <td>55</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>84.00%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>68</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>41.86%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43</td>\n",
       "      <td>25</td>\n",
       "      <td>48</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>72.58%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62</td>\n",
       "      <td>17</td>\n",
       "      <td>56</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>81.25%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>70.21%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94</td>\n",
       "      <td>28</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>63.95%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>89.55%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.0</td>\n",
       "      <td>88.46%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>62.50%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>27</td>\n",
       "      <td>72</td>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.0</td>\n",
       "      <td>48.00%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.0</td>\n",
       "      <td>14.29%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>84</td>\n",
       "      <td>86</td>\n",
       "      <td>43</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.0</td>\n",
       "      <td>36.62%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>47</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.0</td>\n",
       "      <td>54.90%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.0</td>\n",
       "      <td>81.63%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.0</td>\n",
       "      <td>71.43%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.0</td>\n",
       "      <td>90.32%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.0</td>\n",
       "      <td>65.85%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "      <td>43</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24.0</td>\n",
       "      <td>74.29%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>83.61%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26.0</td>\n",
       "      <td>58.97%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.0</td>\n",
       "      <td>5.88%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28.0</td>\n",
       "      <td>76.67%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>68</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29.0</td>\n",
       "      <td>81.01%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79</td>\n",
       "      <td>15</td>\n",
       "      <td>79</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30.0</td>\n",
       "      <td>62.35%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>32</td>\n",
       "      <td>92</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31.0</td>\n",
       "      <td>27.91%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32.0</td>\n",
       "      <td>31.48%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>37</td>\n",
       "      <td>64</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33.0</td>\n",
       "      <td>80.33%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34.0</td>\n",
       "      <td>40.74%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>36</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35.0</td>\n",
       "      <td>16.13%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93</td>\n",
       "      <td>78</td>\n",
       "      <td>69</td>\n",
       "      <td>47</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36.0</td>\n",
       "      <td>59.26%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>22</td>\n",
       "      <td>57</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37.0</td>\n",
       "      <td>61.40%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57</td>\n",
       "      <td>22</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38.0</td>\n",
       "      <td>95.92%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39.0</td>\n",
       "      <td>79.35%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40.0</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>27</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subjects    LCT   Unnamed: 2                     UPDRSⅢ  \\\n",
       "0        NaN     NaN         NaN  Preoperative Off-Medicine   \n",
       "1        1.0  62.26%         NaN                         53   \n",
       "2        2.0  75.00%         NaN                         32   \n",
       "3        3.0  51.72%         NaN                         58   \n",
       "4        4.0  22.58%         NaN                         62   \n",
       "5        5.0  55.88%         NaN                         34   \n",
       "6        6.0  75.44%         NaN                         57   \n",
       "7        7.0  84.00%         NaN                        100   \n",
       "8        8.0  41.86%         NaN                         43   \n",
       "9        9.0  72.58%         NaN                         62   \n",
       "10      10.0  81.25%         NaN                         64   \n",
       "11      11.0  70.21%         NaN                         94   \n",
       "12      12.0  63.95%         NaN                         86   \n",
       "13      13.0  89.55%         NaN                         67   \n",
       "14      14.0  88.46%         NaN                         26   \n",
       "15      15.0  62.50%         NaN                         72   \n",
       "16      16.0  48.00%         NaN                         25   \n",
       "17      17.0  14.29%         NaN                         98   \n",
       "18      18.0  36.62%         NaN                         71   \n",
       "19      19.0  54.90%         NaN                         51   \n",
       "20      20.0  81.63%         NaN                         49   \n",
       "21      21.0  71.43%         NaN                         49   \n",
       "22      22.0  90.32%         NaN                         31   \n",
       "23      23.0  65.85%         NaN                         41   \n",
       "24      24.0  74.29%         NaN                         35   \n",
       "25      25.0  83.61%         NaN                         61   \n",
       "26      26.0  58.97%         NaN                         78   \n",
       "27      27.0   5.88%         NaN                         51   \n",
       "28      28.0  76.67%         NaN                         60   \n",
       "29      29.0  81.01%         NaN                         79   \n",
       "30      30.0  62.35%         NaN                         85   \n",
       "31      31.0  27.91%         NaN                         43   \n",
       "32      32.0  31.48%         NaN                         54   \n",
       "33      33.0  80.33%         NaN                         61   \n",
       "34      34.0  40.74%         NaN                         54   \n",
       "35      35.0  16.13%         NaN                         93   \n",
       "36      36.0  59.26%         NaN                         54   \n",
       "37      37.0  61.40%         NaN                         57   \n",
       "38      38.0  95.92%         NaN                         49   \n",
       "39      39.0  79.35%         NaN                         92   \n",
       "40      40.0  50.00%         NaN                         54   \n",
       "\n",
       "                  Unnamed: 4                                       Unnamed: 5  \\\n",
       "0   Preoperative On-Medicine  Postoperative  Off-Medicine &Off-Stimulation\\n    \n",
       "1                         20                                               40   \n",
       "2                          8                                               26   \n",
       "3                         28                                               52   \n",
       "4                         48                                               61   \n",
       "5                         15                                               44   \n",
       "6                         14                                               55   \n",
       "7                         16                                               68   \n",
       "8                         25                                               48   \n",
       "9                         17                                               56   \n",
       "10                        12                                               33   \n",
       "11                        28                                               31   \n",
       "12                        31                                               28   \n",
       "13                         7                                               64   \n",
       "14                         3                                               31   \n",
       "15                        27                                               72   \n",
       "16                        13                                               17   \n",
       "17                        84                                               86   \n",
       "18                        45                                               49   \n",
       "19                        23                                               37   \n",
       "20                         9                                               20   \n",
       "21                        14                                               42   \n",
       "22                         3                                               46   \n",
       "23                        14                                               43   \n",
       "24                         9                                               21   \n",
       "25                        10                                               60   \n",
       "26                        32                                               64   \n",
       "27                        48                                               45   \n",
       "28                        14                                               68   \n",
       "29                        15                                               79   \n",
       "30                        32                                               92   \n",
       "31                        31                                               31   \n",
       "32                        37                                               64   \n",
       "33                        12                                               53   \n",
       "34                        32                                               64   \n",
       "35                        78                                               69   \n",
       "36                        22                                               57   \n",
       "37                        22                                               56   \n",
       "38                         2                                               58   \n",
       "39                        19                                              NaN   \n",
       "40                        27                                               34   \n",
       "\n",
       "                                     Unnamed: 6  \\\n",
       "0   Postoperative  Off-Medicine &On-Stimulation   \n",
       "1                                            15   \n",
       "2                                            17   \n",
       "3                                            18   \n",
       "4                                            36   \n",
       "5                                            29   \n",
       "6                                            32   \n",
       "7                                            12   \n",
       "8                                            26   \n",
       "9                                            25   \n",
       "10                                           12   \n",
       "11                                           34   \n",
       "12                                           27   \n",
       "13                                           19   \n",
       "14                                           18   \n",
       "15                                           44   \n",
       "16                                            8   \n",
       "17                                           43   \n",
       "18                                           47   \n",
       "19                                           21   \n",
       "20                                            3   \n",
       "21                                            8   \n",
       "22                                           28   \n",
       "23                                           24   \n",
       "24                                           15   \n",
       "25                                           21   \n",
       "26                                           29   \n",
       "27                                           27   \n",
       "28                                           43   \n",
       "29                                           32   \n",
       "30                                           40   \n",
       "31                                            6   \n",
       "32                                           28   \n",
       "33                                           21   \n",
       "34                                           36   \n",
       "35                                           47   \n",
       "36                                           12   \n",
       "37                                           11   \n",
       "38                                           14   \n",
       "39                                           63   \n",
       "40                                            9   \n",
       "\n",
       "                                    Unnamed: 7  Unnamed: 8  \n",
       "0   Postoperative  On-Medicine &On-Stimulation         NaN  \n",
       "1                                            7         NaN  \n",
       "2                                           10         NaN  \n",
       "3                                           20         NaN  \n",
       "4                                           28         NaN  \n",
       "5                                           22         NaN  \n",
       "6                                           16         NaN  \n",
       "7                                          NaN         NaN  \n",
       "8                                           26         NaN  \n",
       "9                                           11         NaN  \n",
       "10                                         NaN         NaN  \n",
       "11                                          43         NaN  \n",
       "12                                          18         NaN  \n",
       "13                                         NaN         NaN  \n",
       "14                                          17         NaN  \n",
       "15                                          38         NaN  \n",
       "16                                           8         NaN  \n",
       "17                                          34         NaN  \n",
       "18                                          46         NaN  \n",
       "19                                          18         NaN  \n",
       "20                                         NaN         NaN  \n",
       "21                                         NaN         NaN  \n",
       "22                                         NaN         NaN  \n",
       "23                                          24         NaN  \n",
       "24                                         NaN         NaN  \n",
       "25                                          17         NaN  \n",
       "26                                          21         NaN  \n",
       "27                                          20         NaN  \n",
       "28                                         NaN         NaN  \n",
       "29                                          28         NaN  \n",
       "30                                         NaN         NaN  \n",
       "31                                         NaN         NaN  \n",
       "32                                          25         NaN  \n",
       "33                                          15         NaN  \n",
       "34                                          29         NaN  \n",
       "35                                          44         NaN  \n",
       "36                                         NaN         NaN  \n",
       "37                                         NaN         NaN  \n",
       "38                                          12         NaN  \n",
       "39                                         NaN         NaN  \n",
       "40                                           8         NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/ali/RadDBS-QSM/data/xlxs/updrs_iii_chh.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 01_roi_combined.nii for case 1.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 02_roi_combined.nii for case 2.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 03_roi_combined.nii for case 3.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 04_roi_combined.nii for case 4.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 05_roi_combined.nii for case 5.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 06_roi_combined.nii for case 6.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 07_roi_combined.nii for case 7.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 08_roi_combined.nii for case 8.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 09_roi_combined.nii for case 9.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 10_roi_combined.nii for case 10.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 11_roi_combined.nii for case 11.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 13_roi_combined.nii for case 13.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 14_roi_combined.nii for case 14.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 16_roi_combined.nii for case 16.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 18_roi_combined.nii for case 18.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 19_roi_combined.nii for case 19.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 20_roi_combined.nii for case 20.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 21_roi_combined.nii for case 21.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 22_roi_combined.nii for case 22.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 23_roi_combined.nii for case 23.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 24_roi_combined.nii for case 24.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 25_roi_combined.nii for case 25.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 26_roi_combined.nii for case 26.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 27_roi_combined.nii for case 27.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 28_roi_combined.nii for case 28.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 29_roi_combined.nii for case 29.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 30_roi_combined.nii for case 30.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 31_roi_combined.nii for case 31.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 32_roi_combined.nii for case 32.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 33_roi_combined.nii for case 33.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 34_roi_combined.nii for case 34.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 35_roi_combined.nii for case 35.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 36_roi_combined.nii for case 36.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 37_roi_combined.nii for case 37.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 38_roi_combined.nii for case 38.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 39_roi_combined.nii for case 39.0\n",
      "Found ROIs [0. 1. 2. 3. 4. 5. 6.] at segmentation directory file 40_roi_combined.nii for case 40.0\n"
     ]
    }
   ],
   "source": [
    "# Patient IDs\n",
    "subject_id = np.asarray(df[df.columns[0]])[1:]\n",
    "\n",
    "# Only extract ROI if it is present in all cases\n",
    "seg_labels_all = segs[0]\n",
    "case_number = np.zeros_like(np.asarray(s_directory))\n",
    "for i in range(n_cases):\n",
    "    case_number[i] = float(s_directory[i][:2])\n",
    "subject_id_corr = subject_id[np.in1d(subject_id,case_number)]\n",
    "for i in np.arange(n_cases):\n",
    "    try:\n",
    "        print('Found ROIs',str(np.unique(segs[i])),'at segmentation directory file',s_directory[i],'for case',str(subject_id_corr[i]))\n",
    "    except:\n",
    "        print('Case',subject_id[i],'quarantined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_updrs_iii_off =  np.asarray(df[df.columns[3]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])                                \n",
    "pre_updrs_iii_on =  np.asarray(df[df.columns[4]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])\n",
    "post_updrs_iii_off =  np.asarray(df[df.columns[6]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])\n",
    "\n",
    "per_change = (np.asarray(pre_updrs_iii_off).astype(float)-np.asarray(post_updrs_iii_off).astype(float))/(np.asarray(pre_updrs_iii_off).astype(float))\n",
    "lct_change = (np.asarray(pre_updrs_iii_off).astype(float)-(np.asarray(pre_updrs_iii_on)).astype(float))/(np.asarray(pre_updrs_iii_off).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"radiomics\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "reextract = 0\n",
    "\n",
    "# Assume all voxel sizes are identical\n",
    "voxel_size = (0.9,0.9,0.9)\n",
    "if reextract == 1:\n",
    "    # Generate feature structure Phi from all ROIs and all cases\n",
    "    extractor = featureextractor.RadiomicsFeatureExtractor()\n",
    "    extractor.enableAllFeatures()\n",
    "    extractor.enableAllImageTypes()\n",
    "    extractor.enableFeatureClassByName('shape2D',enabled = False)\n",
    "\n",
    "    seg_labels_all = np.unique(np.asarray(segs))\n",
    "    Phi_gt = []\n",
    "    Phi_vd = []\n",
    "    Phi_lr = []\n",
    "    seg_labels = []\n",
    "    reextract = 0\n",
    "    x_row_gt = []\n",
    "    x_row_lr = []\n",
    "\n",
    "    keylib = []\n",
    "    roilib = []\n",
    "    loop_count = 1\n",
    "    n_rois = seg_labels_all[seg_labels_all>0].__len__()\n",
    "    roi_names = np.asarray(['Background','Right substantia nigra','Right subthalamic nucleus',\n",
    "                            'Left subthalamic nucleus', 'Left substantia nigra', 'Right dentate nucleus', 'Left dentate nucleus'])\n",
    "    for i in np.arange(subject_id_corr.__len__()):\n",
    "        seg_sitk = sitk.GetImageFromArray(segs[i])\n",
    "        seg_sitk.SetSpacing(voxel_size)\n",
    "        qsm_sitk_gt = sitk.GetImageFromArray(qsms[i])\n",
    "        qsm_sitk_gt.SetSpacing(voxel_size)\n",
    "        qsm_sitk_lr = sitk.GetImageFromArray(laros[i])\n",
    "        qsm_sitk_lr.SetSpacing(voxel_size)\n",
    "        # Index back since subject 12 is missing ROIs\n",
    "        for j in seg_labels_all:\n",
    "            if j>0:\n",
    "                fv_count = 0\n",
    "                featureVector_gt = extractor.execute(qsm_sitk_gt,seg_sitk,label=int(j));\n",
    "                featureVector_lr = extractor.execute(qsm_sitk_lr,seg_sitk,label=int(j));\n",
    "                Phi_gt.append(featureVector_gt)\n",
    "                Phi_lr.append(featureVector_lr)\n",
    "                for key, value in six.iteritems(featureVector_gt):\n",
    "                    if 'diagnostic' in key:\n",
    "                        next\n",
    "                    else:\n",
    "                        x_row_gt.append(featureVector_gt[key])\n",
    "                        x_row_lr.append(featureVector_lr[key])\n",
    "                        fv_count = fv_count+1\n",
    "                        keylib.append(key)\n",
    "                        roilib.append(roi_names[int(j)])\n",
    "                x_row_gt.append(pre_updrs_iii_off[i])\n",
    "                x_row_lr.append(pre_updrs_iii_off[i])\n",
    "                fv_count = fv_count+1\n",
    "        print('Extracting features for subject',subject_id_corr[i],'and appending feature matrix with vector of length',fv_count,'with UPDRS score',pre_updrs_iii_off[i])\n",
    "                \n",
    "    X0_gt = np.array(x_row_gt)\n",
    "    X0_lr = np.array(x_row_lr)\n",
    "    np.save('/home/ali/RadDBS-QSM/data/npy/rp/X0_gt_chh_rois_rp.npy',X0_gt)\n",
    "    np.save('/home/ali/RadDBS-QSM/data/npy/rp/X0_lr_chh_rois_rp.npy',X0_lr)\n",
    "\n",
    "    K = np.asarray(keylib)\n",
    "    R = np.asarray(roi_names)\n",
    "    np.save('/home/ali/RadDBS-QSM/data/npy/rp/K_chh_rp.npy',K)\n",
    "    np.save('/home/ali/RadDBS-QSM/data/npy/rp/R_chh_rp.npy',R)\n",
    "\n",
    "    print('Saving ground truth feature vector')\n",
    "    with open('/home/ali/RadDBS-QSM/data/npy/rp/Phi_mcl_gt_roi_chh_rp', 'wb') as fp:  \n",
    "        pickle.dump(Phi_gt, fp)\n",
    "    \n",
    "    print('Saving undersampled feature vector')\n",
    "    with open('/home/ali/RadDBS-QSM/data/npy/rp/Phi_mcl_lr_roi_chh_rp', 'wb') as fp:  \n",
    "        pickle.dump(Phi_lr, fp)\n",
    "\n",
    "else:\n",
    "    X0_gt = np.load('/home/ali/RadDBS-QSM/data/npy/rp/X0_gt_chh_rois_rp.npy')\n",
    "    X0_lr = np.load('/home/ali/RadDBS-QSM/data/npy/rp/X0_lr_chh_rois_rp.npy')\n",
    "    K = np.load('/home/ali/RadDBS-QSM/data/npy/rp/K_chh_rp.npy')\n",
    "    R = np.load('/home/ali/RadDBS-QSM/data/npy/rp/R_chh_rp.npy')\n",
    "    n_rois = R.shape[0]-1\n",
    "    with open('/home/ali/RadDBS-QSM/data/npy/rp/Phi_mcl_gt_roi_chh_rp', \"rb\") as fp:  \n",
    "        Phi_gt = pickle.load(fp)\n",
    "    \n",
    "    with open('/home/ali/RadDBS-QSM/data/npy/rp/Phi_mcl_lr_roi_chh_rp', \"rb\") as fp:  \n",
    "        Phi_lr = pickle.load(fp)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1596\n",
    "n_rois = 6\n",
    "X_all_c = X0_gt.reshape(n_cases,n_rois,n_features)[:,0:4,:]\n",
    "K_all_c = K.reshape(n_cases,n_rois,n_features-1)[:,0:4,:]\n",
    "K_all_c = K_all_c[0,:,:].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cases = len(per_change)\n",
    "n_roisc = Phi_gt.__len__()/n_cases\n",
    "L = int(len(X0_gt)/n_cases)\n",
    "n_features = int(L/n_rois)\n",
    "subsc = subject_id_corr\n",
    "pre_updrs_off = pre_updrs_iii_off\n",
    "results_ls_aug = np.zeros_like(per_change)\n",
    "\n",
    "results_lgr_aug = np.zeros_like(per_change)\n",
    "results_lgrp_aug = np.zeros_like(per_change)\n",
    "results_ls = np.zeros_like(per_change)\n",
    "results_lgr = np.zeros_like(per_change)\n",
    "results_lgrp = np.zeros_like(per_change)\n",
    "r = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution of: 0.5669595264848581 0.20858808082526412 -0.24474920851943865\n",
      "Resampled to size (37,)\n",
      "Resampled to size (38,)\n",
      "LassoCV score for 2 is [0.41454692] from dataset of size (304, 2385)\n",
      "LassoCV score for 3 is [0.40495911] from dataset of size (304, 5385)\n"
     ]
    }
   ],
   "source": [
    "retrain = 1\n",
    "if retrain == 1:\n",
    "\n",
    "  Js = []\n",
    "  aug = True\n",
    "  err_var = np.zeros_like(per_change)\n",
    "  rerror = np.zeros_like(per_change)\n",
    "  kappa = []\n",
    "  K_nz = []\n",
    "  E_nz = []\n",
    "  c = 0\n",
    "\n",
    "  K_all_c = np.append(K_all_c,['pre updrs']*5)\n",
    "  for j in np.arange(len(subsc)):\n",
    "      test_id = subsc[j]\n",
    "      test_index = subsc == test_id\n",
    "      train_index = subsc != test_id\n",
    "      X_train = X_all_c[train_index,:,:]\n",
    "      X_test = X_all_c[test_index,:,:]\n",
    "      y_train = per_change[train_index]\n",
    "      y_test = per_change[test_index]\n",
    "\n",
    "      y_cat = y_train <= 0.3\n",
    "      idy = np.where(y_cat==1)\n",
    "      # Cross validation\n",
    "                                            \n",
    "      X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                  X_train,train_index,X_test,\n",
    "                                                  test_index,pre_updrs_off,None,None,None,None,None,None,None,None,None,False,False,False)\n",
    "      cvn = 5\n",
    "      cv_scores = np.zeros((cvn,1))\n",
    "      cv_lgr_scores = np.zeros((cvn,1))\n",
    "      rs = 1\n",
    "      rcfs = 1000\n",
    "      (mu, sigma) = stats.norm.fit(y_train)\n",
    "      kappa = stats.skew(y_train)\n",
    "      print('Label distribution of:',mu,sigma,kappa)\n",
    "      for jj in np.arange(2,cvn):\n",
    "        # Resample to avoid stratification errors\n",
    "        while np.sum(y_cat) < cvn:\n",
    "          np.random.seed(rs)\n",
    "          idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "          X0_ss0 = np.append(X0_ss0,X0_ss0[idyr,:].reshape(1,-1),axis=0)\n",
    "          y_train = np.append(y_train,y_train[idyr])\n",
    "          y_cat = y_train <= 0.3\n",
    "          rs = rs+1\n",
    "          print('Resampled to size',y_train.shape)\n",
    "        if aug == True:\n",
    "          y_train_n = y_train+(2.326*sigma)*np.random.normal(0,1,1)\n",
    "          y_train = np.hstack((y_train,y_train_n))\n",
    "          y_cat = y_train <= 0.3\n",
    "          X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "      \n",
    "      for jj in np.arange(2,cvn):\n",
    "        skf_g = sms.StratifiedKFold(n_splits=jj,shuffle=True,random_state=0)\n",
    "        skf_gen = skf_g.split(X0_ss0,y_cat)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None)\n",
    "        with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "          # Feature selection\n",
    "          warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "          sel = skf.RFECV(lasso,step=rcfs,cv=skf_gen,n_jobs=1)\n",
    "          # Stratifies classifiers automatically\n",
    "          sel_lr = skf.RFECV(lgr,step=rcfs,cv=jj,n_jobs=1)\n",
    "          X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "          # X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "          est_ls = lasso.fit(X0_ss,y_train)\n",
    "          # est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "          cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "          # cv_lgr_scores[jj] = est_lgr.score(X0_ssl,y_cat)\n",
    "          print('LassoCV score for',jj,'is',cv_scores[jj],'from dataset of size',X0_ss.shape)\n",
    "          \n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "        best_cv = np.argmax(cv_scores)\n",
    "        best_cv_lgr = np.argmax(cv_lgr_scores)\n",
    "\n",
    "        # Break any ties\n",
    "        if np.sum(cv_scores == best_cv) > 1:\n",
    "          cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "          cv_lgr_scores_tb = np.zeros((np.sum(cv_scores == best_cv_lgr),1))\n",
    "          for jjj in (cv_scores == cv_scores[best_cv]):\n",
    "            if jjj > 0:\n",
    "              print('Breaking tie')\n",
    "              skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "              skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "              X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "              X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              # est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "              lasso = slm.LassoLarsCV(max_iter=1000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "              # lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              # est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "              cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "              # cv_lgr_scores_tb[jj] = est_lgr.score(sel_lr.fit_transform(X0_ss0,y_cat),y_cat)\n",
    "          best_cv = np.argmax(cv_scores_tb)\n",
    "          best_cv_lgr = np.argmax(cv_lgr_scores_tb)\n",
    "        \n",
    "        # Fit whole dataset with optimal cv\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None)\n",
    "        sel = skf.RFECV(lasso,step=rcfs,cv=best_cv)\n",
    "        sel_lr = skf.RFECV(lgr,step=rcfs,cv=best_cv)\n",
    "        X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "        X_test_ss = sel.transform(X_test_ss0)\n",
    "        X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "        X_test_ssl = sel_lr.transform(X_test_ss0)\n",
    "        #K_ss = sel.transform(K_all_c.reshape(1,-1))\n",
    "\n",
    "      # LASSO\n",
    "      with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=best_cv,class_weight=None)\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "      #   est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "      # results_lgr_aug[c] = est_lgr.predict(X_test_ssl)\n",
    "      # results_lgrp_aug[c] = est_lgr.predict_proba(X_test_ssl)[0][0]\n",
    "      results_ls_aug[c] = est_ls.predict(X_test_ss)\n",
    "      print('Lasso predicts',str(np.round(results_ls_aug[c],4)),'and logistic regression predicts',results_lgr_aug[c],\n",
    "                'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv,'and',sum(y_cat),'minority cases')\n",
    "      # K_nz.append(np.squeeze(K_ss)[est_ls.coef_>0])\n",
    "      # E_nz.append(est_ls.coef_[est_ls.coef_>0])\n",
    "      c=c+1\n",
    "\n",
    "  # np.save('results_ls_aug_d.npy',results_ls_aug)\n",
    "  # np.save('results_lgr_aug_d.npy',results_lgr_aug)\n",
    "  # np.save('results_lgrp_aug_d.npy',results_lgrp_aug)\n",
    "\n",
    "else:\n",
    "  results_ls_aug = np.load('results_ls_aug_d.npy')\n",
    "  results_lgr_aug = np.load('results_lgr_aug_d.npy')\n",
    "  #results_lgrp_aug = np.load('results_lgr_aug_d.npy')\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do wild bootstrap\n",
    "# Repeat CHH with wild bootstrap and higher zscore\n",
    "Js = []\n",
    "aug = False\n",
    "err_var = np.zeros_like(per_change)\n",
    "rerror = np.zeros_like(per_change)\n",
    "kappa = []\n",
    "c = 0\n",
    "for j in np.arange(c,len(subsc)):\n",
    "    test_id = subsc[j]\n",
    "    test_index = subsc == test_id\n",
    "    train_index = subsc != test_id\n",
    "    X_train = X_all_c[train_index,:,:]\n",
    "    X_test = X_all_c[test_index,:,:]\n",
    "    y_train = per_change[train_index]\n",
    "    y_test = per_change[test_index]\n",
    "\n",
    "    y_cat = y_train <= 0.3\n",
    "    idy = np.where(y_cat==1)\n",
    "    # Cross validation\n",
    "                                          \n",
    "    X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                X_train,train_index,X_test,\n",
    "                                                test_index,pre_updrs_off,None,None,None,None,None,None,None,None,None,False,False,False)\n",
    "    X0_ss00 = X0_ss0\n",
    "    y_train_0 = y_train\n",
    "    cvn = 5\n",
    "\n",
    "    cv_scores = np.zeros((cvn,1))\n",
    "    rs = 1\n",
    "    rcfs = 1000\n",
    "    (mu, sigma) = stats.norm.fit(y_train)\n",
    "    kappa = stats.skew(y_train)\n",
    "    print('Label distribution of:',mu,sigma,kappa)\n",
    "    for jj in np.arange(2,cvn):\n",
    "      # Resample to avoid stratification errors\n",
    "      while np.sum(y_cat) < cvn:\n",
    "        np.random.seed(rs)\n",
    "        idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "        X0_ss0 = np.append(X0_ss0,X0_ss0[idyr,:].reshape(1,-1),axis=0)\n",
    "        y_train = np.append(y_train,y_train[idyr])\n",
    "        y_cat = y_train <= 0.3\n",
    "        rs = rs+1\n",
    "        print('Resampled to size',y_train.shape)\n",
    "        ls0 = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        est0 = ls0.fit(X0_ss0,y_train)\n",
    "        eps = y_train-ls0.predict(X0_ss0)\n",
    "        eps_v = eps*np.random.normal(0,1,1)\n",
    "        y_train_0 = y_train\n",
    "      if aug == True:\n",
    "        y_train_n = y_train+(1.96*sigma)*np.random.normal(0,1,1)\n",
    "        y_train = np.hstack((y_train,y_train_n))\n",
    "        y_cat = y_train <= 0.3\n",
    "        X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "      else: # Control for different training sample sizes\n",
    "        while len(eps_v) < len(y_train):\n",
    "          eps_v = np.hstack((eps_v,eps*np.random.normal(0,1,1)))\n",
    "        y_train_n = y_train+eps_v\n",
    "        y_train = np.hstack((y_train,y_train_n))\n",
    "        y_cat = y_train <= 0.3\n",
    "        X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "\n",
    "    for jj in np.arange(2,cvn):\n",
    "      skf_g = sms.StratifiedKFold(n_splits=jj,shuffle=True,random_state=0)\n",
    "      skf_gen = skf_g.split(X0_ss0,y_cat)\n",
    "      lasso = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "        # Feature selection\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        sel = skf.RFECV(lasso,step=rcfs,cv=skf_gen)\n",
    "        X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "        X0_ss = X0_sst\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "        cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "        print('LassoCV score for',jj,'is',cv_scores[jj],'from dataset of size',X0_ss.shape)\n",
    "        \n",
    "    with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "      best_cv = np.argmax(cv_scores)\n",
    "      # Break any ties\n",
    "      if np.sum(cv_scores == best_cv) > 1:\n",
    "        cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "        for jjj in (cv_scores == cv_scores(best_cv)):\n",
    "          if jjj > 0:\n",
    "            skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "            skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "            X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "            X0_ss = X0_sst\n",
    "            lasso = slm.LassoLarsCV(max_iter=1000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "            est_ls = lasso.fit(X0_ss,y_train)\n",
    "            cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "        best_cv = np.argmax(cv_scores_tb)\n",
    "      \n",
    "      # Fit whole dataset with optimal cv\n",
    "      lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      sel = skf.RFECV(lasso,step=rcfs,cv=best_cv)\n",
    "      X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "      X_test_sst = sel.transform(X_test_ss0)\n",
    "      X0_ss = X0_sst\n",
    "      X_test_ss = X_test_sst\n",
    "\n",
    "    # LASSO\n",
    "    with warnings.catch_warnings():\n",
    "      warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "      lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      est_ls = lasso.fit(X0_ss,y_train)\n",
    "      results_ls[c] = est_ls.predict(X_test_ss)\n",
    "      if results_ls[c] < 0:\n",
    "          dx, y_n = cKDTree(X0_ss00.reshape(X0_ss00.shape[0],-1)).query(X_test_ss0.reshape(1,-1),k=15)\n",
    "          results_ls[c] = np.mean((y_train_0[y_n]))\n",
    "          print('Using nearest neighbor')\n",
    "      print('Lasso predicts',str(np.round(results_ls[c],4)),\n",
    "            'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv)\n",
    "      try:\n",
    "        K_nz.append(np.squeeze(K_ss)[est_ls.coef_>0])\n",
    "      except:\n",
    "        print('No features appended')\n",
    "      c=c+1\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do wild bootstrap\n",
    "# Repeat CHH with wild bootstrap and higher zscore\n",
    "results_ls_smogn = np.zeros_like(per_change)\n",
    "Js = []\n",
    "aug = False\n",
    "err_var = np.zeros_like(per_change)\n",
    "rerror = np.zeros_like(per_change)\n",
    "kappa = []\n",
    "c = 0\n",
    "for j in np.arange(c,len(subsc)):\n",
    "    test_id = subsc[j]\n",
    "    test_index = subsc == test_id\n",
    "    train_index = subsc != test_id\n",
    "    X_train = X_all_c[train_index,:,:]\n",
    "    X_test = X_all_c[test_index,:,:]\n",
    "    y_train = per_change[train_index]\n",
    "    y_test = per_change[test_index]\n",
    "\n",
    "    y_cat = y_train <= 0.3\n",
    "    idy = np.where(y_cat==1)\n",
    "    # Cross validation\n",
    "\n",
    "   \n",
    "                                          \n",
    "    X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                X_train,train_index,X_test,\n",
    "                                                test_index,pre_updrs_off,None,None,None,None,None,None,None,None,None,False,False,False)\n",
    "    X_smogn,y_smogn,idx_kept,sscaler = util.rad_smogn(X0_ss0,y_train,np.amax(y_train),np.amin(y_train),1,0,0.05,0.02)\n",
    "    X0_ss00 = X0_ss0\n",
    "    y_train_0 = y_train\n",
    "    cvn = 5\n",
    "\n",
    "    cv_scores = np.zeros((cvn,1))\n",
    "    rs = 1\n",
    "    rcfs = 1000\n",
    "    (mu, sigma) = stats.norm.fit(y_train)\n",
    "    kappa = stats.skew(y_train)\n",
    "    print('Label distribution of:',mu,sigma,kappa)\n",
    "    for jj in np.arange(2,cvn):\n",
    "      # Resample to avoid stratification errors\n",
    "      while np.sum(y_cat) < cvn:\n",
    "        np.random.seed(rs)\n",
    "        idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "        X0_ss0 = np.append(X0_ss0,X0_ss0[idyr,:].reshape(1,-1),axis=0)\n",
    "        y_train = np.append(y_train,y_train[idyr])\n",
    "        y_cat = y_train <= 0.3\n",
    "        rs = rs+1\n",
    "        print('Resampled to size',y_train.shape)\n",
    "      #   ls0 = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      #   est0 = ls0.fit(X0_ss0,y_train)\n",
    "      #   eps = y_train-ls0.predict(X0_ss0)\n",
    "      #   eps_v = eps*np.random.normal(0,1,1)\n",
    "      #   y_train_0 = y_train\n",
    "      # if aug == True:\n",
    "      #   y_train_n = y_train+(1.96*sigma)*np.random.normal(0,1,1)\n",
    "      #   y_train = np.hstack((y_train,y_train_n))\n",
    "      #   y_cat = y_train <= 0.3\n",
    "      #   X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "      # else: # Control for different training sample sizes\n",
    "      #   while len(eps_v) < len(y_train):\n",
    "      #     eps_v = np.hstack((eps_v,eps*np.random.normal(0,1,1)))\n",
    "      #   y_train_n = y_train+eps_v\n",
    "      #   y_train = np.hstack((y_train,y_train_n))\n",
    "      #   y_cat = y_train <= 0.3\n",
    "      #   X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "\n",
    "    for jj in np.arange(2,cvn):\n",
    "      skf_g = sms.StratifiedKFold(n_splits=jj,shuffle=True,random_state=0)\n",
    "      skf_gen = skf_g.split(X0_ss0,y_cat)\n",
    "      lasso = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "        # Feature selection\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        sel = skf.RFECV(lasso,step=rcfs,cv=skf_gen)\n",
    "        X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "        X0_ss = X0_sst\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "        cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "        print('LassoCV score for',jj,'is',cv_scores[jj],'from dataset of size',X0_ss.shape)\n",
    "        \n",
    "    with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "      best_cv = np.argmax(cv_scores)\n",
    "      # Break any ties\n",
    "      if np.sum(cv_scores == best_cv) > 1:\n",
    "        cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "        for jjj in (cv_scores == cv_scores(best_cv)):\n",
    "          if jjj > 0:\n",
    "            skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "            skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "            X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "            X0_ss = X0_sst\n",
    "            lasso = slm.LassoLarsCV(max_iter=1000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "            est_ls = lasso.fit(X0_ss,y_train)\n",
    "            cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "        best_cv = np.argmax(cv_scores_tb)\n",
    "      \n",
    "      # Fit whole dataset with optimal cv\n",
    "      lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      sel = skf.RFECV(lasso,step=rcfs,cv=best_cv)\n",
    "      X0_sst = sel.fit_transform(X0_ss0,y_train)\n",
    "      X_test_sst = sel.transform(X_test_ss0)\n",
    "      X0_ss = X0_sst\n",
    "      X_test_ss = X_test_sst\n",
    "\n",
    "    # LASSO\n",
    "    with warnings.catch_warnings():\n",
    "      warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "      lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "      est_ls = lasso.fit(X0_ss,y_train)\n",
    "      results_ls_smogn[c] = est_ls.predict(X_test_ss)\n",
    "      if results_ls_smogn[c] < 0:\n",
    "          dx, y_n = cKDTree(X0_ss00.reshape(X0_ss00.shape[0],-1)).query(X_test_ss0.reshape(1,-1),k=15)\n",
    "          results_ls_smogn[c] = np.mean((y_train_0[y_n]))\n",
    "          print('Using nearest neighbor')\n",
    "      print('Lasso predicts',str(np.round(results_ls_smogn[c],4)),\n",
    "            'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv)\n",
    "      try:\n",
    "        K_nz.append(np.squeeze(K_ss)[est_ls.coef_>0])\n",
    "      except:\n",
    "        print('No features appended')\n",
    "      c=c+1\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_imp = (np.asarray(pre_updrs_iii_off,dtype=float)-np.asarray(pre_updrs_iii_on,dtype=float))/np.asarray(pre_updrs_iii_off,dtype=float)\n",
    "util.eval_prediction(np.vstack((pre_imp,\n",
    "                               results_ls_aug,\n",
    "                               )),\n",
    "                               per_change,\n",
    "                               ['LCT',\n",
    "                                'Lasso',\n",
    "                                ],(15,5))\n",
    "plt.ylim([0,2])\n",
    "plt.xlim([0,2])\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = (15,5)\n",
    "results = np.vstack((pre_imp,results_ls_aug))\n",
    "y_test = per_change\n",
    "names = ['LCT','Noise compensated Lasso']\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "# n_models = results.shape[0]\n",
    "# # Cross validation results\n",
    "# if np.mod(n_models,2)==0:\n",
    "#     plt.rcParams[\"figure.figsize\"] = (fig_size[0]/2,fig_size[1]*2)\n",
    "#     [fig,ax] = plt.subplots(2,int(n_models//2),sharex=True, sharey=True)\n",
    "#     ax = np.ravel(ax)\n",
    "#     ax_reshape = 2\n",
    "# elif np.mod(n_models,3)==0:\n",
    "#     plt.rcParams[\"figure.figsize\"] = (fig_size[0]/3,fig_size[1]*3)\n",
    "#     [fig,ax] = plt.subplots(3,int(n_models//3),sharex=True, sharey=True)\n",
    "#     ax = np.ravel(ax)\n",
    "#     ax_reshape = 3\n",
    "# else:\n",
    "#     [fig,ax] = plt.subplots(1,n_models,sharex=True, sharey=True)\n",
    "#     ax_reshape = 0\n",
    "# for j in np.arange(n_models):\n",
    "#     lr_prepost = linregress(results[j],y_test)\n",
    "#     ax[j].scatter(results[j],y_test)\n",
    "#     ax[j].plot(results[j],results[j]*lr_prepost.slope+lr_prepost.intercept,'-r')\n",
    "#     ax[j].set_title(names[j])\n",
    "#     ax[j].set_ylabel(\"DBS improvement\")\n",
    "#     ax[j].set_xlabel(\"Prediction\")\n",
    "#     text = f\"$y={lr_prepost.slope:0.2f}\\; x{lr_prepost.intercept:+0.2f}$\\n$r = {lr_prepost.rvalue:0.2f}$\\n$p = {lr_prepost.pvalue:0.2e}$\"\n",
    "#     ax[j].text(0.7, 0.9, text,transform=ax[j].transAxes,\n",
    "#         fontsize=14, verticalalignment='top')\n",
    "#     ax[j].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "#     ax[j].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "# if ax_reshape == 3:\n",
    "#     ax = np.reshape(ax, (3, int(n_models/3)))\n",
    "# elif ax_reshape == 2:\n",
    "#     ax = np.reshape(ax, (2, int(n_models/2)))\n",
    "# plt.style.use('default')\n",
    "# plt.ylim([0,1.25])\n",
    "# plt.xlim([0,1.25])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train = (y_train-np.mean(y_train))/(np.std(y_train)/np.sqrt(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define probability of an unsucessful procedure as $P = 1-u$ where $u$ is the UPDRS percent improvement prediction between $0$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0.15,0.3,0.45,0.6]\n",
    "for j in np.arange(len(t)):\n",
    "    y_bin = per_change<t[j]\n",
    "    y_predicted = results_ls_aug < t[j]#1-results_ls_aug\n",
    "    fpr, tpr, _ = roc_curve(y_bin,  y_predicted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.title('Lasso+noise AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0.15,0.3,0.45,0.6]\n",
    "for j in np.arange(len(t)):\n",
    "    y_bin = per_change<t[j]\n",
    "    y_predicted = 1-results_ls\n",
    "    fpr, tpr, _ = roc_curve(y_bin,  y_predicted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.title('Lasso AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.3\n",
    "y_bin = per_change<t\n",
    "\n",
    "y_predicted_ls = 1-results_ls\n",
    "y_predicted_ls_aug = 1-results_ls_aug\n",
    "y_predicted_lct = pre_imp < 0.3\n",
    "y_predicted_aug = 1-results_lgrp_aug\n",
    "y_predicted_lr = 1-results_lgrp\n",
    "\n",
    "fprlsa, tprlsa, _ = roc_curve(y_bin,  y_predicted_ls_aug)\n",
    "fprls, tprls, _ = roc_curve(y_bin,  y_predicted_ls)\n",
    "fprl, tprl, _ = roc_curve(y_bin,  y_predicted_lct)\n",
    "fpra, tpra, _ = roc_curve(y_bin, y_predicted_aug)\n",
    "fprlr, tprlr, _ = roc_curve(y_bin, y_predicted_lr)\n",
    "\n",
    "roc_auc_lct = auc(fprl, tprl)\n",
    "roc_auc_lr = auc(fprlr, tprlr)\n",
    "roc_auc_lr_aug = auc(fpra, tpra)\n",
    "roc_auc_ls = auc(fprls, tprls)\n",
    "roc_auc_ls_aug = auc(fprlsa, tprlsa)\n",
    "plt.rcParams['legend.loc']='lower right'\n",
    "plt.plot(fprl, tprl, label = 'LCT AUC = %0.2f' % roc_auc_lct)\n",
    "plt.plot(fprlr, tprlr, label = 'LR AUC = %0.2f' % roc_auc_lr)\n",
    "plt.plot(fpra, tpra, label = 'LR + noise AUC = %0.2f' % roc_auc_lr_aug)\n",
    "# plt.plot(fprls, tprls, label = 'LASSO AUC = %0.2f' % roc_auc_ls)\n",
    "# plt.plot(fprlsa, tprlsa, label = 'LASSO + noise AUC = %0.2f' % roc_auc_ls_aug)\n",
    "plt.title('Classifier performance')\n",
    "plt.xlabel('False positive rate \\n $(1-specificity$)')\n",
    "plt.ylabel('True positive rate \\n $(sensitivity$)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = [0.15,0.3,0.45,0.6]\n",
    "# for j in np.arange(len(t)):\n",
    "#     y_bin = per_change<t[j]\n",
    "#     y_predicted = 1-results_lgrp\n",
    "#     fpr, tpr, _ = roc_curve(y_bin,  y_predicted)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "# plt.title('Logistic regression AUC')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = [0.15,0.3,0.45,0.6]\n",
    "# for j in np.arange(len(t)):\n",
    "#     y_bin = per_change<t[j]\n",
    "#     y_predicted = pre_imp<t[j]\n",
    "#     fpr, tpr, _ = roc_curve(y_bin,  y_predicted)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "# plt.title('LCT AUC')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ls_aug = util.classification_accuracy((results_ls_aug < 0.3).astype(int),(per_change < 0.3).astype(int))\n",
    "sns_ls_aug = util.classification_sensitivity((results_ls_aug < 0.3).astype(int),(per_change < 0.3).astype(int))\n",
    "spf_ls_aug = util.classification_specificity((results_ls_aug < 0.3).astype(int),(per_change < 0.3).astype(int))\n",
    "auc_ls_aug = roc_auc_ls_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ls = util.classification_accuracy((results_ls < 0.3).astype(int),(per_change < 0.3).astype(int))\n",
    "sns_ls = util.classification_sensitivity((results_ls < 0.3).astype(int),(per_change < 0.3).astype(int))\n",
    "spf_ls = util.classification_specificity((results_ls < 0.3).astype(int),(per_change < 0.3).astype(int))\n",
    "auc_ls = roc_auc_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lgr = util.classification_accuracy((results_lgr).astype(int),(per_change < 0.3).astype(int))\n",
    "sns_lgr = util.classification_sensitivity((results_lgr).astype(int),(per_change < 0.3).astype(int))\n",
    "spf_lgr = util.classification_specificity((results_lgr).astype(int),(per_change < 0.3).astype(int))\n",
    "auc_lgr = roc_auc_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lgr_aug = util.classification_accuracy((results_lgr_aug).astype(int),(per_change < 0.3).astype(int))\n",
    "sns_lgr_aug = util.classification_sensitivity((results_lgr_aug).astype(int),(per_change < 0.3).astype(int))\n",
    "spf_lgr_aug = util.classification_specificity((results_lgr_aug).astype(int),(per_change < 0.3).astype(int))\n",
    "auc_lgr_aug = roc_auc_lr_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lct = util.classification_accuracy((pre_imp < 0.3).astype(int),(per_change < 0.3).astype(int))\n",
    "sns_lct = util.classification_sensitivity((pre_imp < 0.3).astype(int),(per_change < 0.3).astype(int))\n",
    "spf_lct = util.classification_specificity((pre_imp < 0.3).astype(int),(per_change < 0.3).astype(int))\n",
    "auc_lct = roc_auc_lct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ['Levodopa challenge','Logistic regression','Logistic regression + noise','Lasso','Lasso + noise']\n",
    "d = {'accuracy': [acc_lct,acc_lgr,acc_lgr_aug,acc_ls,acc_ls_aug], 'sensitivity': [sns_lct,sns_lgr,sns_lgr_aug,sns_ls,sns_ls_aug], 'specificity': [spf_lct,spf_lgr,spf_lgr_aug,spf_ls,spf_ls_aug], 'auc': [auc_lct,auc_lgr,auc_lgr_aug,'-','-']}\n",
    "df = pd.DataFrame(index=c,data=d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_imp = (np.asarray(pre_updrs_iii_off,dtype=float)-np.asarray(pre_updrs_iii_on,dtype=float))/np.asarray(pre_updrs_iii_off,dtype=float)\n",
    "util.eval_prediction(np.vstack((pre_imp,\n",
    "                               results_ls,\n",
    "                               )),\n",
    "                               per_change,\n",
    "                               ['LCT',\n",
    "                                'Lasso',\n",
    "                                ],(15,5))\n",
    "plt.ylim([0,2])\n",
    "plt.xlim([0,2])\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = (15,5)\n",
    "results = np.vstack((pre_imp,results_ls))\n",
    "y_test = per_change\n",
    "names = ['LCT','Lasso']\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_imp = np.repeat((np.asarray(pre_updrs_iii_off,dtype=float)-np.asarray(pre_updrs_iii_on,dtype=float))/np.asarray(pre_updrs_iii_off,dtype=float),r)\n",
    "per_change = np.repeat(per_change,r)\n",
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "fig,ax = plt.subplots(1,3,sharex=True,sharey=True)\n",
    "#ax[0].scatter(pre_imp,per_change)\n",
    "col = np.where(per_change <= 0.3,'orange','blue')\n",
    "ax[0].scatter(pre_imp,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(pre_imp,per_change)\n",
    "y_model = pre_imp*lr.slope+lr.intercept\n",
    "ax[0].plot(pre_imp,y_model,color='r')\n",
    "ax[0].text(0.6,0.75,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.3f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[0].transAxes,fontsize=16)  \n",
    "ax[0].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[0].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[0].set_title('LCT',fontsize=16)\n",
    "ax[0].set_ylabel('True improvement',fontsize=16)\n",
    "\n",
    "col = np.where(per_change <= 0.3,'orange','blue')\n",
    "ax[1].scatter(results_ls,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(np.squeeze(results_ls),per_change)\n",
    "y_model = results_ls*lr.slope+lr.intercept\n",
    "ax[1].plot(results_ls,y_model,color='r')\n",
    "ax[1].text(0.55,0.75,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.7f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[1].transAxes,fontsize=16) \n",
    "ax[1].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[1].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[1].set_title('Lasso',fontsize=16)\n",
    "ax[1].set_xlabel('Prediction',fontsize=16)\n",
    "ax[1].set_ylabel('True Improvement',fontsize=16)\n",
    "ax[1].yaxis.set_tick_params(labelleft=True)\n",
    "col = np.where(per_change <= 0.3,'orange','blue')\n",
    "ax[2].scatter(results_ls_aug,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(np.squeeze(results_ls_aug),per_change)\n",
    "y_model = results_ls_aug*lr.slope+lr.intercept\n",
    "ax[2].plot(results_ls_aug,y_model,color='r')\n",
    "ax[2].text(0.55,0.75,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.7f}'.format(lr.pvalue))),\n",
    "                    ha='left', va='bottom', transform=ax[2].transAxes,fontsize=16) \n",
    "ax[2].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[2].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[2].set_ylabel('True Improvement',fontsize=16)\n",
    "ax[2].yaxis.set_tick_params(labelleft=True)\n",
    "ax[2].set_title('Noise Compensated Lasso',fontsize=16)\n",
    "plt.ylim([0,1.25])\n",
    "plt.xlim([0,1.25])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
    "    data1     = np.asarray(data1)\n",
    "    data2     = np.asarray(data2)\n",
    "    mean      = np.mean([data1, data2], axis=0)\n",
    "    diff      = data1-data2                   # Difference between datasets\n",
    "    md        = np.mean(diff)                 # Mean of the difference\n",
    "    sd        = np.std(diff, axis=0)          # Standard deviation of the difference\n",
    "\n",
    "    plt.scatter(mean, diff, *args, **kwargs)\n",
    "    plt.axhline(md,           color='gray', linestyle='--')\n",
    "    plt.text(0.1, 2*md, r'$\\mu$ = '+str(np.round(md,2)), horizontalalignment='center',verticalalignment='center')\n",
    "    plt.text(0.5, 0.4, r'$\\mu + 1.96 \\sigma$ = '+str(np.round(md + 1.96*sd,2)), horizontalalignment='center',verticalalignment='center')\n",
    "    plt.text(0.5, -0.5, r'$\\mu - 1.96 \\sigma$ = '+str(np.round(md - 1.96*sd,2)), horizontalalignment='center',verticalalignment='center')\n",
    "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
    "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
    "    plt.ylim([-4*1.96*sd,4*1.96*sd])\n",
    "    diff_in = np.logical_and(diff < md + 1.96*sd,diff > md - 1.96*sd)\n",
    "    plt.scatter(mean[diff_in], diff[diff_in], *args, **kwargs)\n",
    "    # print(sum(diff_in))\n",
    "    # print(len(diff))\n",
    "    # print('Number of features outside limits of agreement:',len(diff)-sum(diff_in),'of',len(diff),'evaluated features')\n",
    "    # print('Limits of agreement are:',str(md-1.96*sd),str(md+1.96*sd))\n",
    "    # print('Fraction of features within limits of agreement: ',str(sum(diff_in)/len(diff.ravel())))\n",
    "    plt.xlabel(r'$\\frac{y_{true}+y_{predicted}}{2}$',fontsize=16)\n",
    "    plt.ylabel(r'$y_{predicted}-y_{true}$',fontsize=12)\n",
    "    return diff,diff_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d,din = bland_altman_plot(results_ls_aug,per_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_imp = np.repeat((np.asarray(pre_updrs_iii_off,dtype=float)-np.asarray(pre_updrs_iii_on,dtype=float))/np.asarray(pre_updrs_iii_off,dtype=float),r)\n",
    "per_change = np.repeat(per_change,r)\n",
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "fig,ax = plt.subplots(1,3,sharex=True,sharey=True)\n",
    "#ax[0].scatter(pre_imp,per_change)\n",
    "col = np.where(per_change <= 0.3,'orange','blue')\n",
    "ax[0].scatter(pre_imp,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(pre_imp,per_change)\n",
    "y_model = pre_imp*lr.slope+lr.intercept\n",
    "ax[0].plot(pre_imp,y_model,color='r')\n",
    "# ax[0].text(0.6,0.75,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.3f}'.format(lr.pvalue))),\n",
    "#                     ha='left', va='bottom', transform=ax[0].transAxes,fontsize=16)  \n",
    "ax[0].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[0].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[0].set_title('LCT',fontsize=16)\n",
    "ax[0].set_xlabel('Prediction',fontsize=16)\n",
    "ax[0].set_ylabel('True improvement',fontsize=16)\n",
    "col = np.where(per_change <= 0.3,'orange','blue')\n",
    "ax[1].scatter(results_ls,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(np.squeeze(results_ls),per_change)\n",
    "y_model = results_ls*lr.slope+lr.intercept\n",
    "ax[1].plot(results_ls,y_model,color='r')\n",
    "# ax[1].text(0.55,0.75,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.7f}'.format(lr.pvalue))),\n",
    "#                     ha='left', va='bottom', transform=ax[1].transAxes,fontsize=16) \n",
    "ax[1].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[1].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[1].set_title('Lasso',fontsize=16)\n",
    "ax[1].set_xlabel('Prediction',fontsize=16)\n",
    "ax[1].set_ylabel('True Improvement',fontsize=16)\n",
    "ax[1].yaxis.set_tick_params(labelleft=True)\n",
    "col = np.where(per_change <= 0.3,'orange','blue')\n",
    "ax[2].scatter(results_ls_aug,per_change, c=col,linewidth=0)\n",
    "lr = stats.linregress(np.squeeze(results_ls_aug),per_change)\n",
    "y_model = results_ls_aug*lr.slope+lr.intercept\n",
    "ax[2].plot(results_ls_aug,y_model,color='r')\n",
    "# ax[2].text(0.55,0.75,'$y$ = '+str(np.round(lr.slope,2))+'$x$ + '+str(np.round(lr.intercept,2))+'\\n'+'$r$ = '+str(np.round(lr.rvalue,2))+'\\n'+'$p$ = '+str(('{:.7f}'.format(lr.pvalue))),\n",
    "#                     ha='left', va='bottom', transform=ax[2].transAxes,fontsize=16) \n",
    "ax[2].hlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[2].vlines(0.3,0,2,linestyle='dashed',color='black')\n",
    "ax[2].set_xlabel('Prediction',fontsize=16)\n",
    "ax[2].set_ylabel('True Improvement',fontsize=16)\n",
    "ax[2].yaxis.set_tick_params(labelleft=True)\n",
    "ax[2].set_title('Noise Compensated Lasso',fontsize=16)\n",
    "plt.ylim([0,1])\n",
    "plt.xlim([0,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (30,15)\n",
    "fig, axes = plt.subplots(1,1,sharey=True)\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.8)\n",
    "\n",
    "R = [item for sublist in K_nz for item in sublist]\n",
    "letter_counts = Counter(R)\n",
    "lc = { x: count for x, count in letter_counts.items() if count > 1 }\n",
    "df = pd.DataFrame.from_dict(lc, orient='index')\n",
    "df.sort_values(0, ascending=False, inplace=True)\n",
    "df.plot(ax=axes, y=0, kind='bar', legend=False, fontsize=16)\n",
    "plt.title('Predictive radiomic features',fontsize=16)\n",
    "plt.ylabel('Frequency',fontsize=16)\n",
    "\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ls_aug_clt = np.zeros_like(per_change)\n",
    "results_lgr_aug_clt = np.zeros_like(per_change)\n",
    "results_lgrp_aug_clt = np.zeros_like(per_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain = 1\n",
    "if retrain == 1:\n",
    "\n",
    "  Js = []\n",
    "  aug = True\n",
    "  err_var = np.zeros_like(per_change)\n",
    "  rerror = np.zeros_like(per_change)\n",
    "  kappa = []\n",
    "  K_nz = []\n",
    "  E_nz = []\n",
    "  c = 0\n",
    "\n",
    "  K_all_c = np.append(K_all_c,['pre updrs']*5)\n",
    "  for j in np.arange(len(subsc)):\n",
    "      test_id = subsc[j]\n",
    "      test_index = subsc == test_id\n",
    "      train_index = subsc != test_id\n",
    "      X_train = X_all_c[train_index,:,:]\n",
    "      X_test = X_all_c[test_index,:,:]\n",
    "      y_train = per_change[train_index]\n",
    "      y_test = per_change[test_index]\n",
    "\n",
    "      y_cat = y_train <= 0.3\n",
    "      idy = np.where(y_cat==1)\n",
    "      # Cross validation\n",
    "                                            \n",
    "      X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                  X_train,train_index,X_test,\n",
    "                                                  test_index,pre_updrs_off,None,None,None,None,None,None,None,None,None,False,False,False)\n",
    "      cvn = 5\n",
    "      cv_scores = np.zeros((cvn,1))\n",
    "      cv_lgr_scores = np.zeros((cvn,1))\n",
    "      rs = 1\n",
    "      rcfs = 1000\n",
    "      (mu, sigma) = stats.norm.fit(y_train)\n",
    "      kappa = stats.skew(y_train)\n",
    "      print('Label distribution of:',mu,sigma,kappa)\n",
    "      for jj in np.arange(2,cvn):\n",
    "        # Resample to avoid stratification errors\n",
    "        while np.sum(y_cat) < cvn:\n",
    "          np.random.seed(rs)\n",
    "          idyr = np.random.choice(np.asarray(idy).ravel())\n",
    "          X0_ss0 = np.append(X0_ss0,X0_ss0[idyr,:].reshape(1,-1),axis=0)\n",
    "          y_train = np.append(y_train,y_train[idyr])\n",
    "          y_cat = y_train <= 0.3\n",
    "          rs = rs+1\n",
    "          print('Resampled to size',y_train.shape)\n",
    "        if aug == True:\n",
    "          # y_train_n = y_train+(1.96*sigma)*np.random.normal(0,1,1)\n",
    "          z_train = (y_train-np.mean(y_train))/(np.std(y_train)/np.sqrt(len(y_train)))\n",
    "          z_train_ns = z_train+1.96*np.std(z_train)*np.random.normal(0,1,1)\n",
    "          y_train_n = z_train_ns*((np.std(y_train)/np.sqrt(len(y_train))))+np.mean(y_train)\n",
    "          y_train = np.hstack((y_train,y_train_n))\n",
    "          y_cat = y_train <= 0.3\n",
    "          X0_ss0 = np.vstack((X0_ss0,X0_ss0))\n",
    "      \n",
    "      for jj in np.arange(2,cvn):\n",
    "        skf_g = sms.StratifiedKFold(n_splits=jj,shuffle=True,random_state=0)\n",
    "        skf_gen = skf_g.split(X0_ss0,y_cat)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=jj,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None)\n",
    "        with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):\n",
    "          # Feature selection\n",
    "          warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "          sel = skf.RFECV(lasso,step=rcfs,cv=skf_gen,n_jobs=1)\n",
    "          # Stratifies classifiers automatically\n",
    "          sel_lr = skf.RFECV(lgr,step=rcfs,cv=jj,n_jobs=1)\n",
    "          X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "          X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "          est_ls = lasso.fit(X0_ss,y_train)\n",
    "          est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "          cv_scores[jj] = est_ls.score(X0_ss,y_train)\n",
    "          cv_lgr_scores[jj] = est_lgr.score(X0_ssl,y_cat)\n",
    "          print('LassoCV score for',jj,'is',cv_scores[jj],'from dataset of size',X0_ss.shape)\n",
    "          \n",
    "      with warnings.catch_warnings() and np.errstate(divide='ignore', invalid='ignore'):        \n",
    "        best_cv = np.argmax(cv_scores)\n",
    "        best_cv_lgr = np.argmax(cv_lgr_scores)\n",
    "\n",
    "        # Break any ties\n",
    "        if np.sum(cv_scores == best_cv) > 1:\n",
    "          cv_scores_tb = np.zeros((np.sum(cv_scores == best_cv),1))\n",
    "          cv_lgr_scores_tb = np.zeros((np.sum(cv_scores == best_cv_lgr),1))\n",
    "          for jjj in (cv_scores == cv_scores[best_cv]):\n",
    "            if jjj > 0:\n",
    "              print('Breaking tie')\n",
    "              skf_g = sms.StratifiedKFold(n_splits=np.arange(2,cvn)[jjj],shuffle=True,random_state=1)\n",
    "              skf_gen = skf_g.split(X0_ss0,y_cat) \n",
    "              X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "              X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "              lasso = slm.LassoLarsCV(max_iter=1000,cv=np.arange(2,cvn)[jjj],n_jobs=-1,normalize=False,eps=0.1)\n",
    "              lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None)\n",
    "              est_ls = lasso.fit(X0_ss,y_train)\n",
    "              est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "              cv_scores_tb[jjj] = est_ls.score(X0_ss,y_train)\n",
    "              cv_lgr_scores_tb[jj] = est_lgr.score(sel_lr.fit_transform(X0_ss0,y_cat),y_cat)\n",
    "          best_cv = np.argmax(cv_scores_tb)\n",
    "          best_cv_lgr = np.argmax(cv_lgr_scores_tb)\n",
    "        \n",
    "        # Fit whole dataset with optimal cv\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=jj,class_weight=None)\n",
    "        sel = skf.RFECV(lasso,step=rcfs,cv=best_cv)\n",
    "        sel_lr = skf.RFECV(lgr,step=rcfs,cv=best_cv)\n",
    "        X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "        X_test_ss = sel.transform(X_test_ss0)\n",
    "        X0_ssl = sel_lr.fit_transform(X0_ss0,y_cat)\n",
    "        X_test_ssl = sel_lr.transform(X_test_ss0)\n",
    "        K_ss = sel.transform(K_all_c.reshape(1,-1))\n",
    "\n",
    "      # LASSO\n",
    "      with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        lasso = slm.LassoLarsCV(max_iter=1000,cv=best_cv,n_jobs=-1,normalize=False,eps=0.1)\n",
    "        lgr = slm.LogisticRegressionCV(n_jobs=-1,cv=best_cv,class_weight=None)\n",
    "        est_ls = lasso.fit(X0_ss,y_train)\n",
    "        est_lgr = lgr.fit(X0_ssl,y_cat)\n",
    "      results_lgr_aug_clt[c] = est_lgr.predict(X_test_ssl)\n",
    "      results_lgrp_aug_clt[c] = est_lgr.predict_proba(X_test_ssl)[0][0]\n",
    "      results_ls_aug_clt[c] = est_ls.predict(X_test_ss)\n",
    "      print('Lasso predicts',str(np.round(results_ls_aug_clt[c],4)),'and logistic regression predicts',results_lgr_aug_clt[c],\n",
    "                'for case with',str(np.round(np.repeat(per_change,r)[c],2)),'and selected CV',best_cv,'and',sum(y_cat),'minority cases')\n",
    "      K_nz.append(np.squeeze(K_ss)[est_ls.coef_>0])\n",
    "      E_nz.append(est_ls.coef_[est_ls.coef_>0])\n",
    "      c=c+1\n",
    "\n",
    "  # np.save('results_ls_aug_d.npy',results_ls_aug)\n",
    "  # np.save('results_lgr_aug_d.npy',results_lgr_aug)\n",
    "  # np.save('results_lgrp_aug_d.npy',results_lgrp_aug)\n",
    "\n",
    "else:\n",
    "  print('Loading')\n",
    "  # results_ls_aug = np.load('results_ls_aug_d.npy')\n",
    "  # results_lgr_aug = np.load('results_lgr_aug_d.npy')\n",
    "  #results_lgrp_aug = np.load('results_lgr_aug_d.npy')\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_imp = (np.asarray(pre_updrs_iii_off,dtype=float)-np.asarray(pre_updrs_iii_on,dtype=float))/np.asarray(pre_updrs_iii_off,dtype=float)\n",
    "util.eval_prediction(np.vstack((pre_imp,\n",
    "                               results_ls_aug_clt,\n",
    "                               )),\n",
    "                               per_change,\n",
    "                               ['LCT',\n",
    "                                'Lasso',\n",
    "                                ],(15,5))\n",
    "plt.ylim([0,2])\n",
    "plt.xlim([0,2])\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_aug_clt = 1-results_lgrp_aug_clt\n",
    "fprac, tprac, _ = roc_curve(y_bin, y_predicted_aug_clt)\n",
    "roc_auc_lr_aug_clt = auc(fprac, tprac)\n",
    "\n",
    "acc_lgr_aug = util.classification_accuracy((results_lgr_aug_clt).astype(int),(per_change < 0.3).astype(int))\n",
    "sns_lgr_aug = util.classification_sensitivity((results_lgr_aug_clt).astype(int),(per_change < 0.3).astype(int))\n",
    "spf_lgr_aug = util.classification_specificity((results_lgr_aug_clt).astype(int),(per_change < 0.3).astype(int))\n",
    "auc_lgr_aug_clt = roc_auc_lr_aug_clt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lgr_aug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14732b5bb7ad6abfe54a083b8d194ae3941adfb1b18321b588b21cb8f420fced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
