{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection as sms\n",
    "import sklearn.linear_model as slm\n",
    "import sklearn.preprocessing as skp\n",
    "import sklearn.metrics as sme\n",
    "import sklearn.feature_selection as skf\n",
    "import sklearn.ensemble as ske\n",
    "import sklearn.utils as sku\n",
    "import sklearn.decomposition as skd\n",
    "import sklearn.neural_network as skn\n",
    "from sklearnex import patch_sklearn, config_context\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from IPython.display import HTML\n",
    "import util\n",
    "from scipy.spatial import cKDTree\n",
    "import loss_landscapes\n",
    "import loss_landscapes.metrics\n",
    "import copy\n",
    "from torchviz import make_dot\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For samples $i \\in N$ and features $j \\in p$ with targets $y_i$, the optimal features $\\beta_{ij}$ can be determined by minimizing a loss function $l(y,f(X))$. The matrix $X$ is formed by standardizing the feature vectors such that $\\frac{1}{N}\\sum^N_{i=1}x_{ij}=0$ and $\\frac{1}{N}\\sum^N_{i=1}x_{ij}^2=1$. Then, solving for the optimal weights (least absolute shrinkage operator, LASSO) amounts to minimizing the negative likelihood of observing $(y_i,X_i)$, or\n",
    "$$\\hat{\\beta} = \\mathrm{argmin}_{\\beta} \\sum^N_{i=1} l(y_i,f(X_i)) + \\lambda \\sum^p_{j=1}\\left|\\beta_j\\right|$$\n",
    "Gradient boosting models the prediction as a weighted sum of base learners $h_i(x_i) such that $f(X)=\\beta_0+h_1(x_1)+...+h_p(x_p)$. The optimal learner combination is\n",
    "$$\\hat{f}^m(X) = \\hat{f}^{m-1}(X)+\\nu \\cdot \\hat{h}_{j^*}^m(x_{j^*}) \\ \\ \\ \\mathrm{s.t.} \\ \\ \\ j^* = \\mathrm{argmin}_{1 \\leq j \\leq p} \\sum^N_{i=1}\\left(\\left(-\\frac{\\partial l(y_i,f(X_i))}{\\partial f}\\right) \\bigg|_{f=f^{m-1}(X_i)}-\\hat{h}_j^m(x_{ij})\\right)^2$$\n",
    "Below, feature vectors $x$ and labels $y$ are loaded and $X \\sim \\mathcal{N}(0,1)$ is generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated arrays\n",
      "Created feature matrix\n",
      "Created ROI matrix\n",
      "Created feature label matrix\n",
      "['Left red nucleus' 'Left substantia nigra' 'Left subthalamic nucleus'\n",
      " 'Right Substantia nigra' 'Right red nucleus' 'Right subthalamic nucleus']\n"
     ]
    }
   ],
   "source": [
    "# Get case IDs\n",
    "case_list = open('/home/ali/RadDBS-QSM/data/docs/cases_90','r')\n",
    "lines = case_list.read()\n",
    "lists = np.loadtxt(case_list.name,comments=\"#\", delimiter=\",\",unpack=False,dtype=str)\n",
    "case_id = []\n",
    "for lines in lists:     \n",
    "    case_id.append(lines[-9:-7])\n",
    "\n",
    "# Load scores\n",
    "file_dir = '/home/ali/RadDBS-QSM/data/docs/QSM anonymus- 6.22.2023-1528.csv'\n",
    "motor_df = util.filter_scores(file_dir,'pre-dbs updrs','stim','CORNELL ID')\n",
    "# Find cases with all required scores\n",
    "subs,pre_imp,post_imp,pre_updrs_off = util.get_full_cases(motor_df,\n",
    "                                                          'CORNELL ID',\n",
    "                                                          'OFF (pre-dbs updrs)',\n",
    "                                                          'ON (pre-dbs updrs)',\n",
    "                                                          'OFF meds ON stim 6mo')\n",
    "# Load extracted features\n",
    "npy_dir = '/home/ali/RadDBS-QSM/data/npy/'\n",
    "phi_dir = '/home/ali/RadDBS-QSM/data/phi/phi/'\n",
    "roi_path = '/data/Ali/atlas/mcgill_pd_atlas/PD25-subcortical-labels.csv'\n",
    "n_rois = 6\n",
    "all_rois = False\n",
    "Phi_all, X_all, R_all, K_all, ID_all = util.load_featstruct(phi_dir,npy_dir+'X/',npy_dir+'R/',npy_dir+'K/',n_rois,1595,all_rois)\n",
    "ids = np.asarray(ID_all).astype(int)\n",
    "# Find overlap between scored subjects and feature extraction cases\n",
    "c_cases = np.intersect1d(np.asarray(case_id).astype(int),np.asarray(subs).astype(int))\n",
    "# Complete case indices with respect to feature matrix\n",
    "c_cases_idx = np.in1d(ids,c_cases)\n",
    "\n",
    "X_all_c, K, R, subsc, pre_imp, pre_updrs_off, per_change = util.re_index(X_all,K_all,R_all,c_cases_idx,subs,ids,all_rois,pre_imp,pre_updrs_off,post_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the low-dimension condition \n",
    "$$p < N$$\n",
    "Of the positive cone condition is imposed to ensure gradient boosting and LASSO estimators converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X_all_c.shape[0]-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S$ be a diagonal $p \\times p$ matrix with elements $s_{ij}=s_{ii} \\in \\{-1,1\\}$ and $X$ remains the feature matrix for the dataset, $N \\times p$. The the positive cone condition is met if \n",
    "$$\\left(S'X'XS\\right)^{-1}\\mathbb{I}_{p \\times 1}$$\n",
    "For all subsets of $X$ and possible combinations of $S$. A more tractable form is the diagonal dominance condition\n",
    "$$\\left|M_{jj}\\right| \\geq \\sum_{i \\neq j}\\left|M_{ij}\\right|$$\n",
    "Here, $M$ is the inverse covariance matrix of $X$, $M = (X'X)^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso predicts 0.63 and gradient boost predicts 0.64 for case 67 with 0.48\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.6 for case 75 with 0.97\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.62 for case 68 with 0.75\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.63 for case 79 with 0.66\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.67 for case 59 with 0.74\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.65 for case 85 with 0.15\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.59 for case 63 with 0.85\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.57 for case 66 with 0.53\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.61 for case 86 with 0.5\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.64 for case 69 with 0.49\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.65 for case 72 with 0.26\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.61 for case 80 with 0.52\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.64 for case 81 with 0.19\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.67 for case 77 with 0.56\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.62 for case 64 with 0.66\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.62 for case 83 with 0.84\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.64 for case 62 with 0.47\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.64 for case 87 with 0.09\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.56 for case 58 with 0.52\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.65 for case 89 with 0.36\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.59 for case 78 with 0.82\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.63 for case 90 with 0.3\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.61 for case 61 with 0.85\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.58 for case 1 with 0.83\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.59 for case 2 with 0.91\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.62 for case 3 with 0.56\n",
      "Lasso predicts 0.61 and gradient boost predicts 0.59 for case 6 with 0.95\n",
      "Lasso predicts 0.6 and gradient boost predicts 0.6 for case 9 with 0.89\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.67 for case 10 with 0.59\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.62 for case 11 with 0.74\n",
      "Lasso predicts 0.65 and gradient boost predicts 0.64 for case 12 with 0.5\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.66 for case 13 with 0.7\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.67 for case 15 with 0.69\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.64 for case 16 with 0.88\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.67 for case 20 with 0.67\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.63 for case 25 with 0.7\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.64 for case 26 with 0.88\n",
      "Lasso predicts 0.5 and gradient boost predicts 0.6 for case 27 with 0.74\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.63 for case 29 with 0.8\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.67 for case 32 with 0.67\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.55 for case 34 with 0.69\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.62 for case 41 with 0.43\n",
      "Lasso predicts 0.64 and gradient boost predicts 0.68 for case 44 with 0.19\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.65 for case 45 with 0.94\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.64 for case 46 with 0.56\n",
      "Lasso predicts 0.63 and gradient boost predicts 0.56 for case 52 with 0.67\n",
      "Lasso predicts 0.62 and gradient boost predicts 0.61 for case 54 with 0.87\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "scoring = 'r2'\n",
    "results_bls = np.zeros_like(per_change)\n",
    "results_ls = np.zeros_like(per_change)\n",
    "# Train\n",
    "for j in np.arange(len(subsc)):\n",
    "    test_id = subsc[j]\n",
    "    test_index = subsc == test_id\n",
    "    train_index = subsc != test_id\n",
    "    X_train = X_all_c[train_index,:,:]\n",
    "    X_test = X_all_c[test_index,:,:]\n",
    "    y_train = per_change[train_index]\n",
    "    y_test = per_change[test_index]\n",
    "    # Cross validation\n",
    "    cvn = 6\n",
    "    X0_ss0,scaler_ss,X_test_ss0 = util.model_scale(skp.StandardScaler(),\n",
    "                                                X_train,train_index,X_test,test_index,pre_updrs_off,False)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "      # Feature selection\n",
    "      sel = skf.SelectKBest(skf.r_regression,k=p)\n",
    "      X0_ss = sel.fit_transform(X0_ss0,y_train)\n",
    "      X_test_ss = sel.transform(X_test_ss0)\n",
    "      y_n = cKDTree(X0_ss).query(X_test_ss, k=1)[1]\n",
    "    \n",
    "    # LASSO\n",
    "    lasso = slm.Lasso(max_iter=1e6,alpha=1e-1)\n",
    "    est_ls = lasso.fit(X0_ss,y_train)\n",
    "    results_ls[j] = est_ls.predict(X_test_ss)\n",
    "\n",
    "    # Gradient boosting\n",
    "    gsc = sms.GridSearchCV(\n",
    "            estimator=GradientBoostingRegressor(validation_fraction=0),\n",
    "            param_grid={\"learning_rate\": [1e-2],\n",
    "                        \"max_depth\": [1],\n",
    "                        \"n_estimators\": [p]},\n",
    "                        cv=cvn, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    est_gr = gsc.fit(X0_ss, y_train)\n",
    "    results_bls[j] = est_gr.predict(X_test_ss)\n",
    "\n",
    "    # Training status\n",
    "    print('Lasso predicts',str(np.round(results_ls[j],2)),\n",
    "          'and gradient boost predicts',str(np.round(results_bls[j],2)))#,'for case',str(int(subsc[j])),'with',str(np.round(per_change[j],2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010680490300377559"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((results_ls-results_bls)**2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
