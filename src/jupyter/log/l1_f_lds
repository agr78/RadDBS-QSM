Use GPU: 0 for training
=====> Preparing data...
Using re-weighting: [SQRT_INV]
Using LDS: [GAUSSIAN] (3/1)
Training data size: 33
Validation data size: 9
Test data size: 5
[0.8, 0.2, 0.2, 0.5, 0.7]
=====> Building model...
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(9576, 957, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(957, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(3828, 957, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(957, 957, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(957, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(957, 3828, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=0)
    (bn2): BatchNorm2d(3828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (linear): Linear(in_features=34452, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
 * Overall: MSE 0.131	L1 0.287	G-Mean 0.172
 * Many: MSE 0.082	L1 0.234	G-Mean 0.157
 * Low: MSE 0.229	L1 0.391	G-Mean 0.206
Predicted  [0.99981064, 0.9544837, 0.9295874, 0.951964, 0.8710127, 0.99834454, 0.9719801, 0.9999937, 0.845737]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #0: Train loss [0.2334]; Val loss: MSE [0.1311], L1 [0.2866], G-Mean [0.1716]
 * Overall: MSE 0.159	L1 0.303	G-Mean 0.102
 * Many: MSE 0.180	L1 0.325	G-Mean 0.099
 * Low: MSE 0.117	L1 0.258	G-Mean 0.107
Predicted  [0.0051041045, 0.90389585, 0.8990617, 0.87399185, 0.926147, 0.31038603, 0.78760904, 3.7910577e-05, 0.9521585]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #1: Train loss [0.2924]; Val loss: MSE [0.1590], L1 [0.3030], G-Mean [0.1018]
 * Overall: MSE 0.211	L1 0.385	G-Mean 0.241
 * Many: MSE 0.196	L1 0.375	G-Mean 0.285
 * Low: MSE 0.240	L1 0.404	G-Mean 0.173
Predicted  [0.026204793, 0.9946905, 0.995083, 0.9922956, 0.99574655, 0.90249324, 0.9855665, 3.4802546e-05, 0.99626476]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #2: Train loss [0.3654]; Val loss: MSE [0.2108], L1 [0.3850], G-Mean [0.2411]
 * Overall: MSE 0.151	L1 0.315	G-Mean 0.208
 * Many: MSE 0.091	L1 0.256	G-Mean 0.199
 * Low: MSE 0.272	L1 0.433	G-Mean 0.226
Predicted  [0.9464846, 0.9817456, 0.98283696, 0.9764237, 0.97711974, 0.99315476, 0.9710349, 0.97026676, 0.9778356]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #3: Train loss [0.3719]; Val loss: MSE [0.1509], L1 [0.3150], G-Mean [0.2080]
 * Overall: MSE 0.143	L1 0.327	G-Mean 0.261
 * Many: MSE 0.107	L1 0.298	G-Mean 0.258
 * Low: MSE 0.216	L1 0.387	G-Mean 0.267
Predicted  [0.99995995, 0.5152143, 0.5351961, 0.51051694, 0.35385224, 0.9968239, 0.6046467, 1.0, 0.33058965]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #4: Train loss [0.3369]; Val loss: MSE [0.1432], L1 [0.3274], G-Mean [0.2613]
 * Overall: MSE 0.462	L1 0.637	G-Mean 0.581
 * Many: MSE 0.429	L1 0.611	G-Mean 0.551
 * Low: MSE 0.529	L1 0.689	G-Mean 0.646
Predicted  [1.0, 0.009005462, 0.010465466, 0.012311862, 0.0035366237, 0.998696, 0.030606233, 1.0, 0.0024715143]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #5: Train loss [0.2719]; Val loss: MSE [0.4622], L1 [0.6367], G-Mean [0.5806]
 * Overall: MSE 0.475	L1 0.644	G-Mean 0.586
 * Many: MSE 0.438	L1 0.616	G-Mean 0.555
 * Low: MSE 0.548	L1 0.699	G-Mean 0.654
Predicted  [1.0, 0.00035644474, 0.00040407202, 0.00068646285, 9.624117e-05, 0.9998778, 0.0025688843, 1.0, 7.082131e-05]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #6: Train loss [0.2999]; Val loss: MSE [0.4747], L1 [0.6440], G-Mean [0.5858]
 * Overall: MSE 0.470	L1 0.641	G-Mean 0.584
 * Many: MSE 0.435	L1 0.615	G-Mean 0.553
 * Low: MSE 0.540	L1 0.695	G-Mean 0.651
Predicted  [1.0, 0.0021665464, 0.0024979715, 0.005586997, 0.0008535562, 0.9999746, 0.015150907, 1.0, 0.00058285974]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #7: Train loss [0.2511]; Val loss: MSE [0.4701], L1 [0.6415], G-Mean [0.5841]
 * Overall: MSE 0.425	L1 0.614	G-Mean 0.564
 * Many: MSE 0.400	L1 0.592	G-Mean 0.538
 * Low: MSE 0.474	L1 0.657	G-Mean 0.622
Predicted  [1.0, 0.024897669, 0.03522068, 0.06873723, 0.016687803, 0.9999927, 0.11746666, 1.0, 0.010113026]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #8: Train loss [0.1919]; Val loss: MSE [0.4245], L1 [0.6141], G-Mean [0.5644]
 * Overall: MSE 0.403	L1 0.600	G-Mean 0.554
 * Many: MSE 0.380	L1 0.580	G-Mean 0.528
 * Low: MSE 0.448	L1 0.642	G-Mean 0.610
Predicted  [1.0, 0.03572533, 0.055975776, 0.10895145, 0.021736765, 0.9999993, 0.16006175, 1.0, 0.013978302]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #9: Train loss [0.2377]; Val loss: MSE [0.4028], L1 [0.6004], G-Mean [0.5544]
 * Overall: MSE 0.233	L1 0.461	G-Mean 0.436
 * Many: MSE 0.213	L1 0.443	G-Mean 0.419
 * Low: MSE 0.272	L1 0.497	G-Mean 0.473
Predicted  [1.0, 0.19070034, 0.26577482, 0.44868866, 0.13659011, 0.9999999, 0.5288877, 1.0, 0.07969254]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #10: Train loss [0.2156]; Val loss: MSE [0.2325], L1 [0.4611], G-Mean [0.4364]
 * Overall: MSE 0.151	L1 0.359	G-Mean 0.333
 * Many: MSE 0.123	L1 0.337	G-Mean 0.323
 * Low: MSE 0.206	L1 0.401	G-Mean 0.353
Predicted  [1.0, 0.30187568, 0.47774228, 0.6247744, 0.27199218, 1.0, 0.72485816, 1.0, 0.17207061]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #11: Train loss [0.2670]; Val loss: MSE [0.1509], L1 [0.3585], G-Mean [0.3327]
 * Overall: MSE 0.195	L1 0.418	G-Mean 0.394
 * Many: MSE 0.182	L1 0.412	G-Mean 0.394
 * Low: MSE 0.222	L1 0.431	G-Mean 0.394
Predicted  [1.0, 0.19277911, 0.36267817, 0.5169783, 0.15386257, 1.0, 0.69639695, 1.0, 0.11198818]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #12: Train loss [0.2281]; Val loss: MSE [0.1950], L1 [0.4184], G-Mean [0.3940]
 * Overall: MSE 0.172	L1 0.388	G-Mean 0.361
 * Many: MSE 0.155	L1 0.381	G-Mean 0.365
 * Low: MSE 0.206	L1 0.401	G-Mean 0.353
Predicted  [1.0, 0.22532545, 0.4073198, 0.56453377, 0.21753892, 1.0, 0.7360643, 1.0, 0.16125202]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #13: Train loss [0.2518]; Val loss: MSE [0.1721], L1 [0.3876], G-Mean [0.3611]
 * Overall: MSE 0.141	L1 0.338	G-Mean 0.301
 * Many: MSE 0.122	L1 0.336	G-Mean 0.323
 * Low: MSE 0.181	L1 0.340	G-Mean 0.260
Predicted  [1.0, 0.2665476, 0.5064535, 0.6078088, 0.3018587, 1.0, 0.8140704, 1.0, 0.26546454]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.287
Epoch #14: Train loss [0.2440]; Val loss: MSE [0.1414], L1 [0.3375], G-Mean [0.3006]
 * Overall: MSE 0.100	L1 0.245	G-Mean 0.189
 * Many: MSE 0.063	L1 0.211	G-Mean 0.178
 * Low: MSE 0.173	L1 0.312	G-Mean 0.214
Predicted  [1.0, 0.49220532, 0.6852049, 0.74866974, 0.50830495, 1.0, 0.8910331, 1.0, 0.5277612]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #15: Train loss [0.2329]; Val loss: MSE [0.0997], L1 [0.2447], G-Mean [0.1894]
 * Overall: MSE 0.171	L1 0.383	G-Mean 0.351
 * Many: MSE 0.159	L1 0.387	G-Mean 0.372
 * Low: MSE 0.195	L1 0.376	G-Mean 0.313
Predicted  [1.0, 0.2577331, 0.44911218, 0.47015357, 0.20283967, 1.0, 0.74387383, 1.0, 0.22846594]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #16: Train loss [0.2294]; Val loss: MSE [0.1709], L1 [0.3831], G-Mean [0.3511]
 * Overall: MSE 0.312	L1 0.533	G-Mean 0.499
 * Many: MSE 0.322	L1 0.539	G-Mean 0.499
 * Low: MSE 0.293	L1 0.521	G-Mean 0.500
Predicted  [1.0, 0.08176255, 0.16511309, 0.17952183, 0.040136516, 1.0, 0.47923988, 1.0, 0.05724597]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #17: Train loss [0.2319]; Val loss: MSE [0.3122], L1 [0.5330], G-Mean [0.4990]
 * Overall: MSE 0.313	L1 0.534	G-Mean 0.499
 * Many: MSE 0.317	L1 0.535	G-Mean 0.496
 * Low: MSE 0.307	L1 0.531	G-Mean 0.504
Predicted  [1.0, 0.08401894, 0.17595688, 0.18116173, 0.048209917, 1.0, 0.4283236, 1.0, 0.07920105]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #18: Train loss [0.2547]; Val loss: MSE [0.3135], L1 [0.5337], G-Mean [0.4985]
 * Overall: MSE 0.181	L1 0.390	G-Mean 0.340
 * Many: MSE 0.162	L1 0.388	G-Mean 0.371
 * Low: MSE 0.218	L1 0.394	G-Mean 0.285
Predicted  [1.0, 0.25804076, 0.5280866, 0.37345833, 0.21242647, 1.0, 0.60249865, 1.0, 0.31688845]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #19: Train loss [0.2545]; Val loss: MSE [0.1808], L1 [0.3898], G-Mean [0.3396]
 * Overall: MSE 0.136	L1 0.295	G-Mean 0.200
 * Many: MSE 0.073	L1 0.222	G-Mean 0.158
 * Low: MSE 0.260	L1 0.441	G-Mean 0.322
Predicted  [1.0, 0.8445399, 0.95471436, 0.86092955, 0.8931315, 1.0, 0.9103279, 1.0, 0.93219703]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #20: Train loss [0.3120]; Val loss: MSE [0.1357], L1 [0.2948], G-Mean [0.2000]
 * Overall: MSE 0.155	L1 0.321	G-Mean 0.212
 * Many: MSE 0.094	L1 0.260	G-Mean 0.195
 * Low: MSE 0.279	L1 0.442	G-Mean 0.252
Predicted  [1.0, 0.9499916, 0.9849547, 0.94990647, 0.97349125, 1.0, 0.96126163, 1.0, 0.98788315]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #21: Train loss [0.2492]; Val loss: MSE [0.1554], L1 [0.3206], G-Mean [0.2125]
 * Overall: MSE 0.156	L1 0.321	G-Mean 0.210
 * Many: MSE 0.094	L1 0.260	G-Mean 0.194
 * Low: MSE 0.280	L1 0.442	G-Mean 0.245
Predicted  [1.0, 0.94920653, 0.98605597, 0.94746053, 0.97743726, 1.0, 0.964209, 1.0, 0.98944294]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #22: Train loss [0.2490]; Val loss: MSE [0.1559], L1 [0.3206], G-Mean [0.2100]
 * Overall: MSE 0.133	L1 0.288	G-Mean 0.193
 * Many: MSE 0.068	L1 0.212	G-Mean 0.151
 * Low: MSE 0.261	L1 0.439	G-Mean 0.315
Predicted  [1.0, 0.82490945, 0.93775, 0.84555316, 0.85615915, 1.0, 0.91665983, 1.0, 0.93428785]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #23: Train loss [0.2352]; Val loss: MSE [0.1326], L1 [0.2879], G-Mean [0.1929]
 * Overall: MSE 0.118	L1 0.273	G-Mean 0.188
 * Many: MSE 0.061	L1 0.199	G-Mean 0.139
 * Low: MSE 0.231	L1 0.421	G-Mean 0.343
Predicted  [1.0, 0.7612164, 0.88175714, 0.7328546, 0.7468686, 1.0, 0.86534435, 1.0, 0.82862425]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #24: Train loss [0.2451]; Val loss: MSE [0.1175], L1 [0.2730], G-Mean [0.1879]
 * Overall: MSE 0.121	L1 0.283	G-Mean 0.216
 * Many: MSE 0.085	L1 0.240	G-Mean 0.183
 * Low: MSE 0.192	L1 0.368	G-Mean 0.302
Predicted  [1.0, 0.54008096, 0.7426633, 0.4695032, 0.5068523, 1.0, 0.7581712, 1.0, 0.56209606]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #25: Train loss [0.2874]; Val loss: MSE [0.1208], L1 [0.2828], G-Mean [0.2163]
 * Overall: MSE 0.116	L1 0.272	G-Mean 0.185
 * Many: MSE 0.063	L1 0.203	G-Mean 0.136
 * Low: MSE 0.221	L1 0.412	G-Mean 0.341
Predicted  [1.0, 0.7213888, 0.8855139, 0.69808173, 0.77844507, 1.0, 0.85467774, 1.0, 0.79011506]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #26: Train loss [0.2464]; Val loss: MSE [0.1157], L1 [0.2724], G-Mean [0.1848]
 * Overall: MSE 0.142	L1 0.301	G-Mean 0.157
 * Many: MSE 0.083	L1 0.234	G-Mean 0.114
 * Low: MSE 0.259	L1 0.434	G-Mean 0.296
Predicted  [1.0, 0.89696175, 0.9657123, 0.90329033, 0.9382515, 0.9999999, 0.93019354, 1.0, 0.93104154]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #27: Train loss [0.2572]; Val loss: MSE [0.1415], L1 [0.3006], G-Mean [0.1565]
 * Overall: MSE 0.150	L1 0.315	G-Mean 0.213
 * Many: MSE 0.091	L1 0.255	G-Mean 0.192
 * Low: MSE 0.268	L1 0.435	G-Mean 0.263
Predicted  [1.0, 0.9365255, 0.9824233, 0.94898576, 0.9638412, 0.99999976, 0.9531285, 1.0, 0.9573061]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #28: Train loss [0.2489]; Val loss: MSE [0.1497], L1 [0.3151], G-Mean [0.2130]
 * Overall: MSE 0.147	L1 0.311	G-Mean 0.209
 * Many: MSE 0.088	L1 0.250	G-Mean 0.186
 * Low: MSE 0.265	L1 0.433	G-Mean 0.264
Predicted  [1.0, 0.9231031, 0.97791195, 0.9462098, 0.95138454, 0.9999999, 0.9520273, 1.0, 0.9498198]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #29: Train loss [0.2475]; Val loss: MSE [0.1467], L1 [0.3107], G-Mean [0.2088]
 * Overall: MSE 0.136	L1 0.294	G-Mean 0.186
 * Many: MSE 0.078	L1 0.228	G-Mean 0.151
 * Low: MSE 0.252	L1 0.425	G-Mean 0.284
Predicted  [1.0, 0.8873064, 0.9595838, 0.9226929, 0.9012014, 0.9999999, 0.9356157, 1.0, 0.91029257]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #30: Train loss [0.2282]; Val loss: MSE [0.1358], L1 [0.2939], G-Mean [0.1862]
 * Overall: MSE 0.112	L1 0.253	G-Mean 0.154
 * Many: MSE 0.057	L1 0.178	G-Mean 0.109
 * Low: MSE 0.222	L1 0.403	G-Mean 0.308
Predicted  [1.0, 0.75768465, 0.9231746, 0.870625, 0.7593456, 1.0, 0.89746404, 1.0, 0.80589783]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.245
Epoch #31: Train loss [0.2678]; Val loss: MSE [0.1119], L1 [0.2531], G-Mean [0.1544]
 * Overall: MSE 0.101	L1 0.232	G-Mean 0.120
 * Many: MSE 0.052	L1 0.155	G-Mean 0.073
 * Low: MSE 0.198	L1 0.385	G-Mean 0.330
Predicted  [1.0, 0.5670095, 0.85040116, 0.75673187, 0.60624486, 1.0, 0.78178394, 1.0, 0.6358223]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #32: Train loss [0.2399]; Val loss: MSE [0.1008], L1 [0.2318], G-Mean [0.1203]
 * Overall: MSE 0.102	L1 0.239	G-Mean 0.154
 * Many: MSE 0.052	L1 0.161	G-Mean 0.103
 * Low: MSE 0.203	L1 0.396	G-Mean 0.345
Predicted  [1.0, 0.6276884, 0.85489583, 0.7988468, 0.6924497, 1.0, 0.77763623, 1.0, 0.66491956]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #33: Train loss [0.2192]; Val loss: MSE [0.1024], L1 [0.2393], G-Mean [0.1538]
 * Overall: MSE 0.126	L1 0.286	G-Mean 0.188
 * Many: MSE 0.069	L1 0.208	G-Mean 0.132
 * Low: MSE 0.241	L1 0.441	G-Mean 0.377
Predicted  [1.0, 0.8412783, 0.9358125, 0.8751103, 0.8481993, 0.9999999, 0.83069444, 1.0, 0.8536244]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #34: Train loss [0.2316]; Val loss: MSE [0.1262], L1 [0.2859], G-Mean [0.1875]
 * Overall: MSE 0.134	L1 0.297	G-Mean 0.173
 * Many: MSE 0.075	L1 0.221	G-Mean 0.116
 * Low: MSE 0.251	L1 0.451	G-Mean 0.383
Predicted  [1.0, 0.88757145, 0.9527666, 0.8942634, 0.8781545, 0.99999964, 0.8345156, 1.0, 0.8860887]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #35: Train loss [0.2490]; Val loss: MSE [0.1341], L1 [0.2973], G-Mean [0.1727]
 * Overall: MSE 0.154	L1 0.326	G-Mean 0.238
 * Many: MSE 0.095	L1 0.263	G-Mean 0.203
 * Low: MSE 0.274	L1 0.452	G-Mean 0.327
Predicted  [1.0, 0.9620398, 0.9846548, 0.96206564, 0.9692666, 0.9999989, 0.9119782, 1.0, 0.9692235]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #36: Train loss [0.2568]; Val loss: MSE [0.1545], L1 [0.3261], G-Mean [0.2381]
 * Overall: MSE 0.156	L1 0.329	G-Mean 0.241
 * Many: MSE 0.097	L1 0.265	G-Mean 0.204
 * Low: MSE 0.276	L1 0.457	G-Mean 0.338
Predicted  [1.0, 0.96829, 0.98727906, 0.9591819, 0.9774384, 0.9999974, 0.9034055, 1.0, 0.97323155]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #37: Train loss [0.2655]; Val loss: MSE [0.1563], L1 [0.3291], G-Mean [0.2414]
 * Overall: MSE 0.147	L1 0.322	G-Mean 0.206
 * Many: MSE 0.085	L1 0.240	G-Mean 0.140
 * Low: MSE 0.272	L1 0.486	G-Mean 0.445
Predicted  [1.0, 0.9239835, 0.9698214, 0.8898675, 0.9348624, 0.99999774, 0.7559674, 1.0, 0.9147705]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #38: Train loss [0.2665]; Val loss: MSE [0.1474], L1 [0.3220], G-Mean [0.2062]
 * Overall: MSE 0.141	L1 0.300	G-Mean 0.200
 * Many: MSE 0.065	L1 0.196	G-Mean 0.132
 * Low: MSE 0.292	L1 0.506	G-Mean 0.463
Predicted  [1.0, 0.6832649, 0.86724323, 0.6042795, 0.6651934, 0.9999988, 0.43157473, 1.0, 0.65016013]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #39: Train loss [0.2487]; Val loss: MSE [0.1406], L1 [0.2995], G-Mean [0.2005]
 * Overall: MSE 0.169	L1 0.328	G-Mean 0.234
 * Many: MSE 0.084	L1 0.237	G-Mean 0.181
 * Low: MSE 0.338	L1 0.511	G-Mean 0.388
Predicted  [1.0, 0.4953863, 0.778161, 0.47129807, 0.5348967, 0.99999774, 0.28483546, 1.0, 0.5163634]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #40: Train loss [0.2591]; Val loss: MSE [0.1688], L1 [0.3280], G-Mean [0.2336]
 * Overall: MSE 0.147	L1 0.324	G-Mean 0.237
 * Many: MSE 0.073	L1 0.220	G-Mean 0.160
 * Low: MSE 0.297	L1 0.530	G-Mean 0.516
Predicted  [1.0, 0.80251783, 0.94529104, 0.8409957, 0.9143784, 0.99998915, 0.6025274, 1.0, 0.8931695]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #41: Train loss [0.2248]; Val loss: MSE [0.1474], L1 [0.3235], G-Mean [0.2367]
 * Overall: MSE 0.160	L1 0.335	G-Mean 0.253
 * Many: MSE 0.098	L1 0.270	G-Mean 0.213
 * Low: MSE 0.282	L1 0.466	G-Mean 0.357
Predicted  [0.9999995, 0.9656363, 0.9903763, 0.9731687, 0.9909314, 0.9998766, 0.88940376, 1.0, 0.9868548]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #42: Train loss [0.2426]; Val loss: MSE [0.1596], L1 [0.3353], G-Mean [0.2533]
 * Overall: MSE 0.157	L1 0.331	G-Mean 0.245
 * Many: MSE 0.095	L1 0.263	G-Mean 0.202
 * Low: MSE 0.280	L1 0.465	G-Mean 0.361
Predicted  [0.9999981, 0.95037, 0.9851226, 0.9592607, 0.9861056, 0.99978775, 0.8845352, 1.0, 0.98020005]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #43: Train loss [0.2424]; Val loss: MSE [0.1569], L1 [0.3307], G-Mean [0.2452]
 * Overall: MSE 0.141	L1 0.310	G-Mean 0.178
 * Many: MSE 0.069	L1 0.207	G-Mean 0.106
 * Low: MSE 0.284	L1 0.517	G-Mean 0.501
Predicted  [1.0, 0.73977906, 0.89646935, 0.8059391, 0.9061421, 0.9998578, 0.60424954, 1.0, 0.85494995]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #44: Train loss [0.2298]; Val loss: MSE [0.1406], L1 [0.3105], G-Mean [0.1780]
 * Overall: MSE 0.164	L1 0.349	G-Mean 0.294
 * Many: MSE 0.082	L1 0.256	G-Mean 0.229
 * Low: MSE 0.327	L1 0.534	G-Mean 0.483
Predicted  [1.0, 0.47347534, 0.709927, 0.55805933, 0.777416, 0.9997646, 0.34288675, 1.0, 0.64461625]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #45: Train loss [0.2636]; Val loss: MSE [0.1635], L1 [0.3486], G-Mean [0.2940]
 * Overall: MSE 0.157	L1 0.336	G-Mean 0.244
 * Many: MSE 0.077	L1 0.234	G-Mean 0.168
 * Low: MSE 0.316	L1 0.540	G-Mean 0.514
Predicted  [1.0, 0.57531714, 0.7813662, 0.5978875, 0.85667026, 0.9996376, 0.40644938, 1.0, 0.7263688]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #46: Train loss [0.2540]; Val loss: MSE [0.1567], L1 [0.3357], G-Mean [0.2441]
 * Overall: MSE 0.156	L1 0.338	G-Mean 0.240
 * Many: MSE 0.076	L1 0.230	G-Mean 0.160
 * Low: MSE 0.317	L1 0.553	G-Mean 0.544
Predicted  [0.99999964, 0.7278846, 0.8812765, 0.6812233, 0.9153595, 0.9984701, 0.49544007, 1.0, 0.85647863]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #47: Train loss [0.2349]; Val loss: MSE [0.1561], L1 [0.3378], G-Mean [0.2402]
 * Overall: MSE 0.163	L1 0.348	G-Mean 0.253
 * Many: MSE 0.078	L1 0.237	G-Mean 0.170
 * Low: MSE 0.333	L1 0.569	G-Mean 0.560
Predicted  [0.99999976, 0.7479338, 0.8782758, 0.65521055, 0.9104672, 0.9980405, 0.45230082, 1.0, 0.8597811]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #48: Train loss [0.2204]; Val loss: MSE [0.1630], L1 [0.3478], G-Mean [0.2533]
 * Overall: MSE 0.163	L1 0.347	G-Mean 0.263
 * Many: MSE 0.080	L1 0.240	G-Mean 0.182
 * Low: MSE 0.329	L1 0.562	G-Mean 0.550
Predicted  [1.0, 0.7175967, 0.8643392, 0.60449564, 0.89255995, 0.99936754, 0.43551058, 1.0, 0.8222352]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #49: Train loss [0.2435]; Val loss: MSE [0.1627], L1 [0.3475], G-Mean [0.2631]
 * Overall: MSE 0.149	L1 0.332	G-Mean 0.259
 * Many: MSE 0.082	L1 0.245	G-Mean 0.191
 * Low: MSE 0.284	L1 0.507	G-Mean 0.478
Predicted  [1.0, 0.8679631, 0.947582, 0.7877586, 0.94028836, 0.9994821, 0.70037365, 1.0, 0.9224431]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #50: Train loss [0.2743]; Val loss: MSE [0.1494], L1 [0.3322], G-Mean [0.2595]
 * Overall: MSE 0.148	L1 0.320	G-Mean 0.234
 * Many: MSE 0.086	L1 0.247	G-Mean 0.182
 * Low: MSE 0.270	L1 0.465	G-Mean 0.386
Predicted  [1.0, 0.9052694, 0.9645091, 0.84786284, 0.95731163, 0.9995372, 0.84984607, 1.0, 0.9466368]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #51: Train loss [0.2409]; Val loss: MSE [0.1476], L1 [0.3195], G-Mean [0.2340]
 * Overall: MSE 0.138	L1 0.311	G-Mean 0.239
 * Many: MSE 0.080	L1 0.242	G-Mean 0.192
 * Low: MSE 0.255	L1 0.449	G-Mean 0.369
Predicted  [1.0, 0.8513033, 0.9483158, 0.77380455, 0.9270308, 0.99977, 0.85746, 1.0, 0.903973]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #52: Train loss [0.2179]; Val loss: MSE [0.1380], L1 [0.3110], G-Mean [0.2387]
 * Overall: MSE 0.235	L1 0.447	G-Mean 0.404
 * Many: MSE 0.197	L1 0.416	G-Mean 0.387
 * Low: MSE 0.312	L1 0.510	G-Mean 0.441
Predicted  [1.0, 0.22327696, 0.50074774, 0.20196421, 0.28051916, 0.99996364, 0.36100626, 1.0, 0.20794988]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #53: Train loss [0.2371]; Val loss: MSE [0.2351], L1 [0.4472], G-Mean [0.4042]
 * Overall: MSE 0.343	L1 0.555	G-Mean 0.516
 * Many: MSE 0.318	L1 0.533	G-Mean 0.492
 * Low: MSE 0.393	L1 0.601	G-Mean 0.568
Predicted  [1.0, 0.09209161, 0.24379589, 0.08960656, 0.076924056, 0.9999893, 0.24513632, 1.0, 0.053293254]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #54: Train loss [0.2789]; Val loss: MSE [0.3432], L1 [0.5555], G-Mean [0.5159]
 * Overall: MSE 0.295	L1 0.515	G-Mean 0.480
 * Many: MSE 0.268	L1 0.490	G-Mean 0.456
 * Low: MSE 0.348	L1 0.564	G-Mean 0.531
Predicted  [1.0, 0.16814832, 0.33273718, 0.14282139, 0.116283424, 0.9999882, 0.32535276, 1.0, 0.08329309]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #55: Train loss [0.2964]; Val loss: MSE [0.2946], L1 [0.5146], G-Mean [0.4796]
 * Overall: MSE 0.110	L1 0.254	G-Mean 0.165
 * Many: MSE 0.070	L1 0.207	G-Mean 0.138
 * Low: MSE 0.190	L1 0.347	G-Mean 0.237
Predicted  [1.0, 0.6269516, 0.77824026, 0.5666607, 0.5378031, 0.9999559, 0.72841525, 1.0, 0.46974623]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #56: Train loss [0.2691]; Val loss: MSE [0.1099], L1 [0.2539], G-Mean [0.1649]
 * Overall: MSE 0.114	L1 0.266	G-Mean 0.173
 * Many: MSE 0.065	L1 0.203	G-Mean 0.132
 * Low: MSE 0.213	L1 0.391	G-Mean 0.297
Predicted  [1.0, 0.84222263, 0.9142848, 0.8155991, 0.7781283, 0.99991286, 0.8999263, 1.0, 0.77262706]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #57: Train loss [0.2702]; Val loss: MSE [0.1141], L1 [0.2657], G-Mean [0.1726]
 * Overall: MSE 0.148	L1 0.311	G-Mean 0.200
 * Many: MSE 0.091	L1 0.255	G-Mean 0.190
 * Low: MSE 0.261	L1 0.423	G-Mean 0.224
Predicted  [1.0, 0.9535317, 0.9793493, 0.9475574, 0.948069, 0.9997924, 0.9703389, 1.0, 0.94082665]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #58: Train loss [0.2570]; Val loss: MSE [0.1475], L1 [0.3110], G-Mean [0.2004]
 * Overall: MSE 0.161	L1 0.329	G-Mean 0.198
 * Many: MSE 0.102	L1 0.277	G-Mean 0.224
 * Low: MSE 0.279	L1 0.432	G-Mean 0.154
Predicted  [0.9999974, 0.9873925, 0.99536186, 0.9875995, 0.9904882, 0.99963975, 0.9911514, 1.0, 0.98841184]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #59: Train loss [0.2380]; Val loss: MSE [0.1606], L1 [0.3286], G-Mean [0.1976]
 * Overall: MSE 0.161	L1 0.329	G-Mean 0.197
 * Many: MSE 0.102	L1 0.277	G-Mean 0.224
 * Low: MSE 0.279	L1 0.433	G-Mean 0.153
Predicted  [0.9999974, 0.98744684, 0.9951578, 0.9876693, 0.9903716, 0.9996182, 0.9912709, 1.0, 0.9891539]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #60: Train loss [0.2520]; Val loss: MSE [0.1607], L1 [0.3287], G-Mean [0.1973]
 * Overall: MSE 0.160	L1 0.328	G-Mean 0.200
 * Many: MSE 0.101	L1 0.275	G-Mean 0.222
 * Low: MSE 0.278	L1 0.433	G-Mean 0.164
Predicted  [0.99999785, 0.9837915, 0.9939138, 0.98522633, 0.98760045, 0.9996433, 0.9892976, 1.0, 0.98734504]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #61: Train loss [0.2369]; Val loss: MSE [0.1598], L1 [0.3276], G-Mean [0.2005]
 * Overall: MSE 0.158	L1 0.325	G-Mean 0.204
 * Many: MSE 0.099	L1 0.272	G-Mean 0.217
 * Low: MSE 0.276	L1 0.432	G-Mean 0.180
Predicted  [0.99999845, 0.9789954, 0.99178994, 0.9782738, 0.98308796, 0.999684, 0.9857992, 1.0, 0.9822718]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #62: Train loss [0.2380]; Val loss: MSE [0.1582], L1 [0.3254], G-Mean [0.2037]
 * Overall: MSE 0.156	L1 0.322	G-Mean 0.205
 * Many: MSE 0.097	L1 0.268	G-Mean 0.210
 * Low: MSE 0.273	L1 0.431	G-Mean 0.195
Predicted  [0.99999905, 0.97149557, 0.9884139, 0.9695617, 0.9762021, 0.99970406, 0.9815865, 1.0, 0.9747419]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #63: Train loss [0.2347]; Val loss: MSE [0.1558], L1 [0.3221], G-Mean [0.2049]
 * Overall: MSE 0.154	L1 0.319	G-Mean 0.205
 * Many: MSE 0.095	L1 0.264	G-Mean 0.205
 * Low: MSE 0.271	L1 0.430	G-Mean 0.205
Predicted  [0.9999995, 0.96429735, 0.98544943, 0.964214, 0.97011065, 0.9997603, 0.9782106, 1.0, 0.9686548]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #64: Train loss [0.2245]; Val loss: MSE [0.1538], L1 [0.3194], G-Mean [0.2051]
 * Overall: MSE 0.153	L1 0.319	G-Mean 0.204
 * Many: MSE 0.095	L1 0.263	G-Mean 0.203
 * Low: MSE 0.270	L1 0.430	G-Mean 0.207
Predicted  [0.9999995, 0.96422255, 0.9849916, 0.96174324, 0.96740925, 0.99977845, 0.9777368, 1.0, 0.96658015]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #65: Train loss [0.2302]; Val loss: MSE [0.1533], L1 [0.3186], G-Mean [0.2043]
 * Overall: MSE 0.154	L1 0.320	G-Mean 0.205
 * Many: MSE 0.095	L1 0.264	G-Mean 0.205
 * Low: MSE 0.272	L1 0.431	G-Mean 0.205
Predicted  [0.9999994, 0.9655107, 0.98507315, 0.963843, 0.96984553, 0.9997588, 0.97856164, 1.0, 0.9714325]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #66: Train loss [0.2266]; Val loss: MSE [0.1543], L1 [0.3197], G-Mean [0.2046]
 * Overall: MSE 0.156	L1 0.322	G-Mean 0.203
 * Many: MSE 0.097	L1 0.268	G-Mean 0.210
 * Low: MSE 0.275	L1 0.432	G-Mean 0.190
Predicted  [0.9999999, 0.9739212, 0.98677117, 0.9702271, 0.97535616, 0.99982077, 0.98294777, 1.0, 0.97791016]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #67: Train loss [0.2275]; Val loss: MSE [0.1564], L1 [0.3223], G-Mean [0.2032]
 * Overall: MSE 0.156	L1 0.322	G-Mean 0.203
 * Many: MSE 0.097	L1 0.267	G-Mean 0.208
 * Low: MSE 0.274	L1 0.431	G-Mean 0.192
Predicted  [1.0, 0.9743644, 0.98502153, 0.968574, 0.97353643, 0.99987805, 0.98256356, 1.0, 0.97651446]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #68: Train loss [0.2258]; Val loss: MSE [0.1560], L1 [0.3217], G-Mean [0.2025]
 * Overall: MSE 0.153	L1 0.318	G-Mean 0.205
 * Many: MSE 0.094	L1 0.261	G-Mean 0.200
 * Low: MSE 0.271	L1 0.431	G-Mean 0.214
Predicted  [1.0, 0.9633907, 0.9804376, 0.96036094, 0.96436834, 0.99985516, 0.97552866, 1.0, 0.9690203]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #69: Train loss [0.2648]; Val loss: MSE [0.1532], L1 [0.3180], G-Mean [0.2045]
 * Overall: MSE 0.152	L1 0.317	G-Mean 0.206
 * Many: MSE 0.093	L1 0.260	G-Mean 0.198
 * Low: MSE 0.271	L1 0.432	G-Mean 0.222
Predicted  [1.0, 0.9595179, 0.97923434, 0.95887494, 0.9602352, 0.9998704, 0.9723326, 1.0, 0.9676723]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #70: Train loss [0.2482]; Val loss: MSE [0.1524], L1 [0.3170], G-Mean [0.2059]
 * Overall: MSE 0.153	L1 0.318	G-Mean 0.206
 * Many: MSE 0.094	L1 0.260	G-Mean 0.198
 * Low: MSE 0.271	L1 0.432	G-Mean 0.223
Predicted  [0.9999999, 0.9627189, 0.98022985, 0.95682627, 0.9629594, 0.99980646, 0.97194856, 1.0, 0.9681419]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #71: Train loss [0.2267]; Val loss: MSE [0.1529], L1 [0.3176], G-Mean [0.2060]
 * Overall: MSE 0.153	L1 0.318	G-Mean 0.206
 * Many: MSE 0.094	L1 0.261	G-Mean 0.199
 * Low: MSE 0.272	L1 0.432	G-Mean 0.221
Predicted  [0.9999999, 0.9640088, 0.98160124, 0.9580186, 0.96320266, 0.99975604, 0.9728263, 1.0, 0.97049373]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #72: Train loss [0.2221]; Val loss: MSE [0.1534], L1 [0.3183], G-Mean [0.2064]
 * Overall: MSE 0.154	L1 0.319	G-Mean 0.206
 * Many: MSE 0.095	L1 0.263	G-Mean 0.202
 * Low: MSE 0.272	L1 0.432	G-Mean 0.212
Predicted  [0.99999976, 0.9678243, 0.98406166, 0.9599374, 0.96793985, 0.9996792, 0.9760282, 1.0, 0.9718203]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #73: Train loss [0.2336]; Val loss: MSE [0.1543], L1 [0.3195], G-Mean [0.2055]
 * Overall: MSE 0.154	L1 0.319	G-Mean 0.207
 * Many: MSE 0.095	L1 0.262	G-Mean 0.199
 * Low: MSE 0.272	L1 0.433	G-Mean 0.224
Predicted  [0.9999995, 0.9622064, 0.9826879, 0.9559505, 0.9687412, 0.9995388, 0.97200984, 1.0, 0.97046316]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #74: Train loss [0.2195]; Val loss: MSE [0.1536], L1 [0.3186], G-Mean [0.2067]
 * Overall: MSE 0.154	L1 0.319	G-Mean 0.208
 * Many: MSE 0.095	L1 0.262	G-Mean 0.199
 * Low: MSE 0.272	L1 0.434	G-Mean 0.229
Predicted  [0.9999989, 0.96380115, 0.98365813, 0.9549489, 0.9701974, 0.99944633, 0.96997595, 1.0, 0.9718364]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #75: Train loss [0.2206]; Val loss: MSE [0.1541], L1 [0.3193], G-Mean [0.2085]
 * Overall: MSE 0.154	L1 0.320	G-Mean 0.210
 * Many: MSE 0.095	L1 0.262	G-Mean 0.200
 * Low: MSE 0.273	L1 0.434	G-Mean 0.232
Predicted  [0.99999964, 0.96536946, 0.98325586, 0.95617497, 0.9695794, 0.9995648, 0.96889484, 1.0, 0.9727026]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #76: Train loss [0.2096]; Val loss: MSE [0.1543], L1 [0.3198], G-Mean [0.2098]
 * Overall: MSE 0.152	L1 0.316	G-Mean 0.207
 * Many: MSE 0.093	L1 0.257	G-Mean 0.192
 * Low: MSE 0.271	L1 0.434	G-Mean 0.240
Predicted  [0.9999999, 0.95777804, 0.9772002, 0.95055187, 0.9594458, 0.9997148, 0.96517545, 1.0, 0.96707535]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #77: Train loss [0.2244]; Val loss: MSE [0.1520], L1 [0.3163], G-Mean [0.2069]
 * Overall: MSE 0.149	L1 0.312	G-Mean 0.203
 * Many: MSE 0.090	L1 0.251	G-Mean 0.179
 * Low: MSE 0.268	L1 0.434	G-Mean 0.259
Predicted  [0.9999999, 0.9469833, 0.9724816, 0.9379378, 0.94753534, 0.99960953, 0.9555104, 1.0, 0.95786464]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #78: Train loss [0.2223]; Val loss: MSE [0.1489], L1 [0.3119], G-Mean [0.2026]
 * Overall: MSE 0.149	L1 0.311	G-Mean 0.201
 * Many: MSE 0.089	L1 0.250	G-Mean 0.178
 * Low: MSE 0.268	L1 0.434	G-Mean 0.257
Predicted  [1.0, 0.94056916, 0.9699818, 0.9375972, 0.94942206, 0.9997081, 0.9564018, 1.0, 0.95848525]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #79: Train loss [0.2266]; Val loss: MSE [0.1486], L1 [0.3110], G-Mean [0.2009]
 * Overall: MSE 0.149	L1 0.312	G-Mean 0.202
 * Many: MSE 0.090	L1 0.251	G-Mean 0.178
 * Low: MSE 0.268	L1 0.434	G-Mean 0.258
Predicted  [0.9999999, 0.9482968, 0.96985304, 0.938011, 0.9482291, 0.999665, 0.9558568, 1.0, 0.95884776]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #80: Train loss [0.2107]; Val loss: MSE [0.1492], L1 [0.3119], G-Mean [0.2018]
 * Overall: MSE 0.148	L1 0.310	G-Mean 0.197
 * Many: MSE 0.089	L1 0.248	G-Mean 0.171
 * Low: MSE 0.266	L1 0.433	G-Mean 0.262
Predicted  [0.9999999, 0.94712174, 0.9680283, 0.93056065, 0.94387245, 0.9996325, 0.9536779, 1.0, 0.9544909]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #81: Train loss [0.2241]; Val loss: MSE [0.1482], L1 [0.3100], G-Mean [0.1970]
 * Overall: MSE 0.147	L1 0.308	G-Mean 0.195
 * Many: MSE 0.087	L1 0.245	G-Mean 0.166
 * Low: MSE 0.266	L1 0.434	G-Mean 0.269
Predicted  [0.9999999, 0.94117236, 0.9679119, 0.9271886, 0.9358668, 0.9996068, 0.94946396, 1.0, 0.95231956]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #82: Train loss [0.2164]; Val loss: MSE [0.1468], L1 [0.3083], G-Mean [0.1953]
 * Overall: MSE 0.147	L1 0.308	G-Mean 0.196
 * Many: MSE 0.087	L1 0.245	G-Mean 0.167
 * Low: MSE 0.265	L1 0.434	G-Mean 0.270
Predicted  [0.9999999, 0.93800837, 0.96654195, 0.9286811, 0.9379484, 0.9995834, 0.9487076, 1.0, 0.9505301]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #83: Train loss [0.2413]; Val loss: MSE [0.1465], L1 [0.3081], G-Mean [0.1962]
 * Overall: MSE 0.145	L1 0.306	G-Mean 0.189
 * Many: MSE 0.086	L1 0.242	G-Mean 0.157
 * Low: MSE 0.265	L1 0.434	G-Mean 0.275
Predicted  [0.99999976, 0.92828345, 0.96630126, 0.9204833, 0.9348738, 0.99937963, 0.94614846, 1.0, 0.9495131]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #84: Train loss [0.2080]; Val loss: MSE [0.1454], L1 [0.3059], G-Mean [0.1891]
 * Overall: MSE 0.146	L1 0.307	G-Mean 0.193
 * Many: MSE 0.087	L1 0.244	G-Mean 0.164
 * Low: MSE 0.265	L1 0.433	G-Mean 0.267
Predicted  [0.99999976, 0.9321706, 0.96789616, 0.9254998, 0.93697554, 0.99937636, 0.9505158, 1.0, 0.9512821]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #85: Train loss [0.2223]; Val loss: MSE [0.1461], L1 [0.3070], G-Mean [0.1929]
 * Overall: MSE 0.147	L1 0.309	G-Mean 0.196
 * Many: MSE 0.087	L1 0.246	G-Mean 0.169
 * Low: MSE 0.267	L1 0.434	G-Mean 0.266
Predicted  [0.9999999, 0.9365761, 0.97015285, 0.9286161, 0.939982, 0.9993599, 0.95147717, 1.0, 0.9553223]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #86: Train loss [0.2271]; Val loss: MSE [0.1472], L1 [0.3087], G-Mean [0.1963]
 * Overall: MSE 0.147	L1 0.308	G-Mean 0.195
 * Many: MSE 0.087	L1 0.245	G-Mean 0.165
 * Low: MSE 0.266	L1 0.435	G-Mean 0.272
Predicted  [0.99999964, 0.9338961, 0.96842223, 0.9263201, 0.940938, 0.99932885, 0.94774365, 1.0, 0.9524221]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #87: Train loss [0.2311]; Val loss: MSE [0.1467], L1 [0.3082], G-Mean [0.1953]
 * Overall: MSE 0.148	L1 0.311	G-Mean 0.202
 * Many: MSE 0.089	L1 0.249	G-Mean 0.175
 * Low: MSE 0.267	L1 0.435	G-Mean 0.268
Predicted  [0.9999995, 0.9417877, 0.9733402, 0.93297064, 0.9481426, 0.9993573, 0.95058763, 1.0, 0.95693105]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #88: Train loss [0.2248]; Val loss: MSE [0.1485], L1 [0.3113], G-Mean [0.2017]
 * Overall: MSE 0.150	L1 0.313	G-Mean 0.203
 * Many: MSE 0.090	L1 0.252	G-Mean 0.178
 * Low: MSE 0.268	L1 0.435	G-Mean 0.262
Predicted  [0.99999964, 0.94837326, 0.9764603, 0.9341941, 0.9512974, 0.999335, 0.95378125, 1.0, 0.95909363]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #89: Train loss [0.2240]; Val loss: MSE [0.1495], L1 [0.3128], G-Mean [0.2027]
 * Overall: MSE 0.148	L1 0.310	G-Mean 0.195
 * Many: MSE 0.089	L1 0.248	G-Mean 0.167
 * Low: MSE 0.266	L1 0.435	G-Mean 0.268
Predicted  [0.9999995, 0.9419559, 0.97442085, 0.9242271, 0.94716823, 0.99925023, 0.9505325, 1.0, 0.95489866]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #90: Train loss [0.2205]; Val loss: MSE [0.1481], L1 [0.3102], G-Mean [0.1952]
 * Overall: MSE 0.150	L1 0.312	G-Mean 0.199
 * Many: MSE 0.090	L1 0.251	G-Mean 0.173
 * Low: MSE 0.268	L1 0.435	G-Mean 0.262
Predicted  [0.9999995, 0.94909054, 0.97506034, 0.9292451, 0.95275956, 0.99935716, 0.95370466, 1.0, 0.95852214]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #91: Train loss [0.2202]; Val loss: MSE [0.1496], L1 [0.3123], G-Mean [0.1990]
 * Overall: MSE 0.150	L1 0.313	G-Mean 0.203
 * Many: MSE 0.091	L1 0.252	G-Mean 0.178
 * Low: MSE 0.268	L1 0.435	G-Mean 0.263
Predicted  [0.9999995, 0.9486033, 0.9771458, 0.93362194, 0.9530344, 0.99937254, 0.95341384, 1.0, 0.9590582]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #92: Train loss [0.2251]; Val loss: MSE [0.1497], L1 [0.3130], G-Mean [0.2028]
 * Overall: MSE 0.151	L1 0.315	G-Mean 0.205
 * Many: MSE 0.092	L1 0.255	G-Mean 0.187
 * Low: MSE 0.269	L1 0.433	G-Mean 0.248
Predicted  [0.99999976, 0.9547016, 0.9750114, 0.9451369, 0.95541996, 0.99963796, 0.96115345, 1.0, 0.9617881]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #93: Train loss [0.2530]; Val loss: MSE [0.1507], L1 [0.3145], G-Mean [0.2054]
 * Overall: MSE 0.151	L1 0.315	G-Mean 0.208
 * Many: MSE 0.092	L1 0.256	G-Mean 0.188
 * Low: MSE 0.269	L1 0.434	G-Mean 0.253
Predicted  [0.99999976, 0.95728457, 0.97508043, 0.9464684, 0.95623565, 0.99962914, 0.95884335, 1.0, 0.96132165]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #94: Train loss [0.2405]; Val loss: MSE [0.1510], L1 [0.3152], G-Mean [0.2076]
 * Overall: MSE 0.151	L1 0.316	G-Mean 0.208
 * Many: MSE 0.093	L1 0.257	G-Mean 0.191
 * Low: MSE 0.269	L1 0.433	G-Mean 0.246
Predicted  [0.99999976, 0.95631087, 0.97645617, 0.94969195, 0.9613866, 0.9996259, 0.9623125, 1.0, 0.96201175]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #95: Train loss [0.2326]; Val loss: MSE [0.1514], L1 [0.3159], G-Mean [0.2079]
 * Overall: MSE 0.152	L1 0.316	G-Mean 0.209
 * Many: MSE 0.093	L1 0.258	G-Mean 0.191
 * Low: MSE 0.269	L1 0.434	G-Mean 0.250
Predicted  [0.9999995, 0.9588539, 0.9762965, 0.9482762, 0.9624575, 0.9995018, 0.96029425, 1.0, 0.9633841]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #96: Train loss [0.2183]; Val loss: MSE [0.1518], L1 [0.3165], G-Mean [0.2087]
 * Overall: MSE 0.150	L1 0.314	G-Mean 0.206
 * Many: MSE 0.092	L1 0.255	G-Mean 0.185
 * Low: MSE 0.268	L1 0.433	G-Mean 0.254
Predicted  [0.99999917, 0.9509785, 0.9756502, 0.94215745, 0.95868546, 0.99944943, 0.95813775, 1.0, 0.95916396]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #97: Train loss [0.2233]; Val loss: MSE [0.1503], L1 [0.3142], G-Mean [0.2056]
 * Overall: MSE 0.152	L1 0.317	G-Mean 0.208
 * Many: MSE 0.093	L1 0.259	G-Mean 0.193
 * Low: MSE 0.269	L1 0.433	G-Mean 0.241
Predicted  [0.99999976, 0.95962423, 0.9764233, 0.95153695, 0.96468383, 0.9996617, 0.9646416, 1.0, 0.9627951]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #98: Train loss [0.2242]; Val loss: MSE [0.1520], L1 [0.3167], G-Mean [0.2077]
 * Overall: MSE 0.153	L1 0.317	G-Mean 0.207
 * Many: MSE 0.094	L1 0.259	G-Mean 0.194
 * Low: MSE 0.271	L1 0.433	G-Mean 0.236
Predicted  [0.9999999, 0.95966804, 0.9774826, 0.95242447, 0.96677077, 0.9997645, 0.9667099, 1.0, 0.96718574]  for true improvements  [0.8, 0.6, 0.9, 0.9, 0.6, 0.3, 1.0, 0.5, 0.4]
Best L1 Loss: 0.232
Epoch #99: Train loss [0.2397]; Val loss: MSE [0.1528], L1 [0.3174], G-Mean [0.2073]
========================================================================================================================
Test best model on testset...
Loaded best model, epoch 33, best val loss 0.2318
 * Overall: MSE 0.129	L1 0.308	G-Mean 0.227
 * Many: MSE 0.124	L1 0.325	G-Mean 0.298
 * Low: MSE 0.136	L1 0.282	G-Mean 0.151
Predicted  [0.62412506, 0.24358383, 0.72015226, 1.0, 0.9999999]  for true improvements  [0.8, 0.2, 0.2, 0.5, 0.7]
Test loss: MSE [0.1287], L1 [0.3079], G-Mean [0.2266]
Done
