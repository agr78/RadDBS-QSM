Use GPU: 0 for training
=====> Preparing data...
Using re-weighting: [SQRT_INV]
Training data size: 33
Validation data size: 9
Test data size: 5
[0.8 0.3 0.8 0.7 0.7]
=====> Building model...
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(256, 256, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=0)
    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (linear): Linear(in_features=12288, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
 * Overall: MSE 0.159	L1 0.331	G-Mean 0.064
 * Many: MSE 0.108	L1 0.291	G-Mean 0.193
 * Low: MSE 0.262	L1 0.410	G-Mean 0.007
Predicted  [1.0, 0.9997992, 0.488002, 0.9367649, 0.9997002, 0.99912876, 0.56173587, 0.59394366, 0.99999905]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.331
Epoch #0: Train loss [0.2403]; Val loss: MSE [0.1592], L1 [0.3311], G-Mean [0.0639]
 * Overall: MSE 0.303	L1 0.472	G-Mean 0.372
 * Many: MSE 0.262	L1 0.458	G-Mean 0.381
 * Low: MSE 0.387	L1 0.501	G-Mean 0.356
Predicted  [1.8550122e-07, 0.012139905, 0.62302935, 0.0833448, 0.00713689, 0.048785158, 0.6074217, 0.4858797, 2.6435955e-05]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.331
Epoch #1: Train loss [0.2222]; Val loss: MSE [0.3035], L1 [0.4722], G-Mean [0.3724]
 * Overall: MSE 0.203	L1 0.371	G-Mean 0.237
 * Many: MSE 0.078	L1 0.221	G-Mean 0.141
 * Low: MSE 0.452	L1 0.669	G-Mean 0.666
Predicted  [0.036490906, 0.7137388, 0.668778, 0.80497915, 0.59094536, 0.738618, 0.5659204, 0.8595493, 0.35647282]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.331
Epoch #2: Train loss [0.1952]; Val loss: MSE [0.2030], L1 [0.3706], G-Mean [0.2369]
 * Overall: MSE 0.234	L1 0.410	G-Mean 0.096
 * Many: MSE 0.121	L1 0.337	G-Mean 0.328
 * Low: MSE 0.461	L1 0.554	G-Mean 0.008
Predicted  [1.0, 0.9991491, 0.7712035, 0.9999292, 0.99821764, 0.9921698, 0.5367033, 0.9616107, 0.99999917]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.331
Epoch #3: Train loss [0.1943]; Val loss: MSE [0.2340], L1 [0.4095], G-Mean [0.0964]
 * Overall: MSE 0.262	L1 0.394	G-Mean 0.232
 * Many: MSE 0.220	L1 0.385	G-Mean 0.259
 * Low: MSE 0.347	L1 0.411	G-Mean 0.185
Predicted  [9.411728e-19, 0.042740654, 0.6515203, 1.6600841e-05, 0.014469914, 0.32314697, 0.7581863, 0.13177411, 5.536069e-11]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.331
Epoch #4: Train loss [0.2160]; Val loss: MSE [0.2621], L1 [0.3939], G-Mean [0.2317]
 * Overall: MSE 0.130	L1 0.296	G-Mean 0.184
 * Many: MSE 0.102	L1 0.250	G-Mean 0.141
 * Low: MSE 0.187	L1 0.387	G-Mean 0.315
Predicted  [0.0006667903, 0.74308264, 0.82709324, 0.31347692, 0.68539864, 0.8199963, 0.602632, 0.6181251, 0.4699649]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.296
Epoch #5: Train loss [0.1850]; Val loss: MSE [0.1302], L1 [0.2959], G-Mean [0.1841]
 * Overall: MSE 0.252	L1 0.420	G-Mean 0.000
 * Many: MSE 0.137	L1 0.347	G-Mean 0.314
 * Low: MSE 0.483	L1 0.567	G-Mean 0.000
Predicted  [1.0, 0.9999778, 0.9987949, 1.0, 0.99986315, 0.61286366, 0.33042112, 1.0, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.296
Epoch #6: Train loss [0.2372]; Val loss: MSE [0.2524], L1 [0.4201], G-Mean [0.0000]
/home/ali/anaconda3/envs/pdradenv/lib/python3.7/site-packages/scipy/stats/stats.py:283: RuntimeWarning: divide by zero encountered in log
  log_a = np.log(a)
 * Overall: MSE 0.237	L1 0.396	G-Mean 0.000
 * Many: MSE 0.114	L1 0.310	G-Mean 0.261
 * Low: MSE 0.483	L1 0.567	G-Mean 0.000
Predicted  [1.0, 0.9865433, 0.9897871, 1.0, 0.97386664, 0.5584133, 0.44702664, 1.0, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.296
Epoch #7: Train loss [0.2498]; Val loss: MSE [0.2372], L1 [0.3957], G-Mean [0.0000]
 * Overall: MSE 0.384	L1 0.545	G-Mean 0.372
 * Many: MSE 0.256	L1 0.443	G-Mean 0.275
 * Low: MSE 0.641	L1 0.749	G-Mean 0.681
Predicted  [2.1002669e-32, 0.041588392, 0.9799013, 0.55397415, 0.020681908, 0.7277537, 0.81471395, 0.99285537, 2.4531917e-12]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.296
Epoch #8: Train loss [0.2090]; Val loss: MSE [0.3844], L1 [0.5452], G-Mean [0.3720]
 * Overall: MSE 0.224	L1 0.376	G-Mean 0.234
 * Many: MSE 0.162	L1 0.357	G-Mean 0.253
 * Low: MSE 0.347	L1 0.413	G-Mean 0.199
Predicted  [0.0, 0.48048633, 0.9971943, 1.1760164e-14, 0.30194628, 0.9002746, 0.82527745, 0.06068009, 5.092043e-35]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.296
Epoch #9: Train loss [0.2677]; Val loss: MSE [0.2240], L1 [0.3755], G-Mean [0.2336]
 * Overall: MSE 0.277	L1 0.410	G-Mean 0.155
 * Many: MSE 0.068	L1 0.198	G-Mean 0.067
 * Low: MSE 0.696	L1 0.833	G-Mean 0.832
Predicted  [0.90492916, 0.82221305, 0.9961184, 1.0, 0.66347116, 0.7284298, 0.79940915, 1.0, 0.20039974]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.296
Epoch #10: Train loss [0.2741]; Val loss: MSE [0.2774], L1 [0.4098], G-Mean [0.1553]
 * Overall: MSE 0.227	L1 0.365	G-Mean 0.000
 * Many: MSE 0.100	L1 0.264	G-Mean 0.156
 * Low: MSE 0.483	L1 0.567	G-Mean 0.000
Predicted  [1.0, 0.6359469, 0.92275196, 1.0, 0.45912707, 0.48744622, 0.35713652, 1.0, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.296
Epoch #11: Train loss [0.2120]; Val loss: MSE [0.2275], L1 [0.3648], G-Mean [0.0000]
 * Overall: MSE 0.211	L1 0.345	G-Mean 0.000
 * Many: MSE 0.075	L1 0.235	G-Mean 0.155
 * Low: MSE 0.483	L1 0.567	G-Mean 0.000
Predicted  [1.0, 0.7129782, 0.7093599, 1.0, 0.57089114, 0.25592476, 0.38735083, 0.9999999, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.296
Epoch #12: Train loss [0.2703]; Val loss: MSE [0.2112], L1 [0.3454], G-Mean [0.0000]
 * Overall: MSE 0.201	L1 0.324	G-Mean 0.000
 * Many: MSE 0.060	L1 0.202	G-Mean 0.133
 * Low: MSE 0.483	L1 0.567	G-Mean 0.000
Predicted  [1.0, 0.67967063, 0.6783425, 1.0, 0.65077144, 0.1679599, 0.56671137, 0.9999914, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.296
Epoch #13: Train loss [0.2983]; Val loss: MSE [0.2010], L1 [0.3237], G-Mean [0.0000]
 * Overall: MSE 0.187	L1 0.289	G-Mean 0.000
 * Many: MSE 0.038	L1 0.150	G-Mean 0.100
 * Low: MSE 0.483	L1 0.567	G-Mean 0.000
Predicted  [1.0, 0.64333355, 0.6899223, 1.0, 0.6798566, 0.42773914, 0.64187205, 0.9995291, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.289
Epoch #14: Train loss [0.2270]; Val loss: MSE [0.1866], L1 [0.2885], G-Mean [0.0000]
 * Overall: MSE 0.200	L1 0.329	G-Mean 0.000
 * Many: MSE 0.066	L1 0.214	G-Mean 0.168
 * Low: MSE 0.470	L1 0.559	G-Mean 0.000
Predicted  [1.0, 0.63601893, 0.8984815, 1.0, 0.47624585, 0.5867571, 0.6870675, 0.97750765, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.289
Epoch #15: Train loss [0.2096]; Val loss: MSE [0.2004], L1 [0.3293], G-Mean [0.0000]
 * Overall: MSE 0.184	L1 0.320	G-Mean 0.077
 * Many: MSE 0.068	L1 0.217	G-Mean 0.164
 * Low: MSE 0.416	L1 0.526	G-Mean 0.017
Predicted  [1.0, 0.75160396, 0.8964704, 1.0, 0.5827673, 0.57676184, 0.5375051, 0.879439, 0.9999925]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.289
Epoch #16: Train loss [0.2572]; Val loss: MSE [0.1840], L1 [0.3204], G-Mean [0.0766]
 * Overall: MSE 0.188	L1 0.342	G-Mean 0.070
 * Many: MSE 0.079	L1 0.252	G-Mean 0.217
 * Low: MSE 0.407	L1 0.521	G-Mean 0.007
Predicted  [1.0, 0.38107985, 0.68863755, 1.0, 0.31510836, 0.5819776, 0.6608125, 0.86291987, 0.9999994]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.289
Epoch #17: Train loss [0.1866]; Val loss: MSE [0.1882], L1 [0.3418], G-Mean [0.0696]
 * Overall: MSE 0.259	L1 0.397	G-Mean 0.176
 * Many: MSE 0.095	L1 0.239	G-Mean 0.092
 * Low: MSE 0.585	L1 0.714	G-Mean 0.648
Predicted  [2.2980092e-29, 0.69893456, 0.8273174, 0.9999528, 0.47763178, 0.55665255, 0.5720634, 0.4408142, 4.16757e-09]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.289
Epoch #18: Train loss [0.2546]; Val loss: MSE [0.2586], L1 [0.3973], G-Mean [0.1762]
 * Overall: MSE 0.191	L1 0.333	G-Mean 0.231
 * Many: MSE 0.115	L1 0.299	G-Mean 0.258
 * Low: MSE 0.342	L1 0.400	G-Mean 0.187
Predicted  [0.0, 0.80271864, 0.8339508, 0.35929745, 0.90953434, 0.31180167, 0.43920442, 0.059146505, 6.7040567e-17]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.289
Epoch #19: Train loss [0.2450]; Val loss: MSE [0.1910], L1 [0.3328], G-Mean [0.2314]
 * Overall: MSE 0.206	L1 0.373	G-Mean 0.306
 * Many: MSE 0.119	L1 0.306	G-Mean 0.267
 * Low: MSE 0.381	L1 0.507	G-Mean 0.400
Predicted  [0.0, 0.9260559, 0.92129594, 0.00013367062, 0.9119178, 0.3849323, 0.54007715, 0.42085528, 4.0442775e-18]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.289
Epoch #20: Train loss [0.2332]; Val loss: MSE [0.2064], L1 [0.3728], G-Mean [0.3056]
 * Overall: MSE 0.228	L1 0.376	G-Mean 0.275
 * Many: MSE 0.101	L1 0.257	G-Mean 0.203
 * Low: MSE 0.483	L1 0.613	G-Mean 0.504
Predicted  [7.085355e-39, 0.83261585, 0.92434895, 1.343815e-06, 0.8329743, 0.3593929, 0.6899687, 0.7393119, 6.323855e-09]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.289
Epoch #21: Train loss [0.2367]; Val loss: MSE [0.2284], L1 [0.3755], G-Mean [0.2746]
 * Overall: MSE 0.091	L1 0.222	G-Mean 0.127
 * Many: MSE 0.057	L1 0.188	G-Mean 0.134
 * Low: MSE 0.159	L1 0.290	G-Mean 0.113
Predicted  [0.1869566, 0.8197743, 0.86562544, 0.001985356, 0.5815971, 0.57548773, 0.8362636, 0.7620345, 0.98912853]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #22: Train loss [0.2311]; Val loss: MSE [0.0908], L1 [0.2222], G-Mean [0.1266]
 * Overall: MSE 0.153	L1 0.289	G-Mean 0.000
 * Many: MSE 0.058	L1 0.196	G-Mean 0.109
 * Low: MSE 0.343	L1 0.477	G-Mean 0.000
Predicted  [1.0, 0.77407, 0.8078127, 0.9620973, 0.7060755, 0.22529106, 0.68933094, 0.76841974, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #23: Train loss [0.2609]; Val loss: MSE [0.1528], L1 [0.2893], G-Mean [0.0000]
 * Overall: MSE 0.138	L1 0.267	G-Mean 0.000
 * Many: MSE 0.042	L1 0.169	G-Mean 0.131
 * Low: MSE 0.329	L1 0.464	G-Mean 0.000
Predicted  [1.0, 0.84347373, 0.6965034, 0.99646664, 0.73334044, 0.34630734, 0.8889723, 0.6949128, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #24: Train loss [0.2803]; Val loss: MSE [0.1378], L1 [0.2675], G-Mean [0.0000]
 * Overall: MSE 0.143	L1 0.256	G-Mean 0.000
 * Many: MSE 0.034	L1 0.140	G-Mean 0.106
 * Low: MSE 0.359	L1 0.487	G-Mean 0.000
Predicted  [1.0, 0.7543696, 0.33380008, 0.999448, 0.76538634, 0.5833257, 0.8708686, 0.76180995, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #25: Train loss [0.2534]; Val loss: MSE [0.1427], L1 [0.2557], G-Mean [0.0000]
 * Overall: MSE 0.161	L1 0.297	G-Mean 0.077
 * Many: MSE 0.046	L1 0.190	G-Mean 0.170
 * Low: MSE 0.390	L1 0.510	G-Mean 0.016
Predicted  [1.0, 0.84369695, 0.72164726, 0.9997614, 0.8008222, 0.62883073, 0.94576466, 0.82885575, 0.9999932]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #26: Train loss [0.2650]; Val loss: MSE [0.1610], L1 [0.2966], G-Mean [0.0770]
 * Overall: MSE 0.157	L1 0.280	G-Mean 0.050
 * Many: MSE 0.056	L1 0.177	G-Mean 0.096
 * Low: MSE 0.357	L1 0.486	G-Mean 0.014
Predicted  [1.0, 0.7257203, 0.88497144, 0.9999237, 0.58628535, 0.51378053, 0.9237992, 0.7570181, 0.999995]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #27: Train loss [0.2171]; Val loss: MSE [0.1565], L1 [0.2799], G-Mean [0.0502]
 * Overall: MSE 0.153	L1 0.319	G-Mean 0.000
 * Many: MSE 0.084	L1 0.265	G-Mean 0.227
 * Low: MSE 0.290	L1 0.427	G-Mean 0.000
Predicted  [1.0, 0.50226754, 0.9003455, 0.9999933, 0.7616769, 0.23389722, 0.53524333, 0.5798504, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #28: Train loss [0.2206]; Val loss: MSE [0.1527], L1 [0.3189], G-Mean [0.0000]
 * Overall: MSE 0.166	L1 0.339	G-Mean 0.000
 * Many: MSE 0.121	L1 0.315	G-Mean 0.203
 * Low: MSE 0.256	L1 0.387	G-Mean 0.000
Predicted  [1.0, 0.26271036, 0.90958434, 0.99999726, 0.3460609, 0.4901452, 0.5193441, 0.45959577, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #29: Train loss [0.2561]; Val loss: MSE [0.1658], L1 [0.3390], G-Mean [0.0000]
 * Overall: MSE 0.191	L1 0.372	G-Mean 0.000
 * Many: MSE 0.117	L1 0.322	G-Mean 0.282
 * Low: MSE 0.340	L1 0.472	G-Mean 0.000
Predicted  [1.0, 0.33515057, 0.85172164, 0.9990357, 0.33181337, 0.8770051, 0.7298852, 0.71726644, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #30: Train loss [0.3147]; Val loss: MSE [0.1910], L1 [0.3720], G-Mean [0.0000]
 * Overall: MSE 0.200	L1 0.378	G-Mean 0.093
 * Many: MSE 0.132	L1 0.332	G-Mean 0.248
 * Low: MSE 0.337	L1 0.469	G-Mean 0.013
Predicted  [1.0, 0.3838954, 0.95942163, 0.9999546, 0.3671185, 0.95932406, 0.8264144, 0.70843756, 0.99999547]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #31: Train loss [0.2397]; Val loss: MSE [0.2004], L1 [0.3781], G-Mean [0.0929]
 * Overall: MSE 0.324	L1 0.471	G-Mean 0.306
 * Many: MSE 0.140	L1 0.295	G-Mean 0.188
 * Low: MSE 0.692	L1 0.821	G-Mean 0.812
Predicted  [1.5345308e-26, 0.7813884, 0.9720214, 0.8940757, 0.77026916, 0.9930662, 0.8550073, 0.87007034, 1.1880472e-07]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #32: Train loss [0.2180]; Val loss: MSE [0.3239], L1 [0.4707], G-Mean [0.3059]
 * Overall: MSE 0.284	L1 0.425	G-Mean 0.119
 * Many: MSE 0.142	L1 0.291	G-Mean 0.053
 * Low: MSE 0.570	L1 0.692	G-Mean 0.608
Predicted  [1.8489975e-29, 0.6967445, 0.9589977, 0.48399013, 0.50542, 0.99105364, 0.799753, 0.8929, 5.5625016e-10]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #33: Train loss [0.3293]; Val loss: MSE [0.2843], L1 [0.4250], G-Mean [0.1189]
 * Overall: MSE 0.235	L1 0.390	G-Mean 0.129
 * Many: MSE 0.127	L1 0.310	G-Mean 0.195
 * Low: MSE 0.452	L1 0.549	G-Mean 0.057
Predicted  [0.9999958, 0.55976915, 0.94941217, 0.99999905, 0.33727288, 0.99595034, 0.81205213, 0.9453788, 0.9997248]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #34: Train loss [0.2444]; Val loss: MSE [0.2349], L1 [0.3896], G-Mean [0.1294]
 * Overall: MSE 0.235	L1 0.391	G-Mean 0.000
 * Many: MSE 0.127	L1 0.312	G-Mean 0.167
 * Low: MSE 0.450	L1 0.548	G-Mean 0.000
Predicted  [1.0, 0.53885376, 0.970312, 1.0, 0.32877254, 0.9680481, 0.8041468, 0.9429358, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #35: Train loss [0.2377]; Val loss: MSE [0.2350], L1 [0.3909], G-Mean [0.0000]
 * Overall: MSE 0.249	L1 0.430	G-Mean 0.000
 * Many: MSE 0.182	L1 0.393	G-Mean 0.325
 * Low: MSE 0.383	L1 0.504	G-Mean 0.000
Predicted  [1.0, 0.09921438, 0.9464713, 1.0, 0.21616006, 0.8647741, 0.7375669, 0.8126433, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #36: Train loss [0.2329]; Val loss: MSE [0.2488], L1 [0.4301], G-Mean [0.0000]
 * Overall: MSE 0.199	L1 0.377	G-Mean 0.000
 * Many: MSE 0.162	L1 0.361	G-Mean 0.295
 * Low: MSE 0.274	L1 0.409	G-Mean 0.000
Predicted  [1.0, 0.109639734, 0.91565514, 1.0, 0.19535668, 0.5752924, 0.6214997, 0.5264941, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #37: Train loss [0.2392]; Val loss: MSE [0.1995], L1 [0.3768], G-Mean [0.0000]
 * Overall: MSE 0.170	L1 0.327	G-Mean 0.000
 * Many: MSE 0.145	L1 0.336	G-Mean 0.178
 * Low: MSE 0.219	L1 0.309	G-Mean 0.000
Predicted  [1.0, 0.124330685, 0.95235497, 1.0, 0.3565707, 0.49633527, 0.5612298, 0.22664805, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #38: Train loss [0.2684]; Val loss: MSE [0.1697], L1 [0.3267], G-Mean [0.0000]
 * Overall: MSE 0.138	L1 0.279	G-Mean 0.000
 * Many: MSE 0.101	L1 0.284	G-Mean 0.212
 * Low: MSE 0.213	L1 0.270	G-Mean 0.000
Predicted  [1.0, 0.32875228, 0.94161075, 1.0, 0.47818634, 0.745797, 0.7744614, 0.10918371, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #39: Train loss [0.2224]; Val loss: MSE [0.1381], L1 [0.2795], G-Mean [0.0000]
 * Overall: MSE 0.129	L1 0.255	G-Mean 0.063
 * Many: MSE 0.087	L1 0.248	G-Mean 0.184
 * Low: MSE 0.213	L1 0.269	G-Mean 0.007
Predicted  [1.0, 0.7395147, 0.97870517, 1.0, 0.86598116, 0.80497915, 0.69890195, 0.107867, 0.99993396]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #40: Train loss [0.2298]; Val loss: MSE [0.1291], L1 [0.2554], G-Mean [0.0632]
 * Overall: MSE 0.133	L1 0.273	G-Mean 0.118
 * Many: MSE 0.091	L1 0.257	G-Mean 0.200
 * Low: MSE 0.218	L1 0.305	G-Mean 0.041
Predicted  [1.0, 0.7693156, 0.95490885, 1.0, 0.83312005, 0.8820524, 0.7003287, 0.21479304, 0.99922645]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #41: Train loss [0.2500]; Val loss: MSE [0.1332], L1 [0.2727], G-Mean [0.1183]
 * Overall: MSE 0.122	L1 0.255	G-Mean 0.079
 * Many: MSE 0.054	L1 0.188	G-Mean 0.114
 * Low: MSE 0.257	L1 0.388	G-Mean 0.038
Predicted  [1.0, 0.7206936, 0.80551314, 1.0, 0.89849144, 0.68082166, 0.775766, 0.46310404, 0.99981815]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #42: Train loss [0.2360]; Val loss: MSE [0.1220], L1 [0.2548], G-Mean [0.0787]
 * Overall: MSE 0.120	L1 0.245	G-Mean 0.054
 * Many: MSE 0.046	L1 0.166	G-Mean 0.104
 * Low: MSE 0.268	L1 0.402	G-Mean 0.015
Predicted  [1.0, 0.43786788, 0.7014878, 1.0, 0.7523213, 0.435128, 0.817379, 0.505459, 0.9999896]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #43: Train loss [0.2161]; Val loss: MSE [0.1201], L1 [0.2449], G-Mean [0.0544]
 * Overall: MSE 0.120	L1 0.248	G-Mean 0.077
 * Many: MSE 0.052	L1 0.180	G-Mean 0.122
 * Low: MSE 0.254	L1 0.383	G-Mean 0.030
Predicted  [1.0, 0.7506638, 0.836594, 1.0, 0.8249079, 0.3561713, 0.77339923, 0.45035955, 0.9998994]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #44: Train loss [0.2618]; Val loss: MSE [0.1195], L1 [0.2481], G-Mean [0.0767]
 * Overall: MSE 0.164	L1 0.301	G-Mean 0.189
 * Many: MSE 0.044	L1 0.177	G-Mean 0.132
 * Low: MSE 0.403	L1 0.548	G-Mean 0.388
Predicted  [1.0, 0.8812285, 0.47744557, 0.9999989, 0.8957887, 0.65015936, 0.910309, 0.84776556, 0.90227056]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #45: Train loss [0.2702]; Val loss: MSE [0.1639], L1 [0.3006], G-Mean [0.1892]
 * Overall: MSE 0.173	L1 0.315	G-Mean 0.222
 * Many: MSE 0.045	L1 0.182	G-Mean 0.154
 * Low: MSE 0.431	L1 0.582	G-Mean 0.460
Predicted  [0.99999976, 0.8030691, 0.32748866, 0.99997675, 0.91736495, 0.43309018, 0.9301271, 0.8929698, 0.8463107]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #46: Train loss [0.2545]; Val loss: MSE [0.1735], L1 [0.3152], G-Mean [0.2220]
 * Overall: MSE 0.229	L1 0.370	G-Mean 0.219
 * Many: MSE 0.074	L1 0.193	G-Mean 0.121
 * Low: MSE 0.540	L1 0.725	G-Mean 0.714
Predicted  [0.0069241696, 0.8220434, 0.5235014, 0.996752, 0.7842594, 0.25748488, 0.8908629, 0.9222411, 0.44508538]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #47: Train loss [0.2599]; Val loss: MSE [0.2289], L1 [0.3700], G-Mean [0.2188]
 * Overall: MSE 0.275	L1 0.444	G-Mean 0.320
 * Many: MSE 0.119	L1 0.284	G-Mean 0.207
 * Low: MSE 0.587	L1 0.763	G-Mean 0.760
Predicted  [2.1477284e-05, 0.7790584, 0.88054234, 0.99869186, 0.52343965, 0.092623636, 0.8612247, 0.9255999, 0.33462217]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #48: Train loss [0.2325]; Val loss: MSE [0.2749], L1 [0.4438], G-Mean [0.3197]
 * Overall: MSE 0.201	L1 0.336	G-Mean 0.058
 * Many: MSE 0.093	L1 0.241	G-Mean 0.069
 * Low: MSE 0.418	L1 0.528	G-Mean 0.041
Predicted  [0.9999994, 0.73286456, 0.96217066, 0.9986224, 0.5467175, 0.10367353, 0.8002934, 0.8846415, 0.9998889]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #49: Train loss [0.2763]; Val loss: MSE [0.2010], L1 [0.3365], G-Mean [0.0581]
 * Overall: MSE 0.221	L1 0.370	G-Mean 0.000
 * Many: MSE 0.105	L1 0.280	G-Mean 0.193
 * Low: MSE 0.455	L1 0.551	G-Mean 0.000
Predicted  [1.0, 0.6812395, 0.992896, 0.9994336, 0.5293334, 0.11290351, 0.5878499, 0.9523202, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #50: Train loss [0.2076]; Val loss: MSE [0.2214], L1 [0.3704], G-Mean [0.0000]
 * Overall: MSE 0.253	L1 0.410	G-Mean 0.000
 * Many: MSE 0.145	L1 0.335	G-Mean 0.241
 * Low: MSE 0.469	L1 0.559	G-Mean 0.000
Predicted  [1.0, 0.674319, 0.99341846, 0.9996861, 0.4513988, 0.2303566, 0.2254625, 0.9766424, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #51: Train loss [0.2407]; Val loss: MSE [0.2530], L1 [0.4098], G-Mean [0.0000]
 * Overall: MSE 0.233	L1 0.361	G-Mean 0.000
 * Many: MSE 0.110	L1 0.259	G-Mean 0.140
 * Low: MSE 0.479	L1 0.564	G-Mean 0.000
Predicted  [1.0, 0.768772, 0.9701164, 0.99952054, 0.6119358, 0.48715055, 0.28383806, 0.9924896, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #52: Train loss [0.2540]; Val loss: MSE [0.2329], L1 [0.3609], G-Mean [0.0000]
 * Overall: MSE 0.245	L1 0.396	G-Mean 0.000
 * Many: MSE 0.139	L1 0.318	G-Mean 0.204
 * Low: MSE 0.458	L1 0.552	G-Mean 0.000
Predicted  [1.0, 0.81108403, 0.9840926, 0.99999774, 0.6825201, 0.8379928, 0.2407866, 0.9564764, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #53: Train loss [0.2430]; Val loss: MSE [0.2453], L1 [0.3963], G-Mean [0.0000]
 * Overall: MSE 0.237	L1 0.383	G-Mean 0.000
 * Many: MSE 0.124	L1 0.296	G-Mean 0.168
 * Low: MSE 0.464	L1 0.556	G-Mean 0.000
Predicted  [1.0, 0.7138549, 0.98159605, 0.9999995, 0.65125966, 0.9205461, 0.3872798, 0.9669298, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #54: Train loss [0.2788]; Val loss: MSE [0.2370], L1 [0.3827], G-Mean [0.0000]
 * Overall: MSE 0.208	L1 0.352	G-Mean 0.000
 * Many: MSE 0.085	L1 0.252	G-Mean 0.204
 * Low: MSE 0.455	L1 0.551	G-Mean 0.000
Predicted  [1.0, 0.6234359, 0.945867, 0.99999857, 0.6069346, 0.67168075, 0.47227836, 0.9517516, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #55: Train loss [0.2260]; Val loss: MSE [0.2084], L1 [0.3518], G-Mean [0.0000]
 * Overall: MSE 0.115	L1 0.238	G-Mean 0.000
 * Many: MSE 0.061	L1 0.195	G-Mean 0.081
 * Low: MSE 0.223	L1 0.323	G-Mean 0.000
Predicted  [1.0, 0.7018202, 0.72877675, 1.0, 0.5572751, 0.5322622, 0.43284804, 0.26802313, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #56: Train loss [0.2482]; Val loss: MSE [0.1152], L1 [0.2379], G-Mean [0.0000]
 * Overall: MSE 0.148	L1 0.290	G-Mean 0.000
 * Many: MSE 0.107	L1 0.265	G-Mean 0.135
 * Low: MSE 0.230	L1 0.341	G-Mean 0.000
Predicted  [1.0, 0.74522346, 0.9143126, 0.9999999, 0.48926643, 0.50738144, 0.28815386, 0.32393062, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #57: Train loss [0.3148]; Val loss: MSE [0.1478], L1 [0.2904], G-Mean [0.0000]
 * Overall: MSE 0.148	L1 0.278	G-Mean 0.000
 * Many: MSE 0.066	L1 0.193	G-Mean 0.073
 * Low: MSE 0.311	L1 0.448	G-Mean 0.000
Predicted  [1.0, 0.8322136, 0.9289823, 0.9969494, 0.7084497, 0.5043452, 0.6152751, 0.6461695, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #58: Train loss [0.2792]; Val loss: MSE [0.1477], L1 [0.2780], G-Mean [0.0000]
 * Overall: MSE 0.165	L1 0.299	G-Mean 0.000
 * Many: MSE 0.069	L1 0.205	G-Mean 0.087
 * Low: MSE 0.357	L1 0.487	G-Mean 0.000
Predicted  [1.0, 0.56408894, 0.9600145, 0.9802961, 0.81628704, 0.4987366, 0.6852794, 0.7794657, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #59: Train loss [0.2286]; Val loss: MSE [0.1653], L1 [0.2987], G-Mean [0.0000]
 * Overall: MSE 0.162	L1 0.300	G-Mean 0.000
 * Many: MSE 0.075	L1 0.214	G-Mean 0.051
 * Low: MSE 0.338	L1 0.471	G-Mean 0.000
Predicted  [1.0, 0.4597197, 0.96039766, 0.9935776, 0.70005363, 0.43610668, 0.68207043, 0.7194954, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #60: Train loss [0.2320]; Val loss: MSE [0.1624], L1 [0.2995], G-Mean [0.0000]
 * Overall: MSE 0.180	L1 0.331	G-Mean 0.000
 * Many: MSE 0.085	L1 0.249	G-Mean 0.196
 * Low: MSE 0.369	L1 0.495	G-Mean 0.000
Predicted  [1.0, 0.25343746, 0.8498952, 0.99481577, 0.57953584, 0.42569354, 0.6986448, 0.78926694, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #61: Train loss [0.2366]; Val loss: MSE [0.1799], L1 [0.3307], G-Mean [0.0000]
 * Overall: MSE 0.184	L1 0.304	G-Mean 0.031
 * Many: MSE 0.073	L1 0.196	G-Mean 0.086
 * Low: MSE 0.406	L1 0.520	G-Mean 0.004
Predicted  [1.0, 0.68415076, 0.97186273, 0.9986808, 0.7157857, 0.26356143, 0.76332337, 0.8615421, 0.9999999]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #62: Train loss [0.2473]; Val loss: MSE [0.1842], L1 [0.3041], G-Mean [0.0314]
 * Overall: MSE 0.178	L1 0.303	G-Mean 0.043
 * Many: MSE 0.072	L1 0.199	G-Mean 0.082
 * Low: MSE 0.391	L1 0.510	G-Mean 0.012
Predicted  [1.0, 0.5479042, 0.98037547, 0.9692114, 0.7360648, 0.37755808, 0.80233884, 0.86211693, 0.9999969]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #63: Train loss [0.2715]; Val loss: MSE [0.1781], L1 [0.3027], G-Mean [0.0434]
 * Overall: MSE 0.174	L1 0.322	G-Mean 0.100
 * Many: MSE 0.081	L1 0.239	G-Mean 0.166
 * Low: MSE 0.359	L1 0.487	G-Mean 0.036
Predicted  [1.0, 0.5662238, 0.9710598, 0.8566635, 0.71949923, 0.28309676, 0.60457593, 0.90372837, 0.99990904]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #64: Train loss [0.2147]; Val loss: MSE [0.1736], L1 [0.3219], G-Mean [0.1000]
 * Overall: MSE 0.129	L1 0.279	G-Mean 0.092
 * Many: MSE 0.075	L1 0.227	G-Mean 0.152
 * Low: MSE 0.239	L1 0.382	G-Mean 0.034
Predicted  [1.0, 0.6018186, 0.9238228, 0.5994345, 0.7177729, 0.35104537, 0.52513075, 0.84651697, 0.9998658]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #65: Train loss [0.2127]; Val loss: MSE [0.1294], L1 [0.2789], G-Mean [0.0923]
 * Overall: MSE 0.134	L1 0.307	G-Mean 0.097
 * Many: MSE 0.098	L1 0.288	G-Mean 0.261
 * Low: MSE 0.207	L1 0.347	G-Mean 0.014
Predicted  [1.0, 0.51999223, 0.9859218, 0.51868707, 0.57325023, 0.21486828, 0.5510207, 0.8214795, 0.99998903]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #66: Train loss [0.2435]; Val loss: MSE [0.1344], L1 [0.3074], G-Mean [0.0975]
 * Overall: MSE 0.199	L1 0.362	G-Mean 0.000
 * Many: MSE 0.095	L1 0.282	G-Mean 0.257
 * Low: MSE 0.408	L1 0.521	G-Mean 0.000
Predicted  [1.0, 0.46386874, 0.98610353, 0.96635634, 0.5703355, 0.26092726, 0.5967113, 0.8972321, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #67: Train loss [0.2540]; Val loss: MSE [0.1989], L1 [0.3620], G-Mean [0.0000]
 * Overall: MSE 0.188	L1 0.357	G-Mean 0.000
 * Many: MSE 0.095	L1 0.286	G-Mean 0.260
 * Low: MSE 0.374	L1 0.498	G-Mean 0.000
Predicted  [1.0, 0.5754896, 0.9470937, 0.99997985, 0.52803415, 0.22761412, 0.5013089, 0.79519093, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #68: Train loss [0.2114]; Val loss: MSE [0.1880], L1 [0.3566], G-Mean [0.0000]
 * Overall: MSE 0.186	L1 0.336	G-Mean 0.061
 * Many: MSE 0.090	L1 0.254	G-Mean 0.197
 * Low: MSE 0.378	L1 0.501	G-Mean 0.006
Predicted  [1.0, 0.5447218, 0.9878682, 0.999995, 0.59091365, 0.44327673, 0.4837212, 0.8024962, 0.99999964]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #69: Train loss [0.2198]; Val loss: MSE [0.1856], L1 [0.3364], G-Mean [0.0611]
 * Overall: MSE 0.173	L1 0.331	G-Mean 0.062
 * Many: MSE 0.095	L1 0.265	G-Mean 0.199
 * Low: MSE 0.330	L1 0.464	G-Mean 0.006
Predicted  [1.0, 0.58533764, 0.9905365, 0.9999995, 0.49069306, 0.45989168, 0.46720377, 0.69182837, 0.9999995]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #70: Train loss [0.2119]; Val loss: MSE [0.1734], L1 [0.3310], G-Mean [0.0623]
 * Overall: MSE 0.191	L1 0.343	G-Mean 0.069
 * Many: MSE 0.098	L1 0.264	G-Mean 0.173
 * Low: MSE 0.379	L1 0.502	G-Mean 0.011
Predicted  [1.0, 0.60730475, 0.9868454, 0.9999999, 0.34967488, 0.51795787, 0.5626279, 0.80537367, 0.9999976]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #71: Train loss [0.1832]; Val loss: MSE [0.1914], L1 [0.3434], G-Mean [0.0692]
 * Overall: MSE 0.182	L1 0.363	G-Mean 0.098
 * Many: MSE 0.114	L1 0.318	G-Mean 0.296
 * Low: MSE 0.318	L1 0.453	G-Mean 0.011
Predicted  [1.0, 0.44578612, 0.9740651, 0.9999989, 0.32083395, 0.3612149, 0.53691185, 0.6599244, 0.99999726]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #72: Train loss [0.1838]; Val loss: MSE [0.1817], L1 [0.3632], G-Mean [0.0978]
 * Overall: MSE 0.155	L1 0.315	G-Mean 0.060
 * Many: MSE 0.095	L1 0.267	G-Mean 0.199
 * Low: MSE 0.276	L1 0.411	G-Mean 0.005
Predicted  [1.0, 0.66933596, 0.974159, 0.9992218, 0.52963924, 0.32239547, 0.450498, 0.5349557, 0.9999995]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #73: Train loss [0.2144]; Val loss: MSE [0.1552], L1 [0.3152], G-Mean [0.0601]
 * Overall: MSE 0.157	L1 0.332	G-Mean 0.067
 * Many: MSE 0.115	L1 0.314	G-Mean 0.279
 * Low: MSE 0.241	L1 0.367	G-Mean 0.004
Predicted  [1.0, 0.5020717, 0.97254646, 0.99289674, 0.38256866, 0.40000197, 0.4035438, 0.40799567, 0.99999976]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #74: Train loss [0.2378]; Val loss: MSE [0.1571], L1 [0.3317], G-Mean [0.0671]
 * Overall: MSE 0.144	L1 0.304	G-Mean 0.062
 * Many: MSE 0.107	L1 0.299	G-Mean 0.264
 * Low: MSE 0.218	L1 0.314	G-Mean 0.003
Predicted  [1.0, 0.5898981, 0.9649829, 0.9940399, 0.42379743, 0.34881958, 0.4084393, 0.24874634, 0.99999964]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #75: Train loss [0.2288]; Val loss: MSE [0.1437], L1 [0.3041], G-Mean [0.0623]
 * Overall: MSE 0.149	L1 0.318	G-Mean 0.067
 * Many: MSE 0.102	L1 0.292	G-Mean 0.259
 * Low: MSE 0.244	L1 0.369	G-Mean 0.004
Predicted  [1.0, 0.50882906, 0.9563471, 0.9973701, 0.4878371, 0.3946859, 0.41209778, 0.4089433, 0.99999964]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #76: Train loss [0.2459]; Val loss: MSE [0.1492], L1 [0.3177], G-Mean [0.0668]
 * Overall: MSE 0.132	L1 0.291	G-Mean 0.066
 * Many: MSE 0.087	L1 0.265	G-Mean 0.227
 * Low: MSE 0.222	L1 0.342	G-Mean 0.006
Predicted  [1.0, 0.5728833, 0.92553633, 0.977724, 0.49693036, 0.41011113, 0.45699283, 0.3497437, 0.99999905]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #77: Train loss [0.1842]; Val loss: MSE [0.1324], L1 [0.2907], G-Mean [0.0664]
 * Overall: MSE 0.195	L1 0.365	G-Mean 0.064
 * Many: MSE 0.116	L1 0.306	G-Mean 0.202
 * Low: MSE 0.355	L1 0.484	G-Mean 0.006
Predicted  [1.0, 0.42987823, 0.9569191, 0.9999976, 0.42049113, 0.51182777, 0.38293913, 0.75148505, 0.9999995]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #78: Train loss [0.2189]; Val loss: MSE [0.1954], L1 [0.3652], G-Mean [0.0636]
 * Overall: MSE 0.168	L1 0.340	G-Mean 0.080
 * Many: MSE 0.104	L1 0.292	G-Mean 0.213
 * Low: MSE 0.296	L1 0.435	G-Mean 0.011
Predicted  [1.0, 0.4066581, 0.9386121, 0.9885513, 0.43648547, 0.47940677, 0.46404392, 0.615474, 0.99999654]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #79: Train loss [0.2323]; Val loss: MSE [0.1676], L1 [0.3396], G-Mean [0.0798]
 * Overall: MSE 0.159	L1 0.340	G-Mean 0.112
 * Many: MSE 0.108	L1 0.305	G-Mean 0.264
 * Low: MSE 0.260	L1 0.410	G-Mean 0.020
Predicted  [1.0, 0.3414736, 0.9402358, 0.920449, 0.44445226, 0.43253544, 0.4901041, 0.61034995, 0.9999778]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #80: Train loss [0.2074]; Val loss: MSE [0.1586], L1 [0.3403], G-Mean [0.1119]
 * Overall: MSE 0.135	L1 0.313	G-Mean 0.093
 * Many: MSE 0.106	L1 0.294	G-Mean 0.178
 * Low: MSE 0.192	L1 0.352	G-Mean 0.025
Predicted  [1.0, 0.35598946, 0.93570286, 0.82976544, 0.4300089, 0.50641686, 0.4939317, 0.5249942, 0.99994075]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #81: Train loss [0.2179]; Val loss: MSE [0.1347], L1 [0.3130], G-Mean [0.0927]
 * Overall: MSE 0.094	L1 0.267	G-Mean 0.097
 * Many: MSE 0.104	L1 0.290	G-Mean 0.241
 * Low: MSE 0.075	L1 0.221	G-Mean 0.016
Predicted  [1.0, 0.44530922, 0.9364174, 0.47887728, 0.6360225, 0.33444953, 0.38082626, 0.48269412, 0.99996257]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #82: Train loss [0.2418]; Val loss: MSE [0.0941], L1 [0.2668], G-Mean [0.0974]
 * Overall: MSE 0.102	L1 0.277	G-Mean 0.122
 * Many: MSE 0.087	L1 0.270	G-Mean 0.243
 * Low: MSE 0.131	L1 0.290	G-Mean 0.031
Predicted  [1.0, 0.51437575, 0.94093895, 0.7250487, 0.51765984, 0.3802483, 0.5090094, 0.4441063, 0.99983776]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #83: Train loss [0.2186]; Val loss: MSE [0.1017], L1 [0.2766], G-Mean [0.1222]
 * Overall: MSE 0.109	L1 0.301	G-Mean 0.078
 * Many: MSE 0.119	L1 0.331	G-Mean 0.310
 * Low: MSE 0.089	L1 0.241	G-Mean 0.005
Predicted  [1.0, 0.32147586, 0.93593043, 0.50802225, 0.416717, 0.36611587, 0.44780514, 0.5159395, 0.99999905]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #84: Train loss [0.1863]; Val loss: MSE [0.1093], L1 [0.3009], G-Mean [0.0781]
 * Overall: MSE 0.093	L1 0.266	G-Mean 0.061
 * Many: MSE 0.098	L1 0.281	G-Mean 0.225
 * Low: MSE 0.084	L1 0.237	G-Mean 0.004
Predicted  [1.0, 0.36401546, 0.94096637, 0.5461533, 0.5389172, 0.5446168, 0.4978874, 0.46335334, 0.9999993]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #85: Train loss [0.2587]; Val loss: MSE [0.0932], L1 [0.2660], G-Mean [0.0609]
 * Overall: MSE 0.146	L1 0.326	G-Mean 0.088
 * Many: MSE 0.125	L1 0.322	G-Mean 0.218
 * Low: MSE 0.188	L1 0.334	G-Mean 0.014
Predicted  [1.0, 0.29380733, 0.9226504, 0.87466794, 0.41113946, 0.5134975, 0.39819762, 0.42787308, 0.99998736]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #86: Train loss [0.2204]; Val loss: MSE [0.1457], L1 [0.3262], G-Mean [0.0875]
 * Overall: MSE 0.149	L1 0.344	G-Mean 0.104
 * Many: MSE 0.126	L1 0.347	G-Mean 0.335
 * Low: MSE 0.193	L1 0.338	G-Mean 0.010
Predicted  [1.0, 0.34453443, 0.91793495, 0.88792354, 0.3622648, 0.31619576, 0.41428646, 0.4270213, 0.9999956]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #87: Train loss [0.1751]; Val loss: MSE [0.1486], L1 [0.3440], G-Mean [0.1039]
 * Overall: MSE 0.103	L1 0.292	G-Mean 0.079
 * Many: MSE 0.110	L1 0.317	G-Mean 0.295
 * Low: MSE 0.089	L1 0.244	G-Mean 0.006
Predicted  [1.0, 0.40945706, 0.92697763, 0.57464415, 0.4154126, 0.3733972, 0.42802322, 0.4566562, 0.9999987]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #88: Train loss [0.2306]; Val loss: MSE [0.1033], L1 [0.2924], G-Mean [0.0788]
 * Overall: MSE 0.118	L1 0.303	G-Mean 0.068
 * Many: MSE 0.141	L1 0.355	G-Mean 0.325
 * Low: MSE 0.073	L1 0.199	G-Mean 0.003
Predicted  [1.0, 0.39927822, 0.95033115, 0.35906878, 0.35124043, 0.37761378, 0.29327002, 0.5391691, 0.99999964]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #89: Train loss [0.1906]; Val loss: MSE [0.1183], L1 [0.3030], G-Mean [0.0675]
 * Overall: MSE 0.102	L1 0.278	G-Mean 0.000
 * Many: MSE 0.119	L1 0.322	G-Mean 0.296
 * Low: MSE 0.069	L1 0.190	G-Mean 0.000
Predicted  [1.0, 0.47908086, 0.94632125, 0.33359554, 0.5603103, 0.24308261, 0.32908344, 0.53512675, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #90: Train loss [0.1932]; Val loss: MSE [0.1025], L1 [0.2782], G-Mean [0.0000]
 * Overall: MSE 0.104	L1 0.278	G-Mean 0.000
 * Many: MSE 0.125	L1 0.323	G-Mean 0.270
 * Low: MSE 0.063	L1 0.187	G-Mean 0.000
Predicted  [1.0, 0.6411075, 0.9487957, 0.35385478, 0.4013609, 0.24374515, 0.32238504, 0.50751734, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #91: Train loss [0.2124]; Val loss: MSE [0.1042], L1 [0.2780], G-Mean [0.0000]
 * Overall: MSE 0.102	L1 0.292	G-Mean 0.000
 * Many: MSE 0.104	L1 0.312	G-Mean 0.301
 * Low: MSE 0.098	L1 0.253	G-Mean 0.000
Predicted  [1.0, 0.44288006, 0.9374131, 0.5244879, 0.48434728, 0.26024798, 0.47901782, 0.53486913, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.222
Epoch #92: Train loss [0.1807]; Val loss: MSE [0.1021], L1 [0.2923], G-Mean [0.0000]
 * Overall: MSE 0.069	L1 0.192	G-Mean 0.000
 * Many: MSE 0.079	L1 0.207	G-Mean 0.064
 * Low: MSE 0.049	L1 0.162	G-Mean 0.000
Predicted  [1.0, 0.6821744, 0.940547, 0.32519275, 0.69838744, 0.4607428, 0.45698416, 0.4619129, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.192
Epoch #93: Train loss [0.1933]; Val loss: MSE [0.0689], L1 [0.1922], G-Mean [0.0000]
 * Overall: MSE 0.116	L1 0.307	G-Mean 0.063
 * Many: MSE 0.116	L1 0.326	G-Mean 0.312
 * Low: MSE 0.115	L1 0.267	G-Mean 0.003
Predicted  [1.0, 0.46795896, 0.9701432, 0.7130861, 0.43548977, 0.28907046, 0.41884476, 0.38683912, 0.9999999]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.192
Epoch #94: Train loss [0.2082]; Val loss: MSE [0.1156], L1 [0.3065], G-Mean [0.0633]
 * Overall: MSE 0.126	L1 0.305	G-Mean 0.063
 * Many: MSE 0.114	L1 0.304	G-Mean 0.198
 * Low: MSE 0.150	L1 0.308	G-Mean 0.006
Predicted  [1.0, 0.34274822, 0.96749675, 0.76694286, 0.46804622, 0.5108217, 0.44413543, 0.45672396, 0.9999987]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.192
Epoch #95: Train loss [0.2255]; Val loss: MSE [0.1262], L1 [0.3052], G-Mean [0.0631]
 * Overall: MSE 0.118	L1 0.309	G-Mean 0.000
 * Many: MSE 0.130	L1 0.338	G-Mean 0.299
 * Low: MSE 0.094	L1 0.249	G-Mean 0.000
Predicted  [1.0, 0.3465187, 0.96492666, 0.5518307, 0.27478388, 0.41587386, 0.49902984, 0.49597725, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.192
Epoch #96: Train loss [0.2120]; Val loss: MSE [0.1178], L1 [0.3085], G-Mean [0.0000]
 * Overall: MSE 0.109	L1 0.284	G-Mean 0.000
 * Many: MSE 0.123	L1 0.315	G-Mean 0.258
 * Low: MSE 0.080	L1 0.222	G-Mean 0.000
Predicted  [1.0, 0.3776049, 0.93380773, 0.4383195, 0.63890165, 0.32879326, 0.29829273, 0.52843547, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.192
Epoch #97: Train loss [0.2155]; Val loss: MSE [0.1086], L1 [0.2841], G-Mean [0.0000]
 * Overall: MSE 0.086	L1 0.230	G-Mean 0.000
 * Many: MSE 0.098	L1 0.249	G-Mean 0.108
 * Low: MSE 0.063	L1 0.194	G-Mean 0.000
Predicted  [1.0, 0.53921264, 0.930352, 0.38935837, 0.69624317, 0.4671848, 0.33652174, 0.4914558, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.192
Epoch #98: Train loss [0.2104]; Val loss: MSE [0.0862], L1 [0.2302], G-Mean [0.0000]
 * Overall: MSE 0.072	L1 0.216	G-Mean 0.036
 * Many: MSE 0.073	L1 0.218	G-Mean 0.147
 * Low: MSE 0.070	L1 0.212	G-Mean 0.002
Predicted  [1.0, 0.5473909, 0.95212674, 0.4530577, 0.7528851, 0.5309921, 0.5806711, 0.4818291, 0.9999999]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.192
Epoch #99: Train loss [0.1845]; Val loss: MSE [0.0722], L1 [0.2159], G-Mean [0.0365]
========================================================================================================================
Test best model on testset...
Loaded best model, epoch 94, best val loss 0.1922
 * Overall: MSE 0.115	L1 0.226	G-Mean 0.101
 * Many: MSE 0.022	L1 0.107	G-Mean 0.062
 * Low: MSE 0.490	L1 0.700	G-Mean 0.700
Predicted  [0.8380178, 1.0, 0.90361214, 0.6862364, 0.97353923]  for true improvements  [0.8, 0.3, 0.8, 0.7, 0.7]
Test loss: MSE [0.1154], L1 [0.2258], G-Mean [0.1008]
Done
