Use GPU: 0 for training
=====> Preparing data...
Using re-weighting: [SQRT_INV]
Using LDS: [GAUSSIAN] (3/1)
Training data size: 33
Validation data size: 9
Test data size: 5
[0.8 0.3 0.8 0.7 0.7]
=====> Building model...
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(256, 256, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=0)
    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (linear): Linear(in_features=12288, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
 * Overall: MSE 0.180	L1 0.346	G-Mean 0.163
 * Many: MSE 0.095	L1 0.278	G-Mean 0.216
 * Low: MSE 0.349	L1 0.481	G-Mean 0.092
Predicted  [0.99984837, 0.97033453, 0.53165156, 0.98029375, 0.9705747, 0.94613683, 0.5508604, 0.7625226, 0.99847704]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.346
Epoch #0: Train loss [0.2455]; Val loss: MSE [0.1796], L1 [0.3458], G-Mean [0.1630]
 * Overall: MSE 0.149	L1 0.323	G-Mean 0.106
 * Many: MSE 0.102	L1 0.289	G-Mean 0.233
 * Low: MSE 0.243	L1 0.391	G-Mean 0.022
Predicted  [0.99999976, 0.99238646, 0.5408854, 0.9303761, 0.99354494, 0.96520233, 0.55591154, 0.54264104, 0.9999659]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.323
Epoch #1: Train loss [0.1553]; Val loss: MSE [0.1487], L1 [0.3232], G-Mean [0.1065]
 * Overall: MSE 0.133	L1 0.298	G-Mean 0.045
 * Many: MSE 0.082	L1 0.256	G-Mean 0.169
 * Low: MSE 0.236	L1 0.382	G-Mean 0.003
Predicted  [1.0, 0.99641895, 0.4899499, 0.93337303, 0.99760497, 0.8446558, 0.61270046, 0.5113275, 0.9999999]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.298
Epoch #2: Train loss [0.1991]; Val loss: MSE [0.1330], L1 [0.2979], G-Mean [0.0454]
 * Overall: MSE 0.169	L1 0.348	G-Mean 0.000
 * Many: MSE 0.100	L1 0.300	G-Mean 0.281
 * Low: MSE 0.307	L1 0.444	G-Mean 0.000
Predicted  [1.0, 0.99975044, 0.36512393, 0.9992262, 0.99986327, 0.9352774, 0.56804925, 0.6316529, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.298
Epoch #3: Train loss [0.2091]; Val loss: MSE [0.1692], L1 [0.3481], G-Mean [0.0000]
/home/ali/anaconda3/envs/pdradenv/lib/python3.7/site-packages/scipy/stats/stats.py:283: RuntimeWarning: divide by zero encountered in log
  log_a = np.log(a)
 * Overall: MSE 0.203	L1 0.373	G-Mean 0.000
 * Many: MSE 0.109	L1 0.304	G-Mean 0.269
 * Low: MSE 0.391	L1 0.510	G-Mean 0.000
Predicted  [1.0, 0.9999888, 0.41331816, 0.9999982, 0.99999607, 0.9939878, 0.5558074, 0.83061683, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.298
Epoch #4: Train loss [0.2274]; Val loss: MSE [0.2028], L1 [0.3728], G-Mean [0.0000]
 * Overall: MSE 0.129	L1 0.290	G-Mean 0.214
 * Many: MSE 0.079	L1 0.252	G-Mean 0.196
 * Low: MSE 0.228	L1 0.366	G-Mean 0.255
Predicted  [0.99968255, 0.73154527, 0.31267476, 0.99708277, 0.940871, 0.8947144, 0.54502887, 0.2099556, 0.810473]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.290
Epoch #5: Train loss [0.2037]; Val loss: MSE [0.1287], L1 [0.2895], G-Mean [0.2142]
 * Overall: MSE 0.276	L1 0.454	G-Mean 0.110
 * Many: MSE 0.177	L1 0.399	G-Mean 0.375
 * Low: MSE 0.476	L1 0.562	G-Mean 0.009
Predicted  [1.0, 0.07314358, 0.85122806, 0.99710864, 0.27664092, 0.91280556, 0.6180959, 0.9901991, 0.9999988]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.290
Epoch #6: Train loss [0.2236]; Val loss: MSE [0.2763], L1 [0.4537], G-Mean [0.1101]
 * Overall: MSE 0.246	L1 0.414	G-Mean 0.000
 * Many: MSE 0.127	L1 0.338	G-Mean 0.314
 * Low: MSE 0.483	L1 0.567	G-Mean 0.000
Predicted  [1.0, 0.9893798, 0.96679795, 1.0, 0.999138, 0.6370925, 0.36352038, 1.0, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.290
Epoch #7: Train loss [0.1999]; Val loss: MSE [0.2456], L1 [0.4143], G-Mean [0.0000]
 * Overall: MSE 0.217	L1 0.361	G-Mean 0.000
 * Many: MSE 0.084	L1 0.258	G-Mean 0.221
 * Low: MSE 0.483	L1 0.567	G-Mean 0.000
Predicted  [1.0, 0.89145344, 0.8866692, 1.0, 0.81417155, 0.40665463, 0.43498796, 1.0, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.290
Epoch #8: Train loss [0.2276]; Val loss: MSE [0.2168], L1 [0.3612], G-Mean [0.0000]
 * Overall: MSE 0.222	L1 0.368	G-Mean 0.000
 * Many: MSE 0.091	L1 0.269	G-Mean 0.225
 * Low: MSE 0.483	L1 0.567	G-Mean 0.000
Predicted  [1.0, 0.9852636, 0.94825256, 1.0, 0.9925357, 0.39411438, 0.7187516, 1.0, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.290
Epoch #9: Train loss [0.2093]; Val loss: MSE [0.2217], L1 [0.3681], G-Mean [0.0000]
 * Overall: MSE 0.202	L1 0.310	G-Mean 0.000
 * Many: MSE 0.062	L1 0.181	G-Mean 0.073
 * Low: MSE 0.483	L1 0.567	G-Mean 0.000
Predicted  [1.0, 0.61025274, 0.93540406, 1.0, 0.5885283, 0.5017581, 0.7513107, 0.99997306, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.290
Epoch #10: Train loss [0.1881]; Val loss: MSE [0.2025], L1 [0.3097], G-Mean [0.0000]
 * Overall: MSE 0.208	L1 0.349	G-Mean 0.000
 * Many: MSE 0.072	L1 0.241	G-Mean 0.187
 * Low: MSE 0.480	L1 0.565	G-Mean 0.000
Predicted  [1.0, 0.41404107, 0.7073555, 1.0, 0.3821613, 0.7075474, 0.82743263, 0.9948388, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.290
Epoch #11: Train loss [0.2029]; Val loss: MSE [0.2078], L1 [0.3490], G-Mean [0.0000]
 * Overall: MSE 0.190	L1 0.330	G-Mean 0.221
 * Many: MSE 0.110	L1 0.278	G-Mean 0.199
 * Low: MSE 0.350	L1 0.433	G-Mean 0.271
Predicted  [0.0, 0.72830576, 0.65257114, 2.1314207e-14, 0.4528224, 0.7502723, 0.41053823, 7.608557e-05, 1.1211141e-22]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.290
Epoch #12: Train loss [0.1901]; Val loss: MSE [0.1899], L1 [0.3297], G-Mean [0.2208]
 * Overall: MSE 0.173	L1 0.288	G-Mean 0.169
 * Many: MSE 0.087	L1 0.228	G-Mean 0.168
 * Low: MSE 0.347	L1 0.408	G-Mean 0.170
Predicted  [0.0, 0.85573566, 0.82248455, 2.2068241e-13, 0.7596009, 0.64144915, 0.71024364, 0.075450994, 7.3585754e-27]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.288
Epoch #13: Train loss [0.1674]; Val loss: MSE [0.1734], L1 [0.2882], G-Mean [0.1689]
 * Overall: MSE 0.280	L1 0.411	G-Mean 0.231
 * Many: MSE 0.151	L1 0.290	G-Mean 0.152
 * Low: MSE 0.538	L1 0.653	G-Mean 0.533
Predicted  [1.2179492e-11, 0.7388369, 0.8623427, 7.726718e-06, 0.7327803, 0.57128274, 0.16398957, 0.8575648, 2.1346113e-13]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.288
Epoch #14: Train loss [0.1681]; Val loss: MSE [0.2797], L1 [0.4110], G-Mean [0.2314]
 * Overall: MSE 0.119	L1 0.240	G-Mean 0.041
 * Many: MSE 0.037	L1 0.150	G-Mean 0.114
 * Low: MSE 0.285	L1 0.421	G-Mean 0.005
Predicted  [1.0, 0.5826807, 0.5563828, 0.9996922, 0.65393865, 0.60608155, 0.628358, 0.5645175, 0.99999964]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.240
Epoch #15: Train loss [0.2178]; Val loss: MSE [0.1195], L1 [0.2402], G-Mean [0.0405]
 * Overall: MSE 0.113	L1 0.232	G-Mean 0.000
 * Many: MSE 0.039	L1 0.151	G-Mean 0.098
 * Low: MSE 0.262	L1 0.393	G-Mean 0.000
Predicted  [1.0, 0.75828314, 0.5144534, 0.99998736, 0.7988, 0.6982794, 0.9371029, 0.48045546, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #16: Train loss [0.2157]; Val loss: MSE [0.1129], L1 [0.2319], G-Mean [0.0000]
 * Overall: MSE 0.228	L1 0.390	G-Mean 0.270
 * Many: MSE 0.068	L1 0.218	G-Mean 0.164
 * Low: MSE 0.548	L1 0.734	G-Mean 0.728
Predicted  [0.9999999, 0.5526418, 0.6736316, 0.99784744, 0.83971536, 0.91713756, 0.8329566, 0.69977075, 0.19432274]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #17: Train loss [0.2353]; Val loss: MSE [0.2280], L1 [0.3905], G-Mean [0.2698]
 * Overall: MSE 0.259	L1 0.375	G-Mean 0.190
 * Many: MSE 0.098	L1 0.226	G-Mean 0.114
 * Low: MSE 0.580	L1 0.672	G-Mean 0.527
Predicted  [6.4375426e-14, 0.73721856, 0.754243, 0.026093826, 0.67499065, 0.9030433, 0.7614411, 0.94359565, 0.00025107403]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #18: Train loss [0.2328]; Val loss: MSE [0.2591], L1 [0.3750], G-Mean [0.1901]
 * Overall: MSE 0.182	L1 0.330	G-Mean 0.085
 * Many: MSE 0.075	L1 0.239	G-Mean 0.193
 * Low: MSE 0.395	L1 0.512	G-Mean 0.017
Predicted  [1.0, 0.4237715, 0.90734863, 0.99998474, 0.5410022, 0.6447303, 0.7504342, 0.8374724, 0.9999924]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #19: Train loss [0.2425]; Val loss: MSE [0.1816], L1 [0.3305], G-Mean [0.0850]
 * Overall: MSE 0.208	L1 0.372	G-Mean 0.000
 * Many: MSE 0.103	L1 0.294	G-Mean 0.266
 * Low: MSE 0.417	L1 0.528	G-Mean 0.000
Predicted  [1.0, 0.20429982, 0.6363028, 1.0, 0.3654914, 0.7350134, 0.96542513, 0.88257706, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #20: Train loss [0.2425]; Val loss: MSE [0.2079], L1 [0.3722], G-Mean [0.0000]
 * Overall: MSE 0.188	L1 0.311	G-Mean 0.000
 * Many: MSE 0.063	L1 0.197	G-Mean 0.124
 * Low: MSE 0.438	L1 0.540	G-Mean 0.000
Predicted  [1.0, 0.41695014, 0.46209708, 1.0, 0.33870238, 0.46291777, 0.86266965, 0.9214651, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #21: Train loss [0.3152]; Val loss: MSE [0.1880], L1 [0.3115], G-Mean [0.0000]
 * Overall: MSE 0.208	L1 0.350	G-Mean 0.000
 * Many: MSE 0.080	L1 0.246	G-Mean 0.190
 * Low: MSE 0.465	L1 0.557	G-Mean 0.000
Predicted  [1.0, 0.28633723, 0.6017795, 0.9999957, 0.45615488, 0.22318158, 0.841988, 0.9697593, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #22: Train loss [0.2689]; Val loss: MSE [0.2084], L1 [0.3498], G-Mean [0.0000]
 * Overall: MSE 0.177	L1 0.310	G-Mean 0.000
 * Many: MSE 0.058	L1 0.203	G-Mean 0.159
 * Low: MSE 0.414	L1 0.523	G-Mean 0.000
Predicted  [1.0, 0.6022582, 0.82824534, 0.9173187, 0.6404251, 0.25548556, 0.8871582, 0.952699, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #23: Train loss [0.2277]; Val loss: MSE [0.1767], L1 [0.3097], G-Mean [0.0000]
 * Overall: MSE 0.215	L1 0.355	G-Mean 0.161
 * Many: MSE 0.098	L1 0.230	G-Mean 0.091
 * Low: MSE 0.448	L1 0.605	G-Mean 0.507
Predicted  [1.2572435e-08, 0.7015298, 0.9376929, 0.0001730844, 0.81852853, 0.4102349, 0.9300203, 0.90009683, 0.18465568]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #24: Train loss [0.2423]; Val loss: MSE [0.2151], L1 [0.3548], G-Mean [0.1609]
 * Overall: MSE 0.263	L1 0.390	G-Mean 0.252
 * Many: MSE 0.097	L1 0.241	G-Mean 0.170
 * Low: MSE 0.597	L1 0.689	G-Mean 0.558
Predicted  [8.5716994e-19, 0.74954444, 0.90299535, 2.521046e-05, 0.87197477, 0.5891415, 0.92939657, 0.9807153, 0.012350721]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #25: Train loss [0.2671]; Val loss: MSE [0.2634], L1 [0.3902], G-Mean [0.2522]
 * Overall: MSE 0.271	L1 0.394	G-Mean 0.234
 * Many: MSE 0.101	L1 0.244	G-Mean 0.152
 * Low: MSE 0.610	L1 0.694	G-Mean 0.555
Predicted  [3.1483148e-27, 0.83688277, 0.91317546, 0.008636355, 0.9242011, 0.564675, 0.8248168, 0.9910217, 6.018019e-05]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #26: Train loss [0.2301]; Val loss: MSE [0.2705], L1 [0.3940], G-Mean [0.2337]
 * Overall: MSE 0.160	L1 0.303	G-Mean 0.190
 * Many: MSE 0.075	L1 0.226	G-Mean 0.179
 * Low: MSE 0.331	L1 0.456	G-Mean 0.212
Predicted  [0.9982161, 0.54501575, 0.97973734, 0.6623178, 0.8404297, 0.6137205, 0.7299061, 0.98228514, 0.9764974]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #27: Train loss [0.2483]; Val loss: MSE [0.1603], L1 [0.3028], G-Mean [0.1897]
 * Overall: MSE 0.349	L1 0.516	G-Mean 0.393
 * Many: MSE 0.239	L1 0.443	G-Mean 0.348
 * Low: MSE 0.570	L1 0.661	G-Mean 0.502
Predicted  [0.99763787, 0.18944132, 0.98377466, 0.047359463, 0.23546389, 0.55192125, 0.050350226, 0.93239856, 0.0028768908]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #28: Train loss [0.2169]; Val loss: MSE [0.3493], L1 [0.5156], G-Mean [0.3933]
 * Overall: MSE 0.386	L1 0.559	G-Mean 0.472
 * Many: MSE 0.293	L1 0.504	G-Mean 0.443
 * Low: MSE 0.572	L1 0.670	G-Mean 0.535
Predicted  [4.2415668e-05, 0.21653587, 0.97874945, 0.0143431425, 0.14647233, 0.6251185, 0.01725217, 0.92530257, 0.00016948748]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #29: Train loss [0.2458]; Val loss: MSE [0.3859], L1 [0.5594], G-Mean [0.4716]
 * Overall: MSE 0.380	L1 0.555	G-Mean 0.460
 * Many: MSE 0.287	L1 0.499	G-Mean 0.429
 * Low: MSE 0.568	L1 0.667	G-Mean 0.531
Predicted  [3.3307668e-20, 0.06777515, 0.9183004, 0.016641669, 0.12907577, 0.60205424, 0.12975644, 0.91844267, 3.4970188e-05]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #30: Train loss [0.2286]; Val loss: MSE [0.3804], L1 [0.5551], G-Mean [0.4604]
 * Overall: MSE 0.252	L1 0.428	G-Mean 0.354
 * Many: MSE 0.199	L1 0.398	G-Mean 0.342
 * Low: MSE 0.356	L1 0.486	G-Mean 0.379
Predicted  [0.81534106, 0.15153961, 0.72735006, 0.38591915, 0.106320456, 0.6558123, 0.15133011, 0.40265375, 0.029377492]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #31: Train loss [0.2143]; Val loss: MSE [0.2516], L1 [0.4276], G-Mean [0.3543]
 * Overall: MSE 0.109	L1 0.243	G-Mean 0.103
 * Many: MSE 0.064	L1 0.217	G-Mean 0.118
 * Low: MSE 0.197	L1 0.297	G-Mean 0.079
Predicted  [1.0, 0.35715348, 0.5033686, 0.9587759, 0.45359647, 0.33185095, 0.9394482, 0.22681628, 0.9948841]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.232
Epoch #32: Train loss [0.2210]; Val loss: MSE [0.1086], L1 [0.2434], G-Mean [0.1031]
 * Overall: MSE 0.105	L1 0.231	G-Mean 0.039
 * Many: MSE 0.043	L1 0.176	G-Mean 0.146
 * Low: MSE 0.229	L1 0.339	G-Mean 0.003
Predicted  [1.0, 0.8502738, 0.67226386, 0.9998734, 0.65340114, 0.37624142, 0.96494627, 0.31807905, 0.9999999]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #33: Train loss [0.2088]; Val loss: MSE [0.1049], L1 [0.2306], G-Mean [0.0389]
 * Overall: MSE 0.107	L1 0.235	G-Mean 0.000
 * Many: MSE 0.052	L1 0.202	G-Mean 0.171
 * Low: MSE 0.217	L1 0.300	G-Mean 0.000
Predicted  [1.0, 0.47734043, 0.7048723, 0.99998796, 0.65009576, 0.30167383, 0.9385979, 0.19920586, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #34: Train loss [0.2821]; Val loss: MSE [0.1069], L1 [0.2348], G-Mean [0.0000]
 * Overall: MSE 0.175	L1 0.344	G-Mean 0.000
 * Many: MSE 0.102	L1 0.288	G-Mean 0.239
 * Low: MSE 0.321	L1 0.456	G-Mean 0.000
Predicted  [1.0, 0.22683945, 0.8563507, 0.99998796, 0.45281696, 0.44036806, 0.9890171, 0.6671675, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #35: Train loss [0.2384]; Val loss: MSE [0.1748], L1 [0.3436], G-Mean [0.0000]
 * Overall: MSE 0.201	L1 0.337	G-Mean 0.049
 * Many: MSE 0.079	L1 0.234	G-Mean 0.148
 * Low: MSE 0.446	L1 0.545	G-Mean 0.005
Predicted  [1.0, 0.3133756, 0.8651147, 0.99998486, 0.6856928, 0.4111302, 0.9462292, 0.93460137, 0.99999976]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #36: Train loss [0.2093]; Val loss: MSE [0.2010], L1 [0.3373], G-Mean [0.0491]
 * Overall: MSE 0.203	L1 0.317	G-Mean 0.043
 * Many: MSE 0.067	L1 0.194	G-Mean 0.104
 * Low: MSE 0.476	L1 0.563	G-Mean 0.008
Predicted  [1.0, 0.73011047, 0.94875854, 0.99999046, 0.7125295, 0.6274993, 0.94311255, 0.98785305, 0.9999994]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #37: Train loss [0.2671]; Val loss: MSE [0.2031], L1 [0.3167], G-Mean [0.0432]
 * Overall: MSE 0.215	L1 0.362	G-Mean 0.082
 * Many: MSE 0.083	L1 0.260	G-Mean 0.232
 * Low: MSE 0.479	L1 0.564	G-Mean 0.010
Predicted  [1.0, 0.9415027, 0.9492741, 0.9999994, 0.8117766, 0.67369145, 0.9850921, 0.9926917, 0.99999845]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #38: Train loss [0.2424]; Val loss: MSE [0.2149], L1 [0.3616], G-Mean [0.0823]
 * Overall: MSE 0.234	L1 0.412	G-Mean 0.346
 * Many: MSE 0.101	L1 0.295	G-Mean 0.272
 * Low: MSE 0.501	L1 0.647	G-Mean 0.560
Predicted  [0.9972855, 0.90493226, 0.9811219, 0.9999994, 0.9065549, 0.8283411, 0.95283955, 0.9955373, 0.755056]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #39: Train loss [0.2749]; Val loss: MSE [0.2341], L1 [0.4124], G-Mean [0.3461]
 * Overall: MSE 0.214	L1 0.348	G-Mean 0.060
 * Many: MSE 0.079	L1 0.239	G-Mean 0.196
 * Low: MSE 0.482	L1 0.566	G-Mean 0.006
Predicted  [1.0, 0.7954084, 0.97952545, 0.99999714, 0.7894361, 0.67969173, 0.9928247, 0.9983045, 0.99999976]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #40: Train loss [0.2447]; Val loss: MSE [0.2137], L1 [0.3484], G-Mean [0.0598]
 * Overall: MSE 0.222	L1 0.363	G-Mean 0.000
 * Many: MSE 0.091	L1 0.261	G-Mean 0.200
 * Low: MSE 0.483	L1 0.566	G-Mean 0.000
Predicted  [1.0, 0.7420642, 0.96617615, 0.99999464, 0.5785579, 0.835741, 0.9991825, 0.99923646, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #41: Train loss [0.2442]; Val loss: MSE [0.2217], L1 [0.3626], G-Mean [0.0000]
 * Overall: MSE 0.212	L1 0.331	G-Mean 0.000
 * Many: MSE 0.076	L1 0.214	G-Mean 0.077
 * Low: MSE 0.482	L1 0.566	G-Mean 0.000
Predicted  [1.0, 0.70262784, 0.9641485, 1.0, 0.71030235, 0.70676714, 0.9995394, 0.9974825, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #42: Train loss [0.3145]; Val loss: MSE [0.2115], L1 [0.3312], G-Mean [0.0000]
 * Overall: MSE 0.212	L1 0.356	G-Mean 0.000
 * Many: MSE 0.078	L1 0.251	G-Mean 0.211
 * Low: MSE 0.481	L1 0.566	G-Mean 0.000
Predicted  [1.0, 0.7597553, 0.88238925, 0.9999999, 0.8587769, 0.8059246, 0.999721, 0.99652475, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #43: Train loss [0.3347]; Val loss: MSE [0.2125], L1 [0.3559], G-Mean [0.0000]
 * Overall: MSE 0.223	L1 0.371	G-Mean 0.000
 * Many: MSE 0.095	L1 0.274	G-Mean 0.194
 * Low: MSE 0.478	L1 0.564	G-Mean 0.000
Predicted  [1.0, 0.71797234, 0.9328978, 0.9999987, 0.9431873, 0.8545189, 0.99689376, 0.9911863, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #44: Train loss [0.3033]; Val loss: MSE [0.2228], L1 [0.3707], G-Mean [0.0000]
 * Overall: MSE 0.213	L1 0.363	G-Mean 0.000
 * Many: MSE 0.083	L1 0.264	G-Mean 0.241
 * Low: MSE 0.473	L1 0.561	G-Mean 0.000
Predicted  [1.0, 0.86701226, 0.9533338, 0.9999212, 0.9105561, 0.6596337, 0.99116033, 0.98277825, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #45: Train loss [0.2811]; Val loss: MSE [0.2132], L1 [0.3627], G-Mean [0.0000]
 * Overall: MSE 0.209	L1 0.358	G-Mean 0.000
 * Many: MSE 0.083	L1 0.260	G-Mean 0.233
 * Low: MSE 0.460	L1 0.553	G-Mean 0.000
Predicted  [1.0, 0.85117793, 0.96166205, 0.99972934, 0.86748505, 0.27364525, 0.9508497, 0.9604026, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #46: Train loss [0.2543]; Val loss: MSE [0.2087], L1 [0.3575], G-Mean [0.0000]
 * Overall: MSE 0.209	L1 0.345	G-Mean 0.000
 * Many: MSE 0.087	L1 0.243	G-Mean 0.150
 * Low: MSE 0.453	L1 0.549	G-Mean 0.000
Predicted  [1.0, 0.88716066, 0.9446892, 0.9993856, 0.74979323, 0.14386655, 0.81908154, 0.9482613, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #47: Train loss [0.2747]; Val loss: MSE [0.2090], L1 [0.3449], G-Mean [0.0000]
 * Overall: MSE 0.203	L1 0.371	G-Mean 0.000
 * Many: MSE 0.108	L1 0.301	G-Mean 0.236
 * Low: MSE 0.393	L1 0.511	G-Mean 0.000
Predicted  [1.0, 0.7345743, 0.9408724, 0.9954389, 0.45931673, 0.14037438, 0.4719363, 0.83823633, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #48: Train loss [0.2646]; Val loss: MSE [0.2031], L1 [0.3708], G-Mean [0.0000]
 * Overall: MSE 0.197	L1 0.381	G-Mean 0.000
 * Many: MSE 0.111	L1 0.324	G-Mean 0.315
 * Low: MSE 0.370	L1 0.495	G-Mean 0.000
Predicted  [1.0, 0.4807893, 0.8411587, 0.9938784, 0.28156453, 0.25720567, 0.47547692, 0.7920118, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #49: Train loss [0.2869]; Val loss: MSE [0.1970], L1 [0.3813], G-Mean [0.0000]
 * Overall: MSE 0.194	L1 0.359	G-Mean 0.000
 * Many: MSE 0.123	L1 0.303	G-Mean 0.237
 * Low: MSE 0.338	L1 0.471	G-Mean 0.000
Predicted  [1.0, 0.38794774, 0.86696225, 0.99239963, 0.124412335, 0.57877016, 0.7149343, 0.720361, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #50: Train loss [0.2855]; Val loss: MSE [0.1944], L1 [0.3590], G-Mean [0.0000]
 * Overall: MSE 0.122	L1 0.275	G-Mean 0.153
 * Many: MSE 0.062	L1 0.224	G-Mean 0.197
 * Low: MSE 0.242	L1 0.377	G-Mean 0.092
Predicted  [0.50476766, 0.5631999, 0.9012899, 0.97641295, 0.4040271, 0.774632, 0.938375, 0.4505366, 0.9971455]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #51: Train loss [0.2617]; Val loss: MSE [0.1219], L1 [0.2747], G-Mean [0.1529]
 * Overall: MSE 0.163	L1 0.320	G-Mean 0.099
 * Many: MSE 0.115	L1 0.273	G-Mean 0.154
 * Low: MSE 0.260	L1 0.413	G-Mean 0.041
Predicted  [1.9519703e-21, 0.80150354, 0.8676124, 0.73797035, 0.690733, 0.8991337, 0.9603879, 0.10018907, 0.29987264]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #52: Train loss [0.2469]; Val loss: MSE [0.1634], L1 [0.3196], G-Mean [0.0994]
 * Overall: MSE 0.157	L1 0.315	G-Mean 0.196
 * Many: MSE 0.102	L1 0.263	G-Mean 0.204
 * Low: MSE 0.269	L1 0.417	G-Mean 0.183
Predicted  [9.642733e-23, 0.82449645, 0.86185277, 0.6701271, 0.6397976, 0.771573, 0.96172976, 0.11694606, 0.2355311]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #53: Train loss [0.2486]; Val loss: MSE [0.1573], L1 [0.3146], G-Mean [0.1964]
 * Overall: MSE 0.158	L1 0.338	G-Mean 0.226
 * Many: MSE 0.152	L1 0.353	G-Mean 0.303
 * Low: MSE 0.170	L1 0.309	G-Mean 0.126
Predicted  [2.0778194e-05, 0.30511284, 0.92248, 0.8709129, 0.28272286, 0.39289135, 0.9740761, 0.34375757, 0.98790026]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #54: Train loss [0.2206]; Val loss: MSE [0.1578], L1 [0.3381], G-Mean [0.2260]
 * Overall: MSE 0.232	L1 0.422	G-Mean 0.080
 * Many: MSE 0.205	L1 0.418	G-Mean 0.374
 * Low: MSE 0.286	L1 0.432	G-Mean 0.004
Predicted  [1.0, 0.08884318, 0.96905655, 0.9430935, 0.07153908, 0.261554, 0.9601708, 0.65164685, 0.9999999]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #55: Train loss [0.1918]; Val loss: MSE [0.2319], L1 [0.4224], G-Mean [0.0800]
 * Overall: MSE 0.241	L1 0.437	G-Mean 0.000
 * Many: MSE 0.236	L1 0.451	G-Mean 0.407
 * Low: MSE 0.251	L1 0.409	G-Mean 0.000
Predicted  [1.0, 0.022424243, 0.95051956, 0.80584514, 0.034873918, 0.14413916, 0.95662445, 0.7201676, 1.0]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #56: Train loss [0.2504]; Val loss: MSE [0.2408], L1 [0.4369], G-Mean [0.0000]
 * Overall: MSE 0.232	L1 0.419	G-Mean 0.096
 * Many: MSE 0.232	L1 0.436	G-Mean 0.360
 * Low: MSE 0.232	L1 0.386	G-Mean 0.007
Predicted  [1.0, 0.013526377, 0.92298293, 0.6622857, 0.024489403, 0.15105121, 0.87966245, 0.7949697, 0.99999905]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #57: Train loss [0.2750]; Val loss: MSE [0.2324], L1 [0.4190], G-Mean [0.0956]
 * Overall: MSE 0.222	L1 0.415	G-Mean 0.141
 * Many: MSE 0.192	L1 0.407	G-Mean 0.378
 * Low: MSE 0.283	L1 0.432	G-Mean 0.020
Predicted  [1.0, 0.03169542, 0.7451358, 0.78281736, 0.12673414, 0.21661083, 0.5278903, 0.81361985, 0.9999815]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #58: Train loss [0.2530]; Val loss: MSE [0.2221], L1 [0.4154], G-Mean [0.1412]
 * Overall: MSE 0.297	L1 0.488	G-Mean 0.421
 * Many: MSE 0.162	L1 0.365	G-Mean 0.323
 * Low: MSE 0.567	L1 0.733	G-Mean 0.713
Predicted  [1.1935144e-16, 0.13998298, 0.81169796, 0.7391805, 0.32328343, 0.33152273, 0.62828326, 0.804681, 0.04424955]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #59: Train loss [0.2504]; Val loss: MSE [0.2968], L1 [0.4876], G-Mean [0.4209]
 * Overall: MSE 0.249	L1 0.437	G-Mean 0.374
 * Many: MSE 0.139	L1 0.336	G-Mean 0.296
 * Low: MSE 0.469	L1 0.640	G-Mean 0.597
Predicted  [1.2518269e-18, 0.22264121, 0.78795254, 0.5936436, 0.3894409, 0.38357726, 0.57576346, 0.65598, 0.029662821]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #60: Train loss [0.2685]; Val loss: MSE [0.2485], L1 [0.4374], G-Mean [0.3737]
 * Overall: MSE 0.236	L1 0.433	G-Mean 0.328
 * Many: MSE 0.150	L1 0.331	G-Mean 0.236
 * Low: MSE 0.408	L1 0.638	G-Mean 0.637
Predicted  [2.0002708e-16, 0.23722164, 0.88574046, 0.8137422, 0.29098022, 0.42909798, 0.74450207, 0.7089351, 0.30862987]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #61: Train loss [0.2292]; Val loss: MSE [0.2360], L1 [0.4331], G-Mean [0.3285]
 * Overall: MSE 0.201	L1 0.415	G-Mean 0.375
 * Many: MSE 0.138	L1 0.338	G-Mean 0.305
 * Low: MSE 0.326	L1 0.568	G-Mean 0.566
Predicted  [3.7261726e-12, 0.21456009, 0.8078184, 0.8080736, 0.42896324, 0.30748346, 0.6270531, 0.69901633, 0.50175697]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #62: Train loss [0.2370]; Val loss: MSE [0.2009], L1 [0.4150], G-Mean [0.3751]
 * Overall: MSE 0.182	L1 0.365	G-Mean 0.192
 * Many: MSE 0.114	L1 0.275	G-Mean 0.116
 * Low: MSE 0.320	L1 0.546	G-Mean 0.527
Predicted  [3.4411843e-10, 0.30503938, 0.81095564, 0.94195503, 0.4494046, 0.41076648, 0.80148906, 0.48781776, 0.49209237]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #63: Train loss [0.2758]; Val loss: MSE [0.1825], L1 [0.3650], G-Mean [0.1922]
 * Overall: MSE 0.197	L1 0.381	G-Mean 0.283
 * Many: MSE 0.135	L1 0.310	G-Mean 0.219
 * Low: MSE 0.321	L1 0.524	G-Mean 0.473
Predicted  [1.0505661e-05, 0.16903028, 0.7821228, 0.98226225, 0.45794955, 0.3248267, 0.77125525, 0.63755536, 0.7479354]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #64: Train loss [0.2636]; Val loss: MSE [0.1973], L1 [0.3812], G-Mean [0.2829]
 * Overall: MSE 0.178	L1 0.357	G-Mean 0.281
 * Many: MSE 0.104	L1 0.266	G-Mean 0.210
 * Low: MSE 0.326	L1 0.540	G-Mean 0.506
Predicted  [5.0384738e-08, 0.30639917, 0.76364833, 0.96249986, 0.5930676, 0.36117378, 0.70753855, 0.64798504, 0.6903763]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #65: Train loss [0.2037]; Val loss: MSE [0.1779], L1 [0.3573], G-Mean [0.2814]
 * Overall: MSE 0.138	L1 0.299	G-Mean 0.213
 * Many: MSE 0.077	L1 0.201	G-Mean 0.143
 * Low: MSE 0.262	L1 0.494	G-Mean 0.476
Predicted  [1.6717197e-08, 0.6005359, 0.7624034, 0.86691546, 0.7901146, 0.43094733, 0.8876989, 0.57338744, 0.6593801]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #66: Train loss [0.2465]; Val loss: MSE [0.1382], L1 [0.2989], G-Mean [0.2134]
 * Overall: MSE 0.155	L1 0.341	G-Mean 0.274
 * Many: MSE 0.090	L1 0.245	G-Mean 0.197
 * Low: MSE 0.286	L1 0.534	G-Mean 0.533
Predicted  [1.4015636e-07, 0.5119994, 0.78871053, 0.7370491, 0.7787806, 0.2989141, 0.91300446, 0.6656361, 0.5015302]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #67: Train loss [0.1789]; Val loss: MSE [0.1550], L1 [0.3412], G-Mean [0.2744]
 * Overall: MSE 0.211	L1 0.366	G-Mean 0.220
 * Many: MSE 0.085	L1 0.213	G-Mean 0.127
 * Low: MSE 0.463	L1 0.672	G-Mean 0.664
Predicted  [5.6896243e-07, 0.54609984, 0.83910304, 0.73944575, 0.7784351, 0.4130148, 0.78062755, 0.77865696, 0.20103689]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #68: Train loss [0.2636]; Val loss: MSE [0.2114], L1 [0.3661], G-Mean [0.2201]
 * Overall: MSE 0.215	L1 0.364	G-Mean 0.219
 * Many: MSE 0.079	L1 0.208	G-Mean 0.127
 * Low: MSE 0.486	L1 0.674	G-Mean 0.649
Predicted  [5.382854e-07, 0.62603176, 0.74585754, 0.6408293, 0.8420902, 0.3288383, 0.8158766, 0.8094933, 0.12719682]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #69: Train loss [0.2058]; Val loss: MSE [0.2150], L1 [0.3636], G-Mean [0.2188]
 * Overall: MSE 0.243	L1 0.400	G-Mean 0.274
 * Many: MSE 0.083	L1 0.225	G-Mean 0.165
 * Low: MSE 0.564	L1 0.751	G-Mean 0.750
Predicted  [0.0020129727, 0.620255, 0.7198288, 0.951079, 0.85745573, 0.25577444, 0.74929875, 0.8241775, 0.22306497]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #70: Train loss [0.2297]; Val loss: MSE [0.2435], L1 [0.4002], G-Mean [0.2738]
 * Overall: MSE 0.192	L1 0.354	G-Mean 0.210
 * Many: MSE 0.076	L1 0.209	G-Mean 0.121
 * Low: MSE 0.423	L1 0.642	G-Mean 0.634
Predicted  [0.022335257, 0.5705276, 0.7389394, 0.96021885, 0.8908077, 0.38860467, 0.80829954, 0.7548634, 0.487845]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #71: Train loss [0.2212]; Val loss: MSE [0.1917], L1 [0.3538], G-Mean [0.2103]
 * Overall: MSE 0.183	L1 0.349	G-Mean 0.260
 * Many: MSE 0.093	L1 0.266	G-Mean 0.227
 * Low: MSE 0.363	L1 0.515	G-Mean 0.342
Predicted  [0.058971368, 0.80717623, 0.86702013, 0.9817861, 0.89201814, 0.23756658, 0.92758256, 0.78799844, 0.9258562]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #72: Train loss [0.2226]; Val loss: MSE [0.1834], L1 [0.3490], G-Mean [0.2601]
 * Overall: MSE 0.183	L1 0.342	G-Mean 0.209
 * Many: MSE 0.094	L1 0.242	G-Mean 0.142
 * Low: MSE 0.362	L1 0.541	G-Mean 0.448
Predicted  [0.021981714, 0.84349734, 0.8717118, 0.95622814, 0.93563044, 0.38549432, 0.8100172, 0.79611045, 0.829252]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #73: Train loss [0.2183]; Val loss: MSE [0.1830], L1 [0.3418], G-Mean [0.2086]
 * Overall: MSE 0.173	L1 0.328	G-Mean 0.217
 * Many: MSE 0.078	L1 0.216	G-Mean 0.147
 * Low: MSE 0.363	L1 0.551	G-Mean 0.477
Predicted  [0.08889499, 0.6307181, 0.9061773, 0.9397205, 0.8532984, 0.4615781, 0.68203306, 0.80742365, 0.7930376]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #74: Train loss [0.1785]; Val loss: MSE [0.1734], L1 [0.3278], G-Mean [0.2173]
 * Overall: MSE 0.183	L1 0.336	G-Mean 0.230
 * Many: MSE 0.096	L1 0.253	G-Mean 0.204
 * Low: MSE 0.358	L1 0.502	G-Mean 0.291
Predicted  [4.0354723e-07, 0.6021655, 0.8664769, 0.8855313, 0.8395669, 0.63262993, 0.61778253, 0.8748535, 0.9534246]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #75: Train loss [0.2295]; Val loss: MSE [0.1830], L1 [0.3362], G-Mean [0.2299]
 * Overall: MSE 0.162	L1 0.321	G-Mean 0.210
 * Many: MSE 0.089	L1 0.222	G-Mean 0.139
 * Low: MSE 0.307	L1 0.520	G-Mean 0.482
Predicted  [9.715427e-13, 0.81146926, 0.85436624, 0.732618, 0.88137025, 0.46903425, 0.8542099, 0.8470034, 0.7192743]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #76: Train loss [0.2233]; Val loss: MSE [0.1617], L1 [0.3214], G-Mean [0.2103]
 * Overall: MSE 0.170	L1 0.341	G-Mean 0.249
 * Many: MSE 0.091	L1 0.238	G-Mean 0.172
 * Low: MSE 0.327	L1 0.547	G-Mean 0.525
Predicted  [3.058036e-19, 0.75253975, 0.8322013, 0.64445096, 0.8963933, 0.6774555, 0.7295244, 0.88143617, 0.58454007]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #77: Train loss [0.1705]; Val loss: MSE [0.1699], L1 [0.3412], G-Mean [0.2492]
 * Overall: MSE 0.169	L1 0.336	G-Mean 0.249
 * Many: MSE 0.092	L1 0.241	G-Mean 0.180
 * Low: MSE 0.323	L1 0.526	G-Mean 0.477
Predicted  [2.0985427e-19, 0.5806811, 0.8530368, 0.72883815, 0.8454841, 0.55462825, 0.62878114, 0.88776886, 0.73983175]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #78: Train loss [0.1820]; Val loss: MSE [0.1689], L1 [0.3356], G-Mean [0.2494]
 * Overall: MSE 0.216	L1 0.405	G-Mean 0.335
 * Many: MSE 0.105	L1 0.282	G-Mean 0.242
 * Low: MSE 0.436	L1 0.652	G-Mean 0.643
Predicted  [3.1079293e-23, 0.50782514, 0.8028424, 0.7166434, 0.8718672, 0.6036071, 0.47866008, 0.86911476, 0.3295365]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #79: Train loss [0.1991]; Val loss: MSE [0.2156], L1 [0.4053], G-Mean [0.3351]
 * Overall: MSE 0.173	L1 0.353	G-Mean 0.275
 * Many: MSE 0.091	L1 0.246	G-Mean 0.194
 * Low: MSE 0.337	L1 0.567	G-Mean 0.555
Predicted  [1.0033763e-18, 0.4254469, 0.7601905, 0.7363251, 0.7734974, 0.6067913, 0.6407775, 0.8291031, 0.56353915]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #80: Train loss [0.1646]; Val loss: MSE [0.1728], L1 [0.3529], G-Mean [0.2755]
 * Overall: MSE 0.200	L1 0.394	G-Mean 0.332
 * Many: MSE 0.105	L1 0.282	G-Mean 0.244
 * Low: MSE 0.391	L1 0.619	G-Mean 0.613
Predicted  [4.9079977e-15, 0.53427577, 0.7372126, 0.92784464, 0.82674044, 0.6913776, 0.43141475, 0.71513397, 0.4861563]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #81: Train loss [0.2003]; Val loss: MSE [0.2005], L1 [0.3941], G-Mean [0.3316]
 * Overall: MSE 0.155	L1 0.324	G-Mean 0.231
 * Many: MSE 0.084	L1 0.224	G-Mean 0.156
 * Low: MSE 0.298	L1 0.524	G-Mean 0.502
Predicted  [4.6411e-13, 0.67155445, 0.7632778, 0.91448045, 0.87224585, 0.674859, 0.6919741, 0.6124384, 0.65432364]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #82: Train loss [0.1841]; Val loss: MSE [0.1550], L1 [0.3244], G-Mean [0.2307]
 * Overall: MSE 0.148	L1 0.296	G-Mean 0.177
 * Many: MSE 0.079	L1 0.198	G-Mean 0.112
 * Low: MSE 0.287	L1 0.492	G-Mean 0.442
Predicted  [5.748786e-12, 0.6623115, 0.7011891, 0.9585032, 0.762771, 0.75753576, 0.82749575, 0.5781794, 0.76166904]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #83: Train loss [0.1937]; Val loss: MSE [0.1482], L1 [0.2957], G-Mean [0.1775]
 * Overall: MSE 0.149	L1 0.301	G-Mean 0.193
 * Many: MSE 0.083	L1 0.211	G-Mean 0.130
 * Low: MSE 0.280	L1 0.479	G-Mean 0.422
Predicted  [5.5390874e-12, 0.67404956, 0.8039321, 0.9683954, 0.87093526, 0.6162612, 0.85183764, 0.5513101, 0.78330666]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #84: Train loss [0.1977]; Val loss: MSE [0.1488], L1 [0.3006], G-Mean [0.1927]
 * Overall: MSE 0.158	L1 0.319	G-Mean 0.236
 * Many: MSE 0.083	L1 0.227	G-Mean 0.177
 * Low: MSE 0.308	L1 0.501	G-Mean 0.420
Predicted  [4.6957846e-15, 0.60729825, 0.7743383, 0.9190906, 0.8611114, 0.5783105, 0.64276296, 0.7167683, 0.8328398]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #85: Train loss [0.1756]; Val loss: MSE [0.1585], L1 [0.3185], G-Mean [0.2357]
 * Overall: MSE 0.171	L1 0.337	G-Mean 0.245
 * Many: MSE 0.087	L1 0.231	G-Mean 0.171
 * Low: MSE 0.340	L1 0.548	G-Mean 0.502
Predicted  [1.3065935e-12, 0.7557128, 0.7866018, 0.8940139, 0.921503, 0.63272655, 0.71148074, 0.78400826, 0.7342693]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #86: Train loss [0.2136]; Val loss: MSE [0.1711], L1 [0.3365], G-Mean [0.2447]
 * Overall: MSE 0.171	L1 0.346	G-Mean 0.266
 * Many: MSE 0.089	L1 0.237	G-Mean 0.185
 * Low: MSE 0.337	L1 0.566	G-Mean 0.549
Predicted  [1.2975058e-13, 0.60250056, 0.8273453, 0.8230742, 0.8721076, 0.61930966, 0.69679606, 0.78731674, 0.61329836]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #87: Train loss [0.1747]; Val loss: MSE [0.1713], L1 [0.3463], G-Mean [0.2662]
 * Overall: MSE 0.174	L1 0.350	G-Mean 0.231
 * Many: MSE 0.090	L1 0.238	G-Mean 0.147
 * Low: MSE 0.343	L1 0.575	G-Mean 0.564
Predicted  [6.237415e-16, 0.7114672, 0.7838061, 0.7723626, 0.9245564, 0.65825987, 0.65212893, 0.814059, 0.56115013]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #88: Train loss [0.1482]; Val loss: MSE [0.1742], L1 [0.3501], G-Mean [0.2306]
 * Overall: MSE 0.182	L1 0.360	G-Mean 0.279
 * Many: MSE 0.085	L1 0.237	G-Mean 0.191
 * Low: MSE 0.376	L1 0.607	G-Mean 0.600
Predicted  [9.227635e-14, 0.6389765, 0.70093095, 0.8366531, 0.89218736, 0.69395745, 0.62532395, 0.79508877, 0.5117495]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #89: Train loss [0.1718]; Val loss: MSE [0.1818], L1 [0.3603], G-Mean [0.2793]
 * Overall: MSE 0.173	L1 0.368	G-Mean 0.314
 * Many: MSE 0.111	L1 0.295	G-Mean 0.253
 * Low: MSE 0.298	L1 0.515	G-Mean 0.485
Predicted  [2.0419602e-13, 0.7822751, 0.7799701, 0.9614046, 0.9216151, 0.7750128, 0.48757085, 0.5513683, 0.6681109]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #90: Train loss [0.2052]; Val loss: MSE [0.1735], L1 [0.3684], G-Mean [0.3144]
 * Overall: MSE 0.189	L1 0.368	G-Mean 0.287
 * Many: MSE 0.089	L1 0.244	G-Mean 0.197
 * Low: MSE 0.389	L1 0.617	G-Mean 0.611
Predicted  [7.470369e-15, 0.5939932, 0.74198395, 0.9412654, 0.8561214, 0.76748973, 0.70976186, 0.6356069, 0.42529523]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #91: Train loss [0.2112]; Val loss: MSE [0.1889], L1 [0.3682], G-Mean [0.2871]
 * Overall: MSE 0.170	L1 0.335	G-Mean 0.219
 * Many: MSE 0.082	L1 0.216	G-Mean 0.137
 * Low: MSE 0.345	L1 0.574	G-Mean 0.561
Predicted  [3.241241e-16, 0.5480247, 0.76105213, 0.9536431, 0.79424924, 0.6742832, 0.783449, 0.5797452, 0.5122709]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #92: Train loss [0.1841]; Val loss: MSE [0.1696], L1 [0.3355], G-Mean [0.2186]
 * Overall: MSE 0.179	L1 0.362	G-Mean 0.299
 * Many: MSE 0.088	L1 0.249	G-Mean 0.216
 * Low: MSE 0.360	L1 0.587	G-Mean 0.573
Predicted  [7.2306485e-15, 0.5556246, 0.7400508, 0.9517995, 0.8546564, 0.6890403, 0.63379204, 0.66600674, 0.55748385]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #93: Train loss [0.1984]; Val loss: MSE [0.1786], L1 [0.3616], G-Mean [0.2989]
 * Overall: MSE 0.195	L1 0.366	G-Mean 0.247
 * Many: MSE 0.086	L1 0.231	G-Mean 0.155
 * Low: MSE 0.412	L1 0.637	G-Mean 0.631
Predicted  [7.885915e-16, 0.6807526, 0.74981946, 0.89850676, 0.86073756, 0.72014886, 0.6655516, 0.7925911, 0.48150226]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #94: Train loss [0.1842]; Val loss: MSE [0.1946], L1 [0.3660], G-Mean [0.2472]
 * Overall: MSE 0.171	L1 0.356	G-Mean 0.291
 * Many: MSE 0.099	L1 0.270	G-Mean 0.225
 * Low: MSE 0.316	L1 0.529	G-Mean 0.487
Predicted  [5.1475566e-13, 0.6239305, 0.7448607, 0.81304324, 0.8623401, 0.7735141, 0.5380967, 0.8075066, 0.7329233]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #95: Train loss [0.1871]; Val loss: MSE [0.1715], L1 [0.3563], G-Mean [0.2912]
 * Overall: MSE 0.172	L1 0.352	G-Mean 0.282
 * Many: MSE 0.101	L1 0.271	G-Mean 0.224
 * Low: MSE 0.314	L1 0.515	G-Mean 0.451
Predicted  [3.1598966e-15, 0.5860462, 0.81539416, 0.8274362, 0.7905265, 0.75541496, 0.54944605, 0.8112747, 0.7949278]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #96: Train loss [0.1591]; Val loss: MSE [0.1723], L1 [0.3522], G-Mean [0.2824]
 * Overall: MSE 0.214	L1 0.384	G-Mean 0.288
 * Many: MSE 0.086	L1 0.235	G-Mean 0.187
 * Low: MSE 0.469	L1 0.682	G-Mean 0.679
Predicted  [9.4539635e-21, 0.597712, 0.8024245, 0.7960274, 0.8305966, 0.6634646, 0.69073135, 0.8461741, 0.2959205]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #97: Train loss [0.1476]; Val loss: MSE [0.2139], L1 [0.3838], G-Mean [0.2878]
 * Overall: MSE 0.195	L1 0.380	G-Mean 0.295
 * Many: MSE 0.103	L1 0.270	G-Mean 0.210
 * Low: MSE 0.378	L1 0.599	G-Mean 0.581
Predicted  [4.4006445e-14, 0.5411457, 0.82343733, 0.94608027, 0.7491565, 0.6942176, 0.50489783, 0.7383812, 0.58776253]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #98: Train loss [0.2010]; Val loss: MSE [0.1946], L1 [0.3797], G-Mean [0.2953]
 * Overall: MSE 0.255	L1 0.396	G-Mean 0.243
 * Many: MSE 0.079	L1 0.206	G-Mean 0.137
 * Low: MSE 0.605	L1 0.774	G-Mean 0.771
Predicted  [6.676286e-18, 0.6600486, 0.7645988, 0.8840377, 0.79386395, 0.56141245, 0.62279844, 0.87134093, 0.13198087]  for true improvements  [0.6, 0.7, 0.5, 0.2, 0.7, 0.5, 0.8, 0.1, 1.0]
Best L1 Loss: 0.231
Epoch #99: Train loss [0.2014]; Val loss: MSE [0.2547], L1 [0.3956], G-Mean [0.2431]
========================================================================================================================
Test best model on testset...
Loaded best model, epoch 34, best val loss 0.2306
 * Overall: MSE 0.113	L1 0.236	G-Mean 0.156
 * Many: MSE 0.019	L1 0.120	G-Mean 0.107
 * Low: MSE 0.490	L1 0.700	G-Mean 0.700
Predicted  [0.7024469, 1.0, 0.8810644, 0.77276355, 0.46947435]  for true improvements  [0.8, 0.3, 0.8, 0.7, 0.7]
Test loss: MSE [0.1129], L1 [0.2364], G-Mean [0.1562]
Done
