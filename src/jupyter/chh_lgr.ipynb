{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary data notebook for\n",
    "# NIH: Imaging Guided Intervention Surgery Study Section\n",
    "\n",
    "# Exploratory aim: evaluate presurgical scans between STN and GPi targets\n",
    "#   Given retrospective GPi acquisitions?\n",
    "#   Search for radiomic differentiators for STN versus GPi selection in presurgical scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import nibabel as nib\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score\n",
    "import SimpleITK as sitk\n",
    "import six\n",
    "from radiomics import featureextractor \n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "def remove_keymap_conflicts(new_keys_set):\n",
    "    for prop in plt.rcParams:\n",
    "        if prop.startswith('keymap.'):\n",
    "            keys = plt.rcParams[prop]\n",
    "            remove_list = set(keys) & new_keys_set\n",
    "            for key in remove_list:\n",
    "                keys.remove(key)\n",
    "\n",
    "def multi_slice_viewer(volume):\n",
    "    remove_keymap_conflicts({'j', 'k'})\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.volume = volume\n",
    "    ax.index = volume.shape[0]//2\n",
    "    ax.imshow(volume[ax.index])\n",
    "    fig.canvas.mpl_connect('key_press_event', process_key)\n",
    "\n",
    "def process_key(event):\n",
    "    fig = event.canvas.figure\n",
    "    ax = fig.axes[0]\n",
    "    if event.key == 'j':\n",
    "        previous_slice(ax)\n",
    "    elif event.key == 'k':\n",
    "        next_slice(ax)\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "\n",
    "def previous_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index-1) % volume.shape[0] \n",
    "    ax.images[0].set_array(volume[ax.index])\n",
    "\n",
    "def next_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index+1) % volume.shape[0]\n",
    "    ax.images[0].set_array(volume[ax.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending arrays with segmentation 01_roi_combined.nii QSM, 00001_qsm.nii.gz LARO, QSM_lr_01.nii and mask mask01.nii\n",
      "Appending arrays with segmentation 02_roi_combined.nii QSM, 00002_qsm.nii.gz LARO, QSM_lr_02.nii and mask mask02.nii\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18242/2090303035.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0md_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_count\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mqsms_wl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqsms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mlaros_wl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlaros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0msegs_wl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set window level\n",
    "level = 0\n",
    "window = 500\n",
    "m1=level-window/2\n",
    "m2=level+window/2\n",
    "visualize = 1\n",
    "# Load data\n",
    "nrows = 256\n",
    "ncols = 256\n",
    "nslices = 160\n",
    "segs = []\n",
    "qsms = []\n",
    "laros = []\n",
    "voxel_sizes = []\n",
    "trackers = []\n",
    "q_directory = '/media/mts_dbs/ChanghaiDBS_QSM/nii/qsm/'\n",
    "q_directory = os.listdir(q_directory)\n",
    "q_directory = sorted(q_directory)\n",
    "qu_directory = '/media/mts_dbs/ChanghaiDBS_QSM/nii/laro/'\n",
    "qu_directory = os.listdir(qu_directory)\n",
    "qu_directory = sorted(qu_directory)\n",
    "s_directory = '/media/mts_dbs/ChanghaiDBS_QSM/nii/roi/'\n",
    "s_directory = os.listdir(s_directory)\n",
    "s_directory = sorted(s_directory)\n",
    "m_directory = '/media/mts_dbs/ChanghaiDBS_QSM/nii/mask/'\n",
    "m_directory = os.listdir(m_directory)\n",
    "m_directory = sorted(m_directory)\n",
    "case_list = []\n",
    "d_count = 0\n",
    "if visualize == 1:\n",
    "    for filename in q_directory:\n",
    "    \n",
    "        seg_filename = s_directory[d_count]\n",
    "        laro_filename = qu_directory[d_count]\n",
    "        mask_filename = m_directory[d_count]\n",
    "        seg = nib.load('/media/mts_dbs/ChanghaiDBS_QSM/nii/roi/'+seg_filename)\n",
    "        mask = nib.load('/media/mts_dbs/ChanghaiDBS_QSM/nii/mask/'+mask_filename)\n",
    "        voxel_size = seg.header['pixdim'][0:3]\n",
    "        voxel_sizes.append(voxel_size)\n",
    "        segs.append(seg.get_fdata()[:nrows,:ncols,:nslices])\n",
    "        qsm = nib.load('/media/mts_dbs/ChanghaiDBS_QSM/nii/qsm/'+filename)\n",
    "        qsms.append(qsm.get_fdata()[:nrows,:ncols,:nslices])\n",
    "\n",
    "        laro = nib.load('/media/mts_dbs/ChanghaiDBS_QSM/nii/laro/'+laro_filename)\n",
    "        laros.append(1000*laro.get_fdata()[:nrows,:ncols,:nslices])\n",
    "        print('Appending arrays with segmentation',seg_filename,'QSM,',filename,\n",
    "              'LARO,',laro_filename,'and mask',mask_filename)\n",
    "        case_list.append(filename)\n",
    "        n_cases = len(segs)\n",
    "        d_count = d_count+1\n",
    "\n",
    "        qsms_wl = np.asarray(qsms)\n",
    "        laros_wl = np.asarray(laros)\n",
    "        segs_wl = np.asarray(segs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize == 1:\n",
    "    qsms_wl[qsms_wl < m1] = m1\n",
    "    qsms_wl[qsms_wl > m2] = m2\n",
    "    laros_wl[laros_wl < m1] = m1\n",
    "    laros_wl[laros_wl > m2] = m2\n",
    "    multi_slice_viewer(np.hstack(((np.vstack(qsms_wl[:n_cases//2,:,:,:]/1000+0*segs_wl[:n_cases//2,:,:,:]).T),\n",
    "                                  (np.vstack(laros_wl[:n_cases//2,:,:,:]/1000+0*segs_wl[:n_cases//2,:,:,:]).T),\n",
    "                                  (np.vstack(qsms_wl[(n_cases-n_cases//2):,:,:,:]/1000+0*segs_wl[(n_cases-n_cases//2):,:,:,:]).T),\n",
    "                                  (np.vstack(laros_wl[(n_cases-n_cases//2):,:,:,:]/1000+0*segs_wl[(n_cases-n_cases//2):,:,:,:]).T))))\n",
    "                                    \n",
    "    label_min = np.partition(np.unique(seg.get_fdata().ravel()), 1)[1]\n",
    "    label_max = np.amax(seg.get_fdata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/media/mts_dbs/ChanghaiDBS_QSM/xlsx/updrs_iii_chh.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patient IDs\n",
    "subject_id = np.asarray(df[df.columns[0]])[1:]\n",
    "\n",
    "# Only extract ROI if it is present in all cases\n",
    "seg_labels_all = segs[0]\n",
    "case_number = np.zeros_like(np.asarray(s_directory))\n",
    "for i in range(n_cases):\n",
    "    case_number[i] = float(s_directory[i][:2])\n",
    "subject_id_corr = subject_id[np.in1d(subject_id,case_number)]\n",
    "for i in range(n_cases):\n",
    "    try:\n",
    "        print('Found ROIs',str(np.unique(segs[i])),'at segmentation directory file',s_directory[i],'for case',str(subject_id_corr[i]))\n",
    "    except:\n",
    "        print('Case',subject_id[i],'quarantined')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_updrs_iii_off =  np.asarray(df[df.columns[3]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])                                \n",
    "pre_updrs_iii_on =  np.asarray(df[df.columns[4]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])\n",
    "post_updrs_iii_off =  np.asarray(df[df.columns[6]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])\n",
    "\n",
    "per_change = (np.asarray(pre_updrs_iii_off).astype(float)-np.asarray(post_updrs_iii_off).astype(float))/(np.asarray(pre_updrs_iii_off).astype(float))\n",
    "lct_change = (np.asarray(pre_updrs_iii_off).astype(float)-(np.asarray(pre_updrs_iii_on)).astype(float))/(np.asarray(pre_updrs_iii_off).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"radiomics\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "reextract = 0\n",
    "\n",
    "# Assume all voxel sizes are identical\n",
    "voxel_size = (0.9,0.9,0.9)\n",
    "if reextract == 1:\n",
    "    # Generate feature structure Phi from all ROIs and all cases\n",
    "    extractor = featureextractor.RadiomicsFeatureExtractor()\n",
    "    extractor.enableAllFeatures()\n",
    "    extractor.enableAllImageTypes()\n",
    "    extractor.enableFeatureClassByName('shape2D',enabled = False)\n",
    "\n",
    "    seg_labels_all = np.unique(np.asarray(segs))\n",
    "    Phi_gt = []\n",
    "    Phi_vd = []\n",
    "    Phi_lr = []\n",
    "    seg_labels = []\n",
    "    reextract = 0\n",
    "    x_row_gt = []\n",
    "    x_row_lr = []\n",
    "\n",
    "    keylib = []\n",
    "    roilib = []\n",
    "    loop_count = 1\n",
    "    n_rois = seg_labels_all[seg_labels_all>0].__len__()\n",
    "    roi_names = np.asarray(['Background','Right substantia nigra','Right subthalamic nucleus',\n",
    "                            'Left subthalamic nucleus', 'Left substantia nigra', 'Right dentate nucleus', 'Left dentate nucleus'])\n",
    "    for i in np.arange(subject_id_corr.__len__()):\n",
    "        seg_sitk = sitk.GetImageFromArray(segs[i])\n",
    "        seg_sitk.SetSpacing(voxel_size)\n",
    "        qsm_sitk_gt = sitk.GetImageFromArray(qsms[i])\n",
    "        qsm_sitk_gt.SetSpacing(voxel_size)\n",
    "        qsm_sitk_lr = sitk.GetImageFromArray(laros[i])\n",
    "        qsm_sitk_lr.SetSpacing(voxel_size)\n",
    "        # Index back since subject 12 is missing ROIs\n",
    "        for j in seg_labels_all:\n",
    "            if j>0:\n",
    "                fv_count = 0\n",
    "                featureVector_gt = extractor.execute(qsm_sitk_gt,seg_sitk,label=int(j));\n",
    "                featureVector_lr = extractor.execute(qsm_sitk_lr,seg_sitk,label=int(j));\n",
    "                Phi_gt.append(featureVector_gt)\n",
    "                Phi_lr.append(featureVector_lr)\n",
    "                for key, value in six.iteritems(featureVector_gt):\n",
    "                    if 'diagnostic' in key:\n",
    "                        next\n",
    "                    else:\n",
    "                        x_row_gt.append(featureVector_gt[key])\n",
    "                        x_row_lr.append(featureVector_lr[key])\n",
    "                        fv_count = fv_count+1\n",
    "                        keylib.append(key)\n",
    "                        roilib.append(roi_names[int(j)])\n",
    "                x_row_gt.append(pre_updrs_iii_off[i])\n",
    "                x_row_lr.append(pre_updrs_iii_off[i])\n",
    "                fv_count = fv_count+1\n",
    "        print('Extracting features for subject',subject_id_corr[i],'and appending feature matrix with vector of length',fv_count,'with UPDRS score',pre_updrs_iii_off[i])\n",
    "                \n",
    "    X0_gt = np.array(x_row_gt)\n",
    "    X0_lr = np.array(x_row_lr)\n",
    "    np.save('./npy/X0_gt_chh_rois.npy',X0_gt)\n",
    "    np.save('./npy/X0_lr_chh_rois.npy',X0_lr)\n",
    "\n",
    "    K = np.asarray(keylib)\n",
    "    R = np.asarray(roi_names)\n",
    "    np.save('./npy/K_chh.npy',K)\n",
    "    np.save('./npy/R_chh.npy',R)\n",
    "\n",
    "    print('Saving ground truth feature vector')\n",
    "    with open('./phi/Phi_mcl_gt_roi_chh', 'wb') as fp:  \n",
    "        pickle.dump(Phi_gt, fp)\n",
    "    \n",
    "    print('Saving undersampled feature vector')\n",
    "    with open('./phi/Phi_mcl_lr_roi_chh', 'wb') as fp:  \n",
    "        pickle.dump(Phi_lr, fp)\n",
    "\n",
    "else:\n",
    "    X0_gt = np.load('./npy/X0_gt_chh_rois.npy')\n",
    "    X0_lr = np.load('./npy/X0_lr_chh_rois.npy')\n",
    "    K = np.load('./npy/K_chh.npy')\n",
    "    R = np.load('./npy/R_chh.npy')\n",
    "    n_rois = R.shape[0]-1\n",
    "    with open('./phi/Phi_mcl_gt_roi_chh', \"rb\") as fp:  \n",
    "        Phi_gt = pickle.load(fp)\n",
    "    \n",
    "    with open('./phi/Phi_mcl_lr_roi_chh', \"rb\") as fp:  \n",
    "        Phi_lr = pickle.load(fp)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cases = len(per_change)\n",
    "n_roisc = Phi_gt.__len__()/n_cases\n",
    "L = int(len(X0_gt)/n_cases)\n",
    "n_features = int(L/n_rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_row_gt = X0_gt.tolist()\n",
    "X = np.zeros((n_cases,n_rois,n_features)).transpose((0,2,1))\n",
    "X = X0_gt.reshape((n_cases,n_rois,n_features)).transpose((0,2,1))\n",
    "X_lr = X0_lr.reshape((n_cases,n_rois,n_features)).transpose((0,2,1))\n",
    "ut_ls = np.zeros((subject_id_corr.__len__()))\n",
    "ut_qr = np.zeros((subject_id_corr.__len__()))\n",
    "ut_qrlr = np.zeros((subject_id_corr.__len__()))\n",
    "ut_lr = np.zeros((subject_id_corr.__len__()))\n",
    "ut_en = np.zeros((subject_id_corr.__len__()))\n",
    "# Normalize testing and training cases together\n",
    "#   Set with_mean=False to preserve data sparsity\n",
    "#   And with_std=False \n",
    "#   However, need a significant number of samples to do this\n",
    "X_all = X.reshape(n_cases,((n_features)*n_rois))\n",
    "X_all_lr = X_lr.reshape(n_cases,((n_features)*n_rois))\n",
    "scaler = StandardScaler()\n",
    "X_all_t = scaler.fit_transform(X_all)\n",
    "X_all_t_lr = scaler.fit_transform(X_all_lr)\n",
    "C = np.zeros_like(X_all_t)\n",
    "for j in np.arange(len(subject_id_corr)):\n",
    "        # Add UPDRS after scaling\n",
    "        # Might be overwriting a feature here\n",
    "        X_all_t[j,-1] = pre_updrs_iii_off[int(j)]\n",
    "        X_in = X_all_t\n",
    "        X_in = np.delete(X_in,j,axis=0)\n",
    "        p_per_change = per_change\n",
    "        per_change_in = np.delete(p_per_change,j,axis=0)\n",
    "        # Cross-validation for model selection\n",
    "        # Identify most important features\n",
    "        per_change_in_binary = per_change_in<0.3\n",
    "        clf_ls = LogisticRegression(C=10).fit(X_in,per_change_in_binary)\n",
    "        print('Fit complete')\n",
    "        \n",
    "        ut_ls[j] = clf_ls.predict(X_all_t[j,:].reshape(1, -1))\n",
    "        C[j] = clf_ls.coef_\n",
    "        print('Patient ID',str(subject_id_corr[j]),'with pre-surgical UPDRS score',str(pre_updrs_iii_off[int(j)]),'at feature matrix row',str(j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(per_change<0.3).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# plt.rcParams[\"figure.figsize\"] = (25,5)\n",
    "# # Cross validation results\n",
    "# [fig,ax] = plt.subplots(1,2,sharex=True, sharey=True)\n",
    "# lr_prepost = linregress(lct_change,per_change)\n",
    "# ax[0].scatter(lct_change,per_change,)\n",
    "# ax[0].plot(lct_change,lct_change*lr_prepost.slope+lr_prepost.intercept,'-r')\n",
    "# ax[0].set_title('LCT')\n",
    "# ax[0].set_ylabel(\"DBS improvement\")\n",
    "# ax[0].set_xlabel(\"Prediction\")\n",
    "# ax[0].set_ylim([0, 2])\n",
    "# text = f\"$y={lr_prepost.slope:0.3f}\\;x{lr_prepost.intercept:+0.3f}$\\n$r = {lr_prepost.rvalue:0.3f}$\\n$p = {lr_prepost.pvalue:0.3f}$\"\n",
    "# ax[0].text(0.05, 0.95, text,transform=ax[0].transAxes,\n",
    "#      fontsize=14, verticalalignment='top')\n",
    "# ax[0].hlines(0.4,0,1,linestyle='dashed',color='white')\n",
    "# ax[0].vlines(0.4,0,2,linestyle='dashed',color='white')\n",
    "\n",
    "# lr_pred_ls = linregress(ut_ls,per_change)\n",
    "# ax[1].scatter(ut_ls,per_change)\n",
    "# ax[1].plot(ut_ls,ut_ls*lr_pred_ls.slope+lr_pred_ls.intercept,'-r')\n",
    "# ax[1].set_title('Fully-sampled LASSO')\n",
    "# ax[1].set_ylabel(\"DBS improvement\")\n",
    "# ax[1].set_xlabel(\"Prediction\")\n",
    "# text = f\"$y={lr_pred_ls.slope:0.3f}\\;x{lr_pred_ls.intercept:+0.3f}$\\n$r = {lr_pred_ls.rvalue:0.3f}$\\n$p = {lr_pred_ls.pvalue:0.3f}$\"\n",
    "# ax[1].text(0.05, 0.95, text,transform=ax[1].transAxes,\n",
    "#      fontsize=14, verticalalignment='top')\n",
    "# ax[1].hlines(0.4,0,1,linestyle='dashed',color='white')\n",
    "# ax[1].vlines(0.4,0,2,linestyle='dashed',color='white')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfs = []\n",
    "for j in np.arange(subject_id_corr.__len__()):\n",
    "    Kr =  K.reshape((n_cases,n_rois,(n_features-1))).transpose((0,2,1))\n",
    "    Kr_extended = np.zeros((n_cases,n_rois,n_features)).transpose((0,2,1)).astype('str')\n",
    "    Kr_extended[:,0:n_features-1,:] = Kr\n",
    "    Kr_extended[:,-1,:] = 'po_updrs'\n",
    "    rfs.append(Kr_extended[j,np.asarray(C[j]!=0).reshape((n_rois,n_features)).transpose((1,0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from collections import Counter\n",
    "R = [item for sublist in rfs for item in sublist]\n",
    "letter_counts = Counter(R)\n",
    "df = pandas.DataFrame.from_dict(letter_counts, orient='index')\n",
    "df.sort_values('number', inplace=True)\n",
    "df.plot(y='number', kind='bar', legend=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14732b5bb7ad6abfe54a083b8d194ae3941adfb1b18321b588b21cb8f420fced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
