{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from attrdict import AttrDict\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gmean\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from imbalanced_regression.qsm.resnet import resnet50\n",
    "from imbalanced_regression.qsm import loss\n",
    "from imbalanced_regression.qsm.datasets import QSM\n",
    "from imbalanced_regression.utils import *\n",
    "import util\n",
    "from util import pyvis\n",
    "import sklearn.model_selection as skm\n",
    "import os\n",
    "import nibabel as nib\n",
    "from IPython.display import HTML\n",
    "import datetime\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\"\n",
    "HTML('''\n",
    "<style>\n",
    ".jupyter-matplotlib {\n",
    "    background-color: #000;\n",
    "}\n",
    "\n",
    ".widget-label, .jupyter-matplotlib-header{\n",
    "    color: #fff;\n",
    "}\n",
    "\n",
    ".jupyter-button {\n",
    "    background-color: #333;\n",
    "    color: #fff;\n",
    "}\n",
    "</style>\n",
    "''')\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get case IDs\n",
    "case_list = open('/home/ali/RadDBS-QSM/data/docs/cases_90','r')\n",
    "lines = case_list.read()\n",
    "lists = np.loadtxt(case_list.name,comments=\"#\", delimiter=\",\",unpack=False,dtype=str)\n",
    "case_id = []\n",
    "for lines in lists:     \n",
    "    case_id.append(lines[-9:-7])\n",
    "\n",
    "# Load scores\n",
    "file_dir = '/home/ali/RadDBS-QSM/data/docs/QSM anonymus- 6.22.2023-1528.csv'\n",
    "motor_df = util.filter_scores(file_dir,'pre-dbs updrs','stim','CORNELL ID')\n",
    "# Find cases with all required scores\n",
    "subs,pre_imp,post_imp,pre_updrs_off = util.get_full_cases(motor_df,\n",
    "                                                          'CORNELL ID',\n",
    "                                                          'OFF (pre-dbs updrs)',\n",
    "                                                          'ON (pre-dbs updrs)',\n",
    "                                                          'OFF meds ON stim 6mo')\n",
    "# Find overlap between scored subjects and nii\n",
    "ids = np.asarray(case_id).astype(int)\n",
    "ids = ids[ids != 54]\n",
    "cases_idx = np.in1d(subs,ids)\n",
    "ccases = subs[cases_idx]\n",
    "per_change = np.round(post_imp[cases_idx],1)\n",
    "\n",
    "nii_paths = []\n",
    "seg_nii_paths = []\n",
    "qsm_dir = '/home/ali/RadDBS-QSM/data/nii/qsm/'\n",
    "seg_dir = '/home/ali/RadDBS-QSM/data/nii/seg/'\n",
    "qsm_niis = sorted(os.listdir(qsm_dir))\n",
    "seg_niis = sorted(os.listdir(seg_dir))\n",
    "for k in np.arange(len(ccases)):\n",
    "    for file in qsm_niis:\n",
    "        if int(ccases[k]) == int(file[18:20]):\n",
    "            nii_paths.append(qsm_dir+file)\n",
    "            seg_nii_paths.append(seg_dir+'labels_2iMag'+file[18:20]+'.nii.gz')\n",
    "\n",
    "train_dir, test_dir, train_seg, test_seg, y_train, y_test = skm.train_test_split(nii_paths, seg_nii_paths, per_change, test_size=0.1, random_state=1)\n",
    "train_dir, val_dir, train_seg, val_seg, y_train, y_val = skm.train_test_split(train_dir, train_seg, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = AttrDict()\n",
    "args.gpu = 0\n",
    "args.optimizer = 'sgd'\n",
    "args.lr = 1e-3\n",
    "args.epoch = 100\n",
    "args.momentum = 0.9\n",
    "args.weight_decay = 1e-4\n",
    "args.schedule = [60,80]\n",
    "args.print_freq = 10\n",
    "args.resume = ''\n",
    "args.pretrained = False\n",
    "args.evaluate = False\n",
    "args.loss = 'l1'\n",
    "args.dataset = 'qsm'\n",
    "args.model = 'resnet50'\n",
    "args.store_root = 'checkpoint'\n",
    "args.store_name = ''\n",
    "args.data_dir = '/home/ali/RadDBS-QSM/data/qsm/'\n",
    "args.fds = True\n",
    "args.fds_kernel = 'gaussian'\n",
    "args.fds_ks = 3\n",
    "args.fds_sigma = 1\n",
    "args.fds_mmt = 0.9\n",
    "args.start_update = 0\n",
    "args.start_smooth = 1\n",
    "args.bucket_num = 5\n",
    "args.bucket_start = 3\n",
    "args.start_epoch = 0\n",
    "args.best_loss = 1e5\n",
    "args.reweight = 'sqrt_inv'\n",
    "args.retrain_fc = False\n",
    "args.lds = True\n",
    "args.lds_kernel = 'gaussian'\n",
    "args.lds_ks = 3\n",
    "args.lds_sigma = 1\n",
    "args.batch_size = 3\n",
    "args.workers = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: 0 for training\n",
      "=====> Preparing data...\n",
      "Using re-weighting: [SQRT_INV]\n",
      "Using LDS: [GAUSSIAN] (3/1)\n",
      "Training data size: 33\n",
      "Validation data size: 9\n",
      "Test data size: 5\n",
      "=====> Building model...\n",
      "Create Epoch [0] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 4968.701\tL1 44.869\tG-Mean 18.671\n",
      " * Many: MSE 1010.028\tL1 27.926\tG-Mean 20.966\n",
      " * Median: MSE 14136.013\tL1 86.504\tG-Mean 28.808\n",
      " * Low: MSE 4135.390\tL1 39.702\tG-Mean 11.980\n",
      "Best L1 Loss: 44.869\n",
      "Epoch #0: Train loss [6.8003]; Val loss: MSE [4968.7007], L1 [44.8690], G-Mean [18.6706]\n",
      "Create Epoch [1] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 18937.423\tL1 71.347\tG-Mean 28.513\n",
      " * Many: MSE 324.871\tL1 17.533\tG-Mean 16.968\n",
      " * Median: MSE 74152.547\tL1 196.959\tG-Mean 58.595\n",
      " * Low: MSE 6944.077\tL1 59.357\tG-Mean 35.242\n",
      "Best L1 Loss: 44.869\n",
      "Epoch #1: Train loss [7.3490]; Val loss: MSE [18937.4233], L1 [71.3470], G-Mean [28.5131]\n",
      "Create Epoch [2] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 893.926\tL1 19.045\tG-Mean 12.769\n",
      " * Many: MSE 85.963\tL1 9.197\tG-Mean 9.115\n",
      " * Median: MSE 3421.193\tL1 44.013\tG-Mean 21.288\n",
      " * Low: MSE 286.365\tL1 15.530\tG-Mean 14.235\n",
      "Best L1 Loss: 19.045\n",
      "Epoch #2: Train loss [7.8247]; Val loss: MSE [893.9258], L1 [19.0447], G-Mean [12.7688]\n",
      "Create Epoch [3] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 46.672\tL1 5.200\tG-Mean 2.823\n",
      " * Many: MSE 18.934\tL1 3.979\tG-Mean 3.489\n",
      " * Median: MSE 116.134\tL1 9.258\tG-Mean 7.437\n",
      " * Low: MSE 37.349\tL1 4.123\tG-Mean 1.115\n",
      "Best L1 Loss: 5.200\n",
      "Epoch #3: Train loss [7.5743]; Val loss: MSE [46.6721], L1 [5.2003], G-Mean [2.8227]\n",
      "Create Epoch [4] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 93.000\tL1 6.141\tG-Mean 3.663\n",
      " * Many: MSE 7.183\tL1 2.618\tG-Mean 2.544\n",
      " * Median: MSE 316.635\tL1 14.352\tG-Mean 9.765\n",
      " * Low: MSE 58.332\tL1 5.363\tG-Mean 3.098\n",
      "Best L1 Loss: 5.200\n",
      "Epoch #4: Train loss [6.5539]; Val loss: MSE [92.9998], L1 [6.1407], G-Mean [3.6631]\n",
      "Create Epoch [5] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 38.172\tL1 5.819\tG-Mean 5.355\n",
      " * Many: MSE 27.237\tL1 4.750\tG-Mean 4.229\n",
      " * Median: MSE 51.622\tL1 7.110\tG-Mean 7.035\n",
      " * Low: MSE 43.783\tL1 6.384\tG-Mean 6.118\n",
      "Best L1 Loss: 5.200\n",
      "Epoch #5: Train loss [5.1647]; Val loss: MSE [38.1715], L1 [5.8190], G-Mean [5.3555]\n",
      "Create Epoch [6] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:22<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 218.504\tL1 13.456\tG-Mean 12.610\n",
      " * Many: MSE 120.928\tL1 10.995\tG-Mean 10.994\n",
      " * Median: MSE 530.343\tL1 20.816\tG-Mean 18.338\n",
      " * Low: MSE 140.712\tL1 11.829\tG-Mean 11.796\n",
      "Best L1 Loss: 5.200\n",
      "Epoch #6: Train loss [10.4231]; Val loss: MSE [218.5039], L1 [13.4555], G-Mean [12.6102]\n",
      "Create Epoch [7] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:20<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 106.730\tL1 7.535\tG-Mean 5.487\n",
      " * Many: MSE 22.919\tL1 4.684\tG-Mean 4.557\n",
      " * Median: MSE 330.372\tL1 15.086\tG-Mean 11.170\n",
      " * Low: MSE 69.383\tL1 6.301\tG-Mean 4.376\n",
      "Best L1 Loss: 5.200\n",
      "Epoch #7: Train loss [5.8402]; Val loss: MSE [106.7302], L1 [7.5347], G-Mean [5.4871]\n",
      "Create Epoch [8] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:20<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 272.605\tL1 11.466\tG-Mean 8.299\n",
      " * Many: MSE 34.814\tL1 5.842\tG-Mean 5.782\n",
      " * Median: MSE 932.703\tL1 23.979\tG-Mean 14.741\n",
      " * Low: MSE 149.595\tL1 10.623\tG-Mean 9.162\n",
      "Best L1 Loss: 5.200\n",
      "Epoch #8: Train loss [8.1046]; Val loss: MSE [272.6052], L1 [11.4660], G-Mean [8.2994]\n",
      "Create Epoch [9] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:20<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 445.907\tL1 17.270\tG-Mean 14.984\n",
      " * Many: MSE 133.253\tL1 11.524\tG-Mean 11.505\n",
      " * Median: MSE 1387.951\tL1 32.111\tG-Mean 25.966\n",
      " * Low: MSE 234.751\tL1 15.039\tG-Mean 14.774\n",
      "Best L1 Loss: 5.200\n",
      "Epoch #9: Train loss [5.6478]; Val loss: MSE [445.9075], L1 [17.2703], G-Mean [14.9845]\n",
      "Create Epoch [10] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 82.921\tL1 5.038\tG-Mean 2.402\n",
      " * Many: MSE 4.425\tL1 1.860\tG-Mean 1.458\n",
      " * Median: MSE 334.512\tL1 13.292\tG-Mean 4.341\n",
      " * Low: MSE 19.856\tL1 3.774\tG-Mean 3.152\n",
      "Best L1 Loss: 5.038\n",
      "Epoch #10: Train loss [6.9106]; Val loss: MSE [82.9213], L1 [5.0382], G-Mean [2.4023]\n",
      "Create Epoch [11] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:22<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 278.760\tL1 8.381\tG-Mean 2.027\n",
      " * Many: MSE 7.364\tL1 2.223\tG-Mean 1.334\n",
      " * Median: MSE 1073.347\tL1 23.602\tG-Mean 6.381\n",
      " * Low: MSE 110.896\tL1 6.444\tG-Mean 1.647\n",
      "Best L1 Loss: 5.038\n",
      "Epoch #11: Train loss [1.5824]; Val loss: MSE [278.7597], L1 [8.3809], G-Mean [2.0267]\n",
      "Create Epoch [12] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 197.337\tL1 8.829\tG-Mean 5.992\n",
      " * Many: MSE 18.761\tL1 4.298\tG-Mean 4.260\n",
      " * Median: MSE 780.874\tL1 21.974\tG-Mean 13.597\n",
      " * Low: MSE 46.415\tL1 6.106\tG-Mean 5.469\n",
      "Best L1 Loss: 5.038\n",
      "Epoch #12: Train loss [3.1746]; Val loss: MSE [197.3373], L1 [8.8286], G-Mean [5.9922]\n",
      "Create Epoch [13] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 241.055\tL1 7.721\tG-Mean 2.632\n",
      " * Many: MSE 5.365\tL1 2.203\tG-Mean 2.048\n",
      " * Median: MSE 982.139\tL1 22.335\tG-Mean 3.946\n",
      " * Low: MSE 61.253\tL1 5.335\tG-Mean 2.806\n",
      "Best L1 Loss: 5.038\n",
      "Epoch #13: Train loss [2.3388]; Val loss: MSE [241.0550], L1 [7.7206], G-Mean [2.6317]\n",
      "Create Epoch [14] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 198.923\tL1 7.010\tG-Mean 2.781\n",
      " * Many: MSE 3.211\tL1 1.772\tG-Mean 1.749\n",
      " * Median: MSE 823.628\tL1 21.250\tG-Mean 8.915\n",
      " * Low: MSE 43.401\tL1 4.500\tG-Mean 2.374\n",
      "Best L1 Loss: 5.038\n",
      "Epoch #14: Train loss [1.6494]; Val loss: MSE [198.9225], L1 [7.0097], G-Mean [2.7809]\n",
      "Create Epoch [15] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:20<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 68.087\tL1 4.730\tG-Mean 2.755\n",
      " * Many: MSE 5.130\tL1 2.175\tG-Mean 2.078\n",
      " * Median: MSE 277.551\tL1 12.616\tG-Mean 6.384\n",
      " * Low: MSE 12.387\tL1 2.879\tG-Mean 2.290\n",
      "Best L1 Loss: 4.730\n",
      "Epoch #15: Train loss [2.2804]; Val loss: MSE [68.0870], L1 [4.7299], G-Mean [2.7546]\n",
      "Create Epoch [16] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 153.620\tL1 6.039\tG-Mean 2.048\n",
      " * Many: MSE 1.735\tL1 1.286\tG-Mean 1.246\n",
      " * Median: MSE 623.530\tL1 18.122\tG-Mean 5.771\n",
      " * Low: MSE 42.860\tL1 4.321\tG-Mean 1.990\n",
      "Best L1 Loss: 4.730\n",
      "Epoch #16: Train loss [4.1028]; Val loss: MSE [153.6201], L1 [6.0389], G-Mean [2.0475]\n",
      "Create Epoch [17] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 187.228\tL1 7.370\tG-Mean 3.846\n",
      " * Many: MSE 8.304\tL1 2.810\tG-Mean 2.731\n",
      " * Median: MSE 776.912\tL1 20.436\tG-Mean 7.638\n",
      " * Low: MSE 32.670\tL1 4.739\tG-Mean 3.843\n",
      "Best L1 Loss: 4.730\n",
      "Epoch #17: Train loss [2.8353]; Val loss: MSE [187.2278], L1 [7.3698], G-Mean [3.8461]\n",
      "Create Epoch [18] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 1143.402\tL1 16.318\tG-Mean 5.101\n",
      " * Many: MSE 20.547\tL1 4.120\tG-Mean 3.571\n",
      " * Median: MSE 4712.236\tL1 49.114\tG-Mean 10.594\n",
      " * Low: MSE 261.320\tL1 10.718\tG-Mean 5.041\n",
      "Best L1 Loss: 4.730\n",
      "Epoch #18: Train loss [2.5921]; Val loss: MSE [1143.4022], L1 [16.3181], G-Mean [5.1012]\n",
      "Create Epoch [19] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 91.213\tL1 4.364\tG-Mean 1.703\n",
      " * Many: MSE 3.141\tL1 1.618\tG-Mean 1.403\n",
      " * Median: MSE 401.362\tL1 14.453\tG-Mean 4.054\n",
      " * Low: MSE 1.874\tL1 1.299\tG-Mean 1.236\n",
      "Best L1 Loss: 4.364\n",
      "Epoch #19: Train loss [2.5562]; Val loss: MSE [91.2125], L1 [4.3642], G-Mean [1.7026]\n",
      "Create Epoch [20] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:20<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 981.043\tL1 14.959\tG-Mean 4.957\n",
      " * Many: MSE 8.030\tL1 2.822\tG-Mean 2.810\n",
      " * Median: MSE 4043.959\tL1 46.367\tG-Mean 15.994\n",
      " * Low: MSE 236.450\tL1 10.204\tG-Mean 4.840\n",
      "Best L1 Loss: 4.364\n",
      "Epoch #20: Train loss [1.2463]; Val loss: MSE [981.0430], L1 [14.9593], G-Mean [4.9574]\n",
      "Create Epoch [21] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 6662.785\tL1 38.616\tG-Mean 10.270\n",
      " * Many: MSE 52.421\tL1 6.693\tG-Mean 6.092\n",
      " * Median: MSE 26820.931\tL1 117.374\tG-Mean 27.064\n",
      " * Low: MSE 2037.840\tL1 28.674\tG-Mean 10.800\n",
      "Best L1 Loss: 4.364\n",
      "Epoch #21: Train loss [3.0653]; Val loss: MSE [6662.7852], L1 [38.6159], G-Mean [10.2698]\n",
      "Create Epoch [22] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 3.081\tL1 1.283\tG-Mean 0.640\n",
      " * Many: MSE 2.154\tL1 1.261\tG-Mean 0.996\n",
      " * Median: MSE 1.572\tL1 1.168\tG-Mean 1.074\n",
      " * Low: MSE 5.322\tL1 1.389\tG-Mean 0.252\n",
      "Best L1 Loss: 1.283\n",
      "Epoch #22: Train loss [2.5880]; Val loss: MSE [3.0809], L1 [1.2830], G-Mean [0.6404]\n",
      "Create Epoch [23] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 1817.834\tL1 19.995\tG-Mean 5.089\n",
      " * Many: MSE 13.605\tL1 3.422\tG-Mean 3.127\n",
      " * Median: MSE 7358.879\tL1 61.697\tG-Mean 15.942\n",
      " * Low: MSE 529.443\tL1 14.291\tG-Mean 4.551\n",
      "Best L1 Loss: 1.283\n",
      "Epoch #23: Train loss [1.8533]; Val loss: MSE [1817.8341], L1 [19.9950], G-Mean [5.0894]\n",
      "Create Epoch [24] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 1755.614\tL1 18.833\tG-Mean 4.345\n",
      " * Many: MSE 4.063\tL1 2.010\tG-Mean 2.003\n",
      " * Median: MSE 7286.429\tL1 61.211\tG-Mean 14.395\n",
      " * Low: MSE 403.805\tL1 13.013\tG-Mean 5.490\n",
      "Best L1 Loss: 1.283\n",
      "Epoch #24: Train loss [2.4110]; Val loss: MSE [1755.6139], L1 [18.8331], G-Mean [4.3449]\n",
      "Create Epoch [25] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 323.150\tL1 7.805\tG-Mean 1.327\n",
      " * Many: MSE 0.420\tL1 0.540\tG-Mean 0.405\n",
      " * Median: MSE 1369.017\tL1 26.406\tG-Mean 5.058\n",
      " * Low: MSE 56.211\tL1 5.091\tG-Mean 2.645\n",
      "Best L1 Loss: 1.283\n",
      "Epoch #25: Train loss [1.3221]; Val loss: MSE [323.1495], L1 [7.8051], G-Mean [1.3271]\n",
      "Create Epoch [26] features of all training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shot metrics for validation predictions of size  9  with validations labels of size  9  and training labels of size  33\n",
      " * Overall: MSE 46.074\tL1 3.500\tG-Mean 0.956\n",
      " * Many: MSE 0.757\tL1 0.684\tG-Mean 0.423\n",
      " * Median: MSE 163.308\tL1 9.272\tG-Mean 2.939\n",
      " * Low: MSE 28.340\tL1 3.406\tG-Mean 1.341\n",
      "Best L1 Loss: 1.283\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-920117a9c8ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-920117a9c8ce>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         }, is_best)\n\u001b[0m\u001b[1;32m    101\u001b[0m         print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n\u001b[1;32m    102\u001b[0m               f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
      "\u001b[0;32m~/anaconda3/envs/pdradenv/lib/python3.7/site-packages/imbalanced_regression/utils.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(args, state, is_best, prefix)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{args.store_root}/{args.store_name}/{prefix}ckpt.pth.tar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_best\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"===> Saving current best checkpoint...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pdradenv/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pdradenv/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    if args.gpu is not None:\n",
    "        print(f\"Use GPU: {args.gpu} for training\")\n",
    "\n",
    "    # Data\n",
    "    print('=====> Preparing data...')\n",
    "\n",
    "    train_dataset = QSM(data_dir=train_dir, mask_dir=train_seg, targets=y_train, nx=128, nz=128, split='train',\n",
    "                          reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "    val_dataset = QSM(data_dir=val_dir, mask_dir = val_seg, targets = y_val, nx=128, nz=128, split='val')\n",
    "    test_dataset = QSM(data_dir=test_dir, mask_dir = test_seg, targets = y_test, nx=128, nz=128, split='test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                              num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "    print(f\"Training data size: {len(train_dataset)}\")\n",
    "    print(f\"Validation data size: {len(val_dataset)}\")\n",
    "    print(f\"Test data size: {len(test_dataset)}\")\n",
    "\n",
    "    # Model\n",
    "    print('=====> Building model...')\n",
    "    model = resnet50(fds=args.fds, bucket_num=args.bucket_num, bucket_start=args.bucket_start,\n",
    "                     start_update=args.start_update, start_smooth=args.start_smooth,\n",
    "                     kernel=args.fds_kernel, ks=args.fds_ks, sigma=args.fds_sigma, momentum=args.fds_mmt)\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # evaluate only\n",
    "    if args.evaluate:\n",
    "        assert args.resume, 'Specify a trained model using [args.resume]'\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
    "        validate(test_loader, model, train_labels=y_test, prefix='Test')\n",
    "        return\n",
    "\n",
    "    if args.retrain_fc:\n",
    "        assert args.reweight != 'none' and args.pretrained\n",
    "        print('===> Retrain last regression layer only!')\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'fc' not in name and 'linear' not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Loss and optimizer\n",
    "    if not args.retrain_fc:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "            torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        # optimize only the last linear layer\n",
    "        parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "        names = list(filter(lambda k: k is not None, [k if v.requires_grad else None for k, v in model.module.named_parameters()]))\n",
    "        assert 1 <= len(parameters) <= 2  # fc.weight, fc.bias\n",
    "        print(f'===> Only optimize parameters: {names}')\n",
    "        optimizer = torch.optim.Adam(parameters, lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "            torch.optim.SGD(parameters, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.pretrained:\n",
    "        checkpoint = torch.load(args.pretrained, map_location=\"cpu\")\n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in checkpoint['state_dict'].items():\n",
    "            if 'linear' not in k and 'fc' not in k:\n",
    "                new_state_dict[k] = v\n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "        print(f'===> Pretrained weights found in total: [{len(list(new_state_dict.keys()))}]')\n",
    "        print(f'===> Pre-trained model loaded: {args.pretrained}')\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "            checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "                torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            args.best_loss = checkpoint['best_loss']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "        else:\n",
    "            print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "\n",
    "    globals()[f\"weighted_{args.loss}_loss\"] = loss.weighted_l1_loss\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=y_train)\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "\n",
    "    # test with best checkpoint\n",
    "    print(\"=\" * 120)\n",
    "    print(\"Test best model on testset...\")\n",
    "    args.store_name = str(str(epoch),'_',str(datetime.now()))\n",
    "    checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "    test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=y_test, prefix='Test')\n",
    "    print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs, targets, weights = \\\n",
    "            inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        if args.fds:\n",
    "            outputs, _ = model(inputs, targets, epoch)\n",
    "        else:\n",
    "            outputs = model(inputs, targets, epoch)\n",
    "\n",
    "        loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, torch.unsqueeze(targets,dim=1), weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx)\n",
    "\n",
    "    if args.fds and epoch >= args.start_update:\n",
    "        print(f\"Create Epoch [{epoch}] features of all training data...\")\n",
    "        encodings, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets, _) in tqdm(train_loader):\n",
    "                inputs = inputs.cuda(non_blocking=True)\n",
    "                outputs, feature = model(inputs, targets, epoch)\n",
    "                encodings.extend(feature.data.squeeze().cpu().numpy())\n",
    "                labels.extend(targets.data.squeeze().cpu().numpy())\n",
    "\n",
    "        encodings, labels = torch.from_numpy(np.vstack(encodings)).cuda(), torch.from_numpy(np.hstack(labels)).cuda()\n",
    "        model.module.FDS.update_last_epoch_stats(epoch)\n",
    "        model.module.FDS.update_running_stats(encodings, labels, epoch)\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "            loss_mse = criterion_mse(outputs, torch.unsqueeze(targets,dim=1))\n",
    "            loss_l1 = criterion_l1(outputs, torch.unsqueeze(targets,dim=1))\n",
    "            loss_all = criterion_gmean(outputs, torch.unsqueeze(targets,dim=1))\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "        # print('Validate labels: ',str(labels))\n",
    "        print('Calculating shot metrics for validation predictions of size ',str(len(preds)),' with validations labels of size ',str(len(labels)), ' and training labels of size ', str(len(train_labels)))\n",
    "        # print('Train labels: ',str(train_labels))\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "        print('Predicted ',str(preds),' for true improvements ',str(targets))\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n",
    "\n",
    "\n",
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=5, low_shot_thr=2):\n",
    "   \n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    #print('Unique validate labels: ',str(np.unique(labels)))\n",
    "    for l in np.unique(labels):\n",
    "        # print('Looking for label ',str(l),' in train labels: ',str(train_labels))\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        #print(train_labels)\n",
    "        #print(train_labels == l)\n",
    "        # print('Looking for label ',str(l),' in test labels: ',str(labels))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        # print('Train class count is ',str(train_class_count))\n",
    "        # print('Test class count is ',str(test_class_count))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "    return shot_dict\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
