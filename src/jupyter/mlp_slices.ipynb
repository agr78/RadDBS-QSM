{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.svm as svm\n",
    "import sklearn.pipeline as spl\n",
    "import sklearn.kernel_ridge as skr\n",
    "import sklearn.model_selection as sms\n",
    "import sklearn.linear_model as slm\n",
    "import sklearn.preprocessing as skp\n",
    "import sklearn.neighbors as snn\n",
    "import sklearn.metrics as sme\n",
    "import sklearn.decomposition as sdc\n",
    "import sklearn.cross_decomposition as skd\n",
    "import sklearn.feature_selection as skf\n",
    "import sklearn.ensemble as ske\n",
    "import sklearn.utils as sku\n",
    "from sklearn.utils import resample\n",
    "from sklearnex import patch_sklearn, config_context\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from IPython.display import HTML\n",
    "import util\n",
    "from lao_net import Net, train_model\n",
    "from sklearn import metrics\n",
    "import nibabel as nib\n",
    "import gc\n",
    "import torch\n",
    "patch_sklearn()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".jupyter-matplotlib {\n",
       "    background-color: #000;\n",
       "}\n",
       "\n",
       ".widget-label, .jupyter-matplotlib-header{\n",
       "    color: #fff;\n",
       "}\n",
       "\n",
       ".jupyter-button {\n",
       "    background-color: #333;\n",
       "    color: #fff;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''\n",
    "<style>\n",
    ".jupyter-matplotlib {\n",
    "    background-color: #000;\n",
    "}\n",
    "\n",
    ".widget-label, .jupyter-matplotlib-header{\n",
    "    color: #fff;\n",
    "}\n",
    "\n",
    ".jupyter-button {\n",
    "    background-color: #333;\n",
    "    color: #fff;\n",
    "}\n",
    "</style>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe y should also be scaled? \n",
    "#     Performed poorly using Standard and MinMax scalers. Trying with LOOCV to see if predictions stabilize.\n",
    "#     Does not appear to stabilize predictions with LOOCV (using StandardScaler())\n",
    "# Perhaps a transform would be more effective, or scaling implemented with consistent cross-validation\n",
    "# Different scaling methods? \n",
    "#     This seems most important for noise-sensitive models like LARS. All other use StandardScaler()\n",
    "# Transformers?\n",
    "# Model-specific scaling methods?\n",
    "#     Yes, see above\n",
    "# Common cross-validation function?\n",
    "#     Use built-in functions wherever possible and `utils.gridsearch_pickparams()` elsewhere\n",
    "# Quantile loss\n",
    "# RANSAC\n",
    "# Data augmentation? (Mixup)\n",
    "# Data generation? (SMOGN)\n",
    "# CHECK PHI AND X DIRECTORIES, WHICH ONE IS RIGHT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0                PRE-OP           Unnamed: 2  \\\n",
      "0      CORNELL ID  Apathy Off (pre-dbs)  Apathy ON (pre-dbs)   \n",
      "1              67                    na                   13   \n",
      "2   only Ct data                     na                   na   \n",
      "3              74                    na                   na   \n",
      "4              84                    na                   22   \n",
      "..            ...                   ...                  ...   \n",
      "87             52                   NaN                  NaN   \n",
      "88             53                   NaN                  NaN   \n",
      "89             54                   NaN                  NaN   \n",
      "90             55                   NaN                  NaN   \n",
      "91             56                   NaN                  NaN   \n",
      "\n",
      "             Unnamed: 3          Unnamed: 4    Unnamed: 5  \\\n",
      "0   OFF (pre-dbs updrs)  ON (pre-dbs updrs)  mri (pre-op)   \n",
      "1                    60                  41      3/9/2020   \n",
      "2                    43                  12            na   \n",
      "3                    34                  11     2/10/2020   \n",
      "4                    53                  13      6/1/2020   \n",
      "..                  ...                 ...           ...   \n",
      "87                   58                  27           NaN   \n",
      "88                   77                  47           NaN   \n",
      "89                   63                  36           NaN   \n",
      "90                   40                  29           NaN   \n",
      "91                   81                  20           NaN   \n",
      "\n",
      "                              Unnamed: 6                  POST-OP (6 MONTHS)  \\\n",
      "0   pre op levadopa equivalent dose (mg)  6 mo levadopa equivalent dose (mg)   \n",
      "1                                    500                                 500   \n",
      "2                                   2304                                 819   \n",
      "3                                   1200                                 600   \n",
      "4                                    350                                 150   \n",
      "..                                   ...                                 ...   \n",
      "87                                  1804                                1804   \n",
      "88                                   575                                 250   \n",
      "89                                  1400                                 400   \n",
      "90                                   400                                 450   \n",
      "91                                  2162                                1050   \n",
      "\n",
      "              Unnamed: 8           Unnamed: 9          Unnamed: 10  \\\n",
      "0   OFF meds ON stim 6mo  ON meds ON stim 6mo  off stim on med 6mo   \n",
      "1                     31                   29                  NaN   \n",
      "2                     na                   20                   18   \n",
      "3                     na                   17                   na   \n",
      "4                     na                   34                   na   \n",
      "..                   ...                  ...                  ...   \n",
      "87                    19                   12                  NaN   \n",
      "88                   NaN                   19                   26   \n",
      "89                     8                  NaN                  NaN   \n",
      "90                   NaN                   13                  NaN   \n",
      "91                   NaN                    7                  NaN   \n",
      "\n",
      "             Unnamed: 11     Unnamed: 12     Unnamed: 13 Unnamed: 14  \n",
      "0   off stim off med 6mo  apathy off 6mo  apathy on 6 mo        ledd  \n",
      "1                    NaN              na              na         NaN  \n",
      "2                    NaN              na              na         NaN  \n",
      "3                    NaN              na              na     1021.00  \n",
      "4                    NaN              na              na         NaN  \n",
      "..                   ...             ...             ...         ...  \n",
      "87                   NaN             NaN             NaN      500.00  \n",
      "88                   NaN             NaN             NaN      100.00  \n",
      "89                   NaN             NaN             NaN      550.00  \n",
      "90                    37             NaN             NaN      400.00  \n",
      "91                   NaN             NaN             NaN         NaN  \n",
      "\n",
      "[92 rows x 15 columns]\n",
      "       CORNELL ID OFF (pre-dbs updrs) ON (pre-dbs updrs)  \\\n",
      "1              67                  60                 41   \n",
      "2   only Ct data                   43                 12   \n",
      "3              74                  34                 11   \n",
      "4              84                  53                 13   \n",
      "5              75                  62                  8   \n",
      "..            ...                 ...                ...   \n",
      "87             52                  58                 27   \n",
      "88             53                  77                 47   \n",
      "89             54                  63                 36   \n",
      "90             55                  40                 29   \n",
      "91             56                  81                 20   \n",
      "\n",
      "   pre op levadopa equivalent dose (mg) OFF meds ON stim 6mo  \\\n",
      "1                                   500                   31   \n",
      "2                                  2304                  NaN   \n",
      "3                                  1200                  NaN   \n",
      "4                                   350                  NaN   \n",
      "5                                     0                    2   \n",
      "..                                  ...                  ...   \n",
      "87                                 1804                   19   \n",
      "88                                  575                  NaN   \n",
      "89                                 1400                    8   \n",
      "90                                  400                  NaN   \n",
      "91                                 2162                  NaN   \n",
      "\n",
      "   ON meds ON stim 6mo  off stim on med 6mo  off stim off med 6mo  \n",
      "1                   29                  NaN                   NaN  \n",
      "2                   20                   18                   NaN  \n",
      "3                   17                  NaN                   NaN  \n",
      "4                   34                  NaN                   NaN  \n",
      "5                  NaN                  NaN                   NaN  \n",
      "..                 ...                  ...                   ...  \n",
      "87                  12                  NaN                   NaN  \n",
      "88                  19                   26                   NaN  \n",
      "89                 NaN                  NaN                   NaN  \n",
      "90                  13                  NaN                    37  \n",
      "91                   7                  NaN                   NaN  \n",
      "\n",
      "[91 rows x 8 columns]\n",
      "Allocated arrays\n",
      "Created feature matrix\n",
      "Created ROI matrix\n",
      "Created feature label matrix\n"
     ]
    }
   ],
   "source": [
    "# Get case IDs\n",
    "case_list = open('/home/ali/RadDBS-QSM/data/docs/cases_90','r')\n",
    "lines = case_list.read()\n",
    "lists = np.loadtxt(case_list.name,comments=\"#\", delimiter=\",\",unpack=False,dtype=str)\n",
    "case_id = []\n",
    "for lines in lists:     \n",
    "    case_id.append(lines[-9:-7])\n",
    "\n",
    "# Load scores\n",
    "file_dir = '/home/ali/RadDBS-QSM/data/docs/QSM anonymus- 6.22.2023-1528_wldd.csv'\n",
    "motor_df = util.filter_scores(file_dir,'pre-dbs updrs','stim','pre op levadopa equivalent dose (mg)','CORNELL ID')\n",
    "# Find cases with all required scores\n",
    "subs,pre_imp,post_imp,pre_updrs_off,ledd = util.get_full_cases(motor_df,\n",
    "                                                          'CORNELL ID',\n",
    "                                                          'OFF (pre-dbs updrs)',\n",
    "                                                          'ON (pre-dbs updrs)',\n",
    "                                                          'OFF meds ON stim 6mo',\n",
    "                                                          'pre op levadopa equivalent dose (mg)')\n",
    "# Load extracted features\n",
    "npy_dir = '/home/ali/RadDBS-QSM/data/npy/'\n",
    "phi_dir = '/home/ali/RadDBS-QSM/data/phi/phi/'\n",
    "roi_path = '/data/Ali/atlas/mcgill_pd_atlas/PD25-subcortical-labels.csv'\n",
    "n_rois = 6\n",
    "Phi_all, X_all, R_all, K_all, ID_all = util.load_featstruct(phi_dir,npy_dir+'X/',npy_dir+'R/',npy_dir+'K/',n_rois,1595,False)\n",
    "del Phi_all, X_all, R_all, K_all\n",
    "ids = np.asarray(ID_all).astype(int)\n",
    "\n",
    "# Find overlap between scored subjects and feature extraction cases\n",
    "c_cases = np.intersect1d(np.asarray(case_id).astype(int),np.asarray(subs).astype(int))\n",
    "# Complete case indices with respect to feature matrix\n",
    "c_cases_idx = np.in1d(ids,c_cases)\n",
    "# Re-index the scored subjects with respect to complete cases\n",
    "s_cases_idx = np.in1d(subs,ids[c_cases_idx])\n",
    "subs_init = subs[s_cases_idx]\n",
    "pre_imp_init = pre_imp[s_cases_idx]\n",
    "post_imp_init = post_imp[s_cases_idx]\n",
    "pre_updrs_off_init = pre_updrs_off[s_cases_idx]\n",
    "per_change_init = post_imp_init\n",
    "subs = np.asarray(ID_all,dtype=float)[np.in1d(np.asarray(ID_all,dtype=float),subs_init)]\n",
    "\n",
    "pre_imp = np.zeros((1,len(subs))).T\n",
    "post_imp = np.zeros((1,len(subs))).T\n",
    "pre_updrs_off = np.zeros((1,len(subs))).T\n",
    "per_change = np.zeros((1,len(subs))).T\n",
    "for j in np.arange(len(subs)):\n",
    "    pre_imp[j] = pre_imp_init[subs_init == subs[j]]\n",
    "    post_imp[j] = post_imp_init[subs_init == subs[j]]\n",
    "    pre_updrs_off[j] = pre_updrs_off_init[subs_init == subs[j]]\n",
    "    per_change[j] = per_change_init[subs_init == subs[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsc = subs_init\n",
    "X_img = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload = 0\n",
    "if reload == 1:\n",
    "    qsms = util.full_path('/home/ali/RadDBS-QSM/data/nii/qsm')\n",
    "    qsms_subs = []\n",
    "    for Q in np.arange(len(qsms)):\n",
    "        qsms_subs.append(int(qsms[Q][-9:-7]))\n",
    "\n",
    "    for j in np.arange(len(subs_init)):\n",
    "        q = np.where(qsms_subs==subs_init[j])[0][0]\n",
    "        data = nib.load(qsms[q])\n",
    "        try:\n",
    "            if qsms_subs[q] < 10:\n",
    "                mask = nib.load('/home/ali/RadDBS-QSM/data/nii/seg/labels_2iMag0'+str(qsms_subs[q])+'.nii.gz').get_fdata()\n",
    "            else:\n",
    "                mask = nib.load('/home/ali/RadDBS-QSM/data/nii/seg/labels_2iMag'+str(qsms_subs[q])+'.nii.gz').get_fdata()\n",
    "            mask[mask > 4] = 0\n",
    "            mask[mask < 3] = 0\n",
    "            maskc = util.pad_to(util.mask_crop(mask,mask),64,64,64)\n",
    "            data = util.pad_to(util.mask_crop(mask*data.get_fdata(),mask),64,64,64)\n",
    "            mask_k = []\n",
    "            k_all = []\n",
    "            for k in np.arange(maskc.shape[2]):\n",
    "                    if np.sum(maskc[:,:,k]) > 0:\n",
    "                        mask_k.append(np.sum(maskc[:,:,k]))\n",
    "                        k_all.append(k)\n",
    "                \n",
    "            img = data[:,:,k_all[np.argmax(mask_k)]]\n",
    "            # img[img<-250] = -250\n",
    "            # img[img>250] = 250\n",
    "            X_img.append(torch.Tensor(img).cuda())\n",
    "            print('Maximum volume found at slice',str(k_all[np.argmax(mask_k)]),'for case',str(qsms_subs[q]))\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "        except:\n",
    "            print('Missing mask at',str(qsms_subs[q]))\n",
    "            subsc = np.delete(subsc,j)\n",
    "            per_change = np.delete(per_change,j)\n",
    "            pre_imp = np.delete(pre_imp,j)\n",
    "            pre_updrs_off = np.delete(pre_updrs_off,j)\n",
    "    torch.save(X_img,'X_img.pt') \n",
    "else:\n",
    "    X_img = torch.load('X_img.pt')\n",
    "    subsc = subsc[:-1]\n",
    "    per_change = per_change[:-1]\n",
    "    pre_imp = pre_imp[:-1]\n",
    "    pre_updrs_off = np.squeeze(pre_updrs_off[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img = torch.stack(X_img).detach().cpu().numpy()\n",
    "results_bls = np.zeros_like(per_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1566801518201828 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.156680\n",
      "0.001\n",
      "Best training loss: 0.028574472293257713 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.021066518500447273 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.054738\n",
      "0.001\n",
      "Best training loss: 0.014737174846231937 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.010408572852611542 at learning rate 0.001 and epoch 17\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.023826\n",
      "0.001\n",
      "Best training loss: 0.00730310520157218 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.005083815194666386 at learning rate 0.001 and epoch 27\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.010264\n",
      "0.001\n",
      "Best training loss: 0.0034954717848449945 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.002380993915721774 at learning rate 0.001 and epoch 37\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.004178\n",
      "0.001\n",
      "Best training loss: 0.0016503904480487108 at learning rate 0.001 and epoch 42\n",
      "Best training loss: 0.0011755172163248062 at learning rate 0.001 and epoch 47\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001733\n",
      "0.001\n",
      "Best training loss: 0.0008454128401353955 at learning rate 0.001 and epoch 52\n",
      "Best training loss: 0.000615044787991792 at learning rate 0.001 and epoch 57\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000757\n",
      "0.001\n",
      "Best training loss: 0.0004525316471699625 at learning rate 0.001 and epoch 62\n",
      "Best training loss: 0.00033224301296286285 at learning rate 0.001 and epoch 67\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000350\n",
      "0.001\n",
      "Best training loss: 0.00024390457838308066 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00018194777658209205 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00017537837265990674 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000175\n",
      "0.001\n",
      "Best training loss: 0.00013837193546351045 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00012923205213155597 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00010671249765437096 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 9.738628432387486e-05 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000097\n",
      "0.001\n",
      "Best training loss: 8.320658525917679e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 7.466047827620059e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 6.542249320773408e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 6.475509144365788e-05 at learning rate 0.001 and epoch 99\n",
      "Best training loss: 5.8131347032031044e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000058\n",
      "0.001\n",
      "Best training loss: 5.190705633140169e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.0603899580892175e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 4.59048715129029e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.1580788092687726e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.0107817767420784e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 3.6767571145901456e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000037\n",
      "0.001\n",
      "Best training loss: 3.365372322150506e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.217849007342011e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 2.9821485441061668e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 2.753311491687782e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.725937702052761e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.622053216327913e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.4543529434595257e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000025\n",
      "0.001\n",
      "Best training loss: 2.282887726323679e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.2437889128923416e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.1675483367289416e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.0483288608375005e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.0355837477836758e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 1.9166509446222335e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.875250381999649e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.8196511518908665e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.732881719362922e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000017\n",
      "0.001\n",
      "Best training loss: 1.711498225631658e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.630337101232726e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.5922134480206296e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.548356522107497e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.4853951142868027e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.4630276382376906e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.404873273713747e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.3716999092139304e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.3383971236180514e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.2916550986119546e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000013\n",
      "0.001\n",
      "Best training loss: 1.2697084457613528e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.2283710020710714e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2006823453702964e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.173691180156311e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.1391027328500059e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1198470929230098e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.088844146579504e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.0659950930858031e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.0449089131725486e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0184494385612197e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 1.0013245628215373e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 9.782595043361653e-06 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 9.595412848284468e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.42020233196672e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.21835089684464e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.074135050468612e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 8.893141057342291e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 8.740180419408716e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 8.601086847193073e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.44214537210064e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 8.319108019350097e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.179732503776904e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.055142643570434e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 7.939164788695052e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 7.815417120582424e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 7.714410457992926e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.602522146044066e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.502092103095492e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.409980753436685e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.311342869797954e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.226967682072427e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.139460649341345e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.057500624796376e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 6.980905709497165e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 6.9030043050588574e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 6.834176019765437e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.762277735106181e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.694937837892212e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.632349141000304e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.568080152646871e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.5097938204417005e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.452007710322505e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.39682821201859e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.344326720864046e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.292163561738562e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.243757525226101e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.195044534251792e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.149159617052646e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.105952707002871e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.062990451027872e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.0229795053601265e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 5.9840440371772274e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 5.946282726654317e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.909659648750676e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.87398335483158e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.840349786012666e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.806814442621544e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.775075806013774e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.744687314290786e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.9069, 0.5568, 0.9512, 0.8866, 0.5910, 0.7405, 0.5013, 0.7026, 0.6886,\n",
      "         0.8742, 0.6625, 0.6964, 0.8846, 0.7433, 0.8015, 0.6672, 0.6937, 0.4281,\n",
      "         0.1906, 0.9256, 0.5520, 0.6727, 0.8711, 0.5181, 0.7414, 0.8437, 0.4693,\n",
      "         0.8505, 0.6638, 0.5292, 0.4851, 0.7446, 0.4869, 0.2670, 0.9645, 0.5605,\n",
      "         0.8189, 0.6541, 0.5273, 0.1984, 0.8394, 0.1503, 0.5024, 0.0896, 0.3618]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.744687314290786e-06 from epoch 199\n",
      "MLP predicts 1.144 for case 67 with [0.83]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15257391333580017 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.152574\n",
      "0.001\n",
      "Best training loss: 0.03154395520687103 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.0179118774831295 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.017912\n",
      "0.001\n",
      "Best training loss: 0.012825174257159233 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.011977245099842548 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.011977\n",
      "0.001\n",
      "Best training loss: 0.009027383290231228 at learning rate 0.001 and epoch 23\n",
      "Best training loss: 0.004981579724699259 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.009725\n",
      "0.001\n",
      "Best training loss: 0.003530159592628479 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.0030546148773282766 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.006138\n",
      "0.001\n",
      "Best training loss: 0.001965835690498352 at learning rate 0.001 and epoch 41\n",
      "Best training loss: 0.001357705914415419 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002947\n",
      "0.001\n",
      "Best training loss: 0.0011613655369728804 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.000840670196339488 at learning rate 0.001 and epoch 54\n",
      "Best training loss: 0.0006083736079744995 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001090\n",
      "0.001\n",
      "Best training loss: 0.0005664240452460945 at learning rate 0.001 and epoch 62\n",
      "Best training loss: 0.0004979285295121372 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00036961465957574546 at learning rate 0.001 and epoch 67\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000373\n",
      "0.001\n",
      "Best training loss: 0.0002751248830463737 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.0002395363844698295 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00022087238903623074 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.0001714351965347305 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000171\n",
      "0.001\n",
      "Best training loss: 0.00016039548791013658 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.00013350561494007707 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00011489755706861615 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 0.00010780386219266802 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000108\n",
      "0.001\n",
      "Best training loss: 8.778947812970728e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 8.721094491193071e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.891272980486974e-05 at learning rate 0.001 and epoch 96\n",
      "Best training loss: 6.986935477470979e-05 at learning rate 0.001 and epoch 98\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000070\n",
      "0.001\n",
      "Best training loss: 6.0383241361705586e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 5.6660948757780716e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.4910771723371e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 4.796985012944788e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.610530231730081e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.269598503015004e-05 at learning rate 0.001 and epoch 109\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000044\n",
      "0.001\n",
      "Best training loss: 3.898933937307447e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.873589957947843e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.750485120690428e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.421260043978691e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.209305214113556e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.085508433287032e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.052792089874856e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.806309930747375e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.7940872314502485e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000028\n",
      "0.001\n",
      "Best training loss: 2.661135840753559e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.5173654648824595e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.5042461857083254e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.3418262571794912e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.2877557057654485e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.2248806999414228e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.1003306756028906e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.077499812003225e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.9808465367532335e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.912776497192681e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000019\n",
      "0.001\n",
      "Best training loss: 1.8765498680295423e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.784525920811575e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.7516835214337334e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.6938809494604357e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.629753205634188e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.603052078280598e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.53932433022419e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.5016594261396676e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4662450666946825e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.4133725926512852e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3876668162993155e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3456247870635707e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.309381150349509e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2836406312999316e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2440643331501633e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2195625458844006e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1904498933290597e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1593425369937904e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1390028703317512e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.1097650713054463e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.087064538296545e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0663462489901576e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0410809409222566e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0232554814137984e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.002181124931667e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.823816071730107e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.659082934376784e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.463489732297603e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.308397238783073e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.147329365077894e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.9829190983437e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.849658115650527e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.696707482158672e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.562648872612044e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.438562872470357e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.30379212857224e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.190320841094945e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.072609489317983e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.959504728205502e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.857389391574543e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.750235454295762e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.654892215214204e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.5609177656588145e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.4678741839306895e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.3848436841217335e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.298523087229114e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.218943210318685e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.14418274583295e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.067760634527076e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.997763193794526e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.92827552484232e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.8610484049713705e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.798055437684525e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.735375336575089e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.6765674091584515e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.61942976876162e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.563652732438641e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.51187110634055e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.460218628490111e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.410995865735458e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.363922693708446e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.317417955870042e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.27366398475715e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.231263341760496e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.1899186221126e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.150599801912904e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.112355549703352e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.0758734434784856e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.040465450496413e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.5568, 0.9510, 0.8868, 0.5910, 0.7412, 0.5013, 0.7027, 0.6888,\n",
      "         0.8742, 0.6627, 0.6963, 0.8843, 0.7426, 0.8014, 0.6671, 0.6941, 0.4279,\n",
      "         0.1904, 0.9251, 0.5520, 0.6726, 0.8712, 0.5178, 0.7412, 0.8434, 0.4693,\n",
      "         0.8507, 0.6642, 0.5288, 0.4856, 0.7446, 0.4867, 0.2675, 0.9645, 0.5606,\n",
      "         0.8188, 0.6540, 0.5274, 0.1979, 0.8391, 0.1505, 0.5026, 0.0895, 0.3616]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 6.040465450496413e-06 from epoch 199\n",
      "MLP predicts 0.572 for case 75 with [0.91]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15735523402690887 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.157355\n",
      "0.001\n",
      "Best training loss: 0.03097931295633316 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.027828127145767212 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.027828\n",
      "0.001\n",
      "Best training loss: 0.014571349136531353 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.008901059627532959 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.008901\n",
      "0.001\n",
      "Best training loss: 0.0059943790547549725 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.004335665609687567 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.004336\n",
      "0.001\n",
      "Best training loss: 0.003282873658463359 at learning rate 0.001 and epoch 35\n",
      "Best training loss: 0.0029366721864789724 at learning rate 0.001 and epoch 38\n",
      "Best training loss: 0.0025691932532936335 at learning rate 0.001 and epoch 40\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.002569\n",
      "0.001\n",
      "Best training loss: 0.0017202929593622684 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0011373060988262296 at learning rate 0.001 and epoch 48\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001651\n",
      "0.001\n",
      "Best training loss: 0.0008364389650523663 at learning rate 0.001 and epoch 53\n",
      "Best training loss: 0.0006550994585268199 at learning rate 0.001 and epoch 58\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000964\n",
      "0.001\n",
      "Best training loss: 0.0005538800614885986 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.0005229896632954478 at learning rate 0.001 and epoch 63\n",
      "Best training loss: 0.00038011945434845984 at learning rate 0.001 and epoch 66\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000500\n",
      "0.001\n",
      "Best training loss: 0.0002758075133897364 at learning rate 0.001 and epoch 71\n",
      "Best training loss: 0.0002648138324730098 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.00020961620612069964 at learning rate 0.001 and epoch 76\n",
      "Best training loss: 0.0001829764514695853 at learning rate 0.001 and epoch 79\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000244\n",
      "0.001\n",
      "Best training loss: 0.0001641389972064644 at learning rate 0.001 and epoch 81\n",
      "Best training loss: 0.00013390395906753838 at learning rate 0.001 and epoch 84\n",
      "Best training loss: 0.0001297624403377995 at learning rate 0.001 and epoch 86\n",
      "Best training loss: 0.00012921285815536976 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00010236911475658417 at learning rate 0.001 and epoch 89\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000122\n",
      "0.001\n",
      "Best training loss: 9.418631088919938e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 8.052792691159993e-05 at learning rate 0.001 and epoch 94\n",
      "Best training loss: 7.121009548427537e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 6.431257497752085e-05 at learning rate 0.001 and epoch 99\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000066\n",
      "0.001\n",
      "Best training loss: 6.401300197467208e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 5.548281842493452e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.179557774681598e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 5.084320218884386e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 5.054808207205497e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.4201326090842485e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.1923321987269446e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 3.987275704275817e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000040\n",
      "0.001\n",
      "Best training loss: 3.579666372388601e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.4125569072784856e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.196766920154914e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 2.9363500289036892e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.9077340514049865e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.7954823963227682e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.6120804250240326e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000026\n",
      "0.001\n",
      "Best training loss: 2.4362998374272138e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.3740154574625194e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.3145295926951803e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.1720345102949068e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.154909452656284e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.0449559087865055e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.975413761101663e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.9360553778824396e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.8327473298995756e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000018\n",
      "0.001\n",
      "Best training loss: 1.805018837330863e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.734354009386152e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.6706948372302577e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.6417514416389167e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.567941035318654e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.5355823052232154e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.490180966357002e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4369681593962014e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4115213161858264e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.3605351341539063e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3292757103045005e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.2970773241249844e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2554210115922615e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2332017831795383e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.1965904377575498e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1684623132168781e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1449713383626658e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1125735909445211e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.0923697118414566e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0662814929673914e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0422540071886033e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0232467502646614e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 9.987501471186988e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.811672498472035e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.612687790649943e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.41525195230497e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.26232951314887e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.07085086510051e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 8.917231753002852e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.765750862949062e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.603528840467334e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.472624358546454e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.325656381202862e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.195117516152095e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.070923286140896e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 7.940042451082263e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.830268259567674e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.711502803431358e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.60185730541707e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.502084372390527e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.394964541163063e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.301202003873186e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.206787813629489e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.114553682185942e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.0299884100677446e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 6.943948392290622e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.866161584184738e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.789487088099122e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.714608389302157e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.646035672019934e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.575038696610136e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.509212653327268e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.447181021940196e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.385113920259755e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.327722076093778e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.2714093473914545e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.218016096681822e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.16735724179307e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.118113105912926e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.072282303648535e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.0270090216363315e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 5.984265044389758e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 5.944516487943474e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.904762019781629e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.867564595973818e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.8326145335740875e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.798878646601224e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.766944923379924e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.735995728173293e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9070, 0.9513, 0.8865, 0.5910, 0.7405, 0.5013, 0.7028, 0.6887,\n",
      "         0.8743, 0.6624, 0.6968, 0.8850, 0.7430, 0.8014, 0.6674, 0.6936, 0.4283,\n",
      "         0.1905, 0.9252, 0.5522, 0.6727, 0.8712, 0.5181, 0.7415, 0.8439, 0.4694,\n",
      "         0.8504, 0.6639, 0.5290, 0.4847, 0.7445, 0.4866, 0.2673, 0.9648, 0.5604,\n",
      "         0.8192, 0.6541, 0.5272, 0.1983, 0.8394, 0.1502, 0.5025, 0.0896, 0.3618]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.735995728173293e-06 from epoch 199\n",
      "MLP predicts 0.647 for case 68 with [0.56]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15256144106388092 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.152561\n",
      "0.001\n",
      "Best training loss: 0.028796039521694183 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.02183135785162449 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.047998\n",
      "0.001\n",
      "Best training loss: 0.01553348172456026 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.01091614831238985 at learning rate 0.001 and epoch 17\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.019897\n",
      "0.001\n",
      "Best training loss: 0.007573776412755251 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.005208414513617754 at learning rate 0.001 and epoch 27\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.008500\n",
      "0.001\n",
      "Best training loss: 0.003578527830541134 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.002445330610498786 at learning rate 0.001 and epoch 37\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.003458\n",
      "0.001\n",
      "Best training loss: 0.0017040700186043978 at learning rate 0.001 and epoch 42\n",
      "Best training loss: 0.0012302391696721315 at learning rate 0.001 and epoch 47\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001454\n",
      "0.001\n",
      "Best training loss: 0.0009036528645083308 at learning rate 0.001 and epoch 52\n",
      "Best training loss: 0.0006685531698167324 at learning rate 0.001 and epoch 57\n",
      "Best training loss: 0.000661935773678124 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000662\n",
      "0.001\n",
      "Best training loss: 0.0004991398891434073 at learning rate 0.001 and epoch 62\n",
      "Best training loss: 0.00046100857434794307 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.0003714052145369351 at learning rate 0.001 and epoch 67\n",
      "Best training loss: 0.0003246890555601567 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000325\n",
      "0.001\n",
      "Best training loss: 0.00027556909481063485 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00023306190269067883 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00020666015916503966 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.0001719771680654958 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000172\n",
      "0.001\n",
      "Best training loss: 0.00015722688112873584 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00012968279770575464 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00012077223800588399 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 9.960940951714292e-05 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000100\n",
      "0.001\n",
      "Best training loss: 9.375442459713668e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 7.766128692310303e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.340485171880573e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 7.215551886474714e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.130074325483292e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000061\n",
      "0.001\n",
      "Best training loss: 5.8019526477437466e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.6097316701198e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 4.893388177151792e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.6283876145025715e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.433401772985235e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 3.948608355131e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000039\n",
      "0.001\n",
      "Best training loss: 3.731050674105063e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.5583543649408966e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.541639671311714e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.218018537154421e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.037181159015745e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.8954591471119784e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.8709690013783984e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.6525625798967667e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000027\n",
      "0.001\n",
      "Best training loss: 2.635213968460448e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.5049377654795535e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.392218266322743e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.3606233298778534e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.2131065634312108e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.181896343245171e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.0907340513076633e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.0031508029205725e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.971818164747674e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.8684071619645692e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000019\n",
      "0.001\n",
      "Best training loss: 1.8326529243495315e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.768689435266424e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.700851680652704e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.6699857951607555e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.597769914951641e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.5641580830560997e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.5164237083808985e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4648113392468076e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4378762898559216e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.3853947166353464e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3551980373449624e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.319735838478664e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2797717317880597e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.255494134966284e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2173957657068968e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.191752562590409e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1638539035629947e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1332333087921143e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1128523510706145e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0839318747457583e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0623595699144062e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0408622983959503e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0169399502046872e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.9939716164954e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.776217666512821e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.597932148608379e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.422493349120487e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.235672223439906e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.089927516470198e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.916979822970461e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.768544830672909e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.628046998637728e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.476394214085303e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.350040843652096e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.212912689486984e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.089284165180288e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.97116899775574e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.8497751019313e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.745032235106919e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.633199857082218e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.530195944127627e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.432949132635258e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.331967026402708e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.242440005938988e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.15193937139702e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.066369562380714e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.985198979236884e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.904363999638008e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.831030077592004e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.7568817030405626e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.687122549919877e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.6216584855283145e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.556256266776472e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.496430614788551e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.43788780507748e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.381449566106312e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.327236405923031e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.274258794292109e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.2240947045211215e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.174305781314615e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.12699022894958e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.082559593778569e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.0388042584236246e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.997853349981597e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.9584740483842324e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.9206076912232675e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.884477104700636e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.84941608394729e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.816258180857403e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9071, 0.5568, 0.8865, 0.5909, 0.7405, 0.5013, 0.7028, 0.6884,\n",
      "         0.8743, 0.6623, 0.6964, 0.8848, 0.7430, 0.8015, 0.6673, 0.6934, 0.4283,\n",
      "         0.1905, 0.9252, 0.5521, 0.6726, 0.8710, 0.5180, 0.7415, 0.8439, 0.4693,\n",
      "         0.8502, 0.6638, 0.5290, 0.4847, 0.7444, 0.4865, 0.2674, 0.9646, 0.5603,\n",
      "         0.8191, 0.6541, 0.5274, 0.1982, 0.8392, 0.1501, 0.5026, 0.0895, 0.3618]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.816258180857403e-06 from epoch 199\n",
      "MLP predicts 0.848 for case 79 with [0.95]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15317794680595398 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.153178\n",
      "0.001\n",
      "Best training loss: 0.02891872078180313 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.022078868001699448 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.047204\n",
      "0.001\n",
      "Best training loss: 0.015754761174321175 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.011048605665564537 at learning rate 0.001 and epoch 17\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.019271\n",
      "0.001\n",
      "Best training loss: 0.007660394534468651 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.00526185380294919 at learning rate 0.001 and epoch 27\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.008132\n",
      "0.001\n",
      "Best training loss: 0.0036132177338004112 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.002466601552441716 at learning rate 0.001 and epoch 37\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.003269\n",
      "0.001\n",
      "Best training loss: 0.0017163470620289445 at learning rate 0.001 and epoch 42\n",
      "Best training loss: 0.0012406499590724707 at learning rate 0.001 and epoch 47\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001366\n",
      "0.001\n",
      "Best training loss: 0.0009157916065305471 at learning rate 0.001 and epoch 52\n",
      "Best training loss: 0.0009118543821386993 at learning rate 0.001 and epoch 55\n",
      "Best training loss: 0.000680259894579649 at learning rate 0.001 and epoch 57\n",
      "Best training loss: 0.0006259008077904582 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000626\n",
      "0.001\n",
      "Best training loss: 0.0005089250043965876 at learning rate 0.001 and epoch 62\n",
      "Best training loss: 0.00043803066364489496 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.00037862543831579387 at learning rate 0.001 and epoch 67\n",
      "Best training loss: 0.00031003550975583494 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000310\n",
      "0.001\n",
      "Best training loss: 0.00028052504057995975 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00022362226445693523 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00020964651776012033 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00016579632938373834 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000166\n",
      "0.001\n",
      "Best training loss: 0.00015884892491158098 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00012560110189951956 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00012147994129918516 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00012105242058169097 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 9.682106610853225e-05 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000097\n",
      "0.001\n",
      "Best training loss: 9.389664774062112e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 9.07211287994869e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 7.571881724288687e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.320807344513014e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 6.940426828805357e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 5.988652992527932e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000060\n",
      "0.001\n",
      "Best training loss: 5.764430534327403e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.4121996072353795e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 4.785774945048615e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.580064342007972e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.2871204641414806e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 3.8614416553173214e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000039\n",
      "0.001\n",
      "Best training loss: 3.678763096104376e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.44620093528647e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.1447205401491374e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.1404535548062995e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 2.982908335980028e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.805705116770696e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.8041889891028404e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.5877492589643225e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000026\n",
      "0.001\n",
      "Best training loss: 2.5542960429447703e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.4515698896721005e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.3180837160907686e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.2994658138486557e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.1560321329161525e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.113912887580227e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.0406258045113645e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.9416689610807225e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.917692134156823e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.8184271539212205e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000018\n",
      "0.001\n",
      "Best training loss: 1.7757060049916618e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.723450441204477e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.649330988584552e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.622560557734687e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.554117989144288e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.5158812857407611e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.475268800277263e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4202534657670185e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.3952356312074699e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.3455063708533999e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000013\n",
      "0.001\n",
      "Best training loss: 1.3125103578204289e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.2816953130823094e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2400278137647547e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2169235560577363e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.1809897841885686e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1535452358657494e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1285399523330852e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.0972706149914302e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.077579418051755e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0504128113097977e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0281335562467575e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.008688377623912e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 9.846005923463963e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.676372428657487e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.47270109463716e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.291170499636792e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.129522368311882e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 8.9444956756779e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 8.802836418908555e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.64023149915738e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.49191565066576e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.361330401385203e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.21341291157296e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.092171810858417e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 7.964052201714367e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 7.841481419745833e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.729907338216435e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.611730779899517e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.509512670367258e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.402793016808573e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.301738150999881e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.210677722468972e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.113792435120558e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.02783654560335e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 6.943017524463357e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 6.860624125692993e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.785124696762068e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.70952613290865e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.6410284489393234e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.573189693881432e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.508541446237359e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.449443844758207e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.389086593117099e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.33378158454434e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.280502020672429e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.228465736057842e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.179524461913388e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.131317149993265e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.085826498747338e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.040840162313543e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 5.997404514346272e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 5.956305813015206e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 5.9151525420020334e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.876315753994277e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.83862811254221e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.80176447328995e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.766883987234905e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.733842954214197e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.70265501664835e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9071, 0.5570, 0.9512, 0.5913, 0.7403, 0.5014, 0.7028, 0.6884,\n",
      "         0.8743, 0.6623, 0.6963, 0.8847, 0.7429, 0.8011, 0.6674, 0.6935, 0.4283,\n",
      "         0.1905, 0.9251, 0.5521, 0.6726, 0.8711, 0.5179, 0.7415, 0.8440, 0.4693,\n",
      "         0.8504, 0.6638, 0.5291, 0.4848, 0.7442, 0.4866, 0.2672, 0.9644, 0.5602,\n",
      "         0.8191, 0.6541, 0.5266, 0.1983, 0.8390, 0.1504, 0.5027, 0.0896, 0.3615]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.70265501664835e-06 from epoch 199\n",
      "MLP predicts 0.773 for case 59 with [0.89]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15693292021751404 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.156933\n",
      "0.001\n",
      "Best training loss: 0.033493246883153915 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.01897331140935421 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.018973\n",
      "0.001\n",
      "Best training loss: 0.01266393717378378 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.011432803235948086 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.011433\n",
      "0.001\n",
      "Best training loss: 0.00954285729676485 at learning rate 0.001 and epoch 23\n",
      "Best training loss: 0.005138141103088856 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.009151\n",
      "0.001\n",
      "Best training loss: 0.0034469864331185818 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.0027881625574082136 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.005768\n",
      "0.001\n",
      "Best training loss: 0.0021173106506466866 at learning rate 0.001 and epoch 41\n",
      "Best training loss: 0.0013111947337165475 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002889\n",
      "0.001\n",
      "Best training loss: 0.0010222826385870576 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0008890299359336495 at learning rate 0.001 and epoch 54\n",
      "Best training loss: 0.0008817585767246783 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.0005863687838427722 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001160\n",
      "0.001\n",
      "Best training loss: 0.0004514440952334553 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00039190269308164716 at learning rate 0.001 and epoch 67\n",
      "Best training loss: 0.00036919498234055936 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000420\n",
      "0.001\n",
      "Best training loss: 0.00027154787676408887 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.0002636229619383812 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.0002075119555229321 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00017916099750436842 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000179\n",
      "0.001\n",
      "Best training loss: 0.0001668641489231959 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00013271992793306708 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00012285921548027545 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 0.00010434380965307355 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000104\n",
      "0.001\n",
      "Best training loss: 9.089085506275296e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 8.429964509559795e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.06487990100868e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.848821067251265e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000068\n",
      "0.001\n",
      "Best training loss: 6.384433800121769e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 5.6683264119783416e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.5422697187168524e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.9850081268232316e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.621755579137243e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.534354593488388e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 4.471973079489544e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000045\n",
      "0.001\n",
      "Best training loss: 4.0060571336653084e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.795626980718225e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.600524360081181e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.283546902821399e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.267016290919855e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.1275481887860224e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.9268710932228714e-05 at learning rate 0.001 and epoch 119\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000029\n",
      "0.001\n",
      "Best training loss: 2.7287132979836315e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.6551380869932473e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.5936547899618745e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.429196501907427e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.4079832655843347e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.292084536748007e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.2022853954695165e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.1662848666892387e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.0487063011387363e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 2.009904528676998e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000020\n",
      "0.001\n",
      "Best training loss: 1.9417690054979175e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.859065014286898e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.830271321523469e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.7501768525107764e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.7038410078384914e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.6625062926323153e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.5954001355567016e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.5658646589145064e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.5138682101678569e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.4690429452457465e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000015\n",
      "0.001\n",
      "Best training loss: 1.4389180250873324e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3892286006012e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.3602640137833077e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.3245770787762012e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2861412869824562e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2622662325156853e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.2257767593837343e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1985974197159521e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.173519285657676e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.142305427492829e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.1212275239813607e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0951902368105948e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0713945812312886e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0517387636355124e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.0278225090587512e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.0096178812091239e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.899839824356604e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.701360795588698e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.544328349875286e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.357590897707269e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 9.198907719110139e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 9.04993612493854e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.888112461136188e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.754826922086067e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.612261808593757e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.477200026391074e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.354827514267527e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.224696102843154e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.11132849776186e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.995903615665156e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.883271791797597e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.782888133078814e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.676875611650757e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.580460078315809e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.48881984691252e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.395503871521214e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.311276476684725e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.2275070124305785e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.147631094994722e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.072550488373963e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.997814580245176e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.9290026658563875e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.860837402200559e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.795011358917691e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.734068847435992e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.672903055005008e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.615754955419106e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.561206646438222e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.507044417958241e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.455904895119602e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.40495909465244e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.3553766267432366e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.307612693490228e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.261197086132597e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.217533154995181e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.1750893110001925e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.134296199888922e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.095994194765808e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.058416602172656e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9072, 0.5570, 0.9512, 0.8868, 0.7410, 0.5013, 0.7027, 0.6888,\n",
      "         0.8743, 0.6625, 0.6964, 0.8849, 0.7430, 0.8014, 0.6673, 0.6939, 0.4281,\n",
      "         0.1904, 0.9250, 0.5521, 0.6727, 0.8708, 0.5177, 0.7413, 0.8437, 0.4692,\n",
      "         0.8504, 0.6639, 0.5291, 0.4851, 0.7446, 0.4867, 0.2679, 0.9645, 0.5604,\n",
      "         0.8189, 0.6539, 0.5270, 0.1980, 0.8387, 0.1504, 0.5028, 0.0896, 0.3616]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 6.058416602172656e-06 from epoch 199\n",
      "MLP predicts 0.557 for case 85 with [0.59]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1557551771402359 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.155755\n",
      "0.001\n",
      "Best training loss: 0.029661379754543304 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.025170862674713135 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.038613\n",
      "0.001\n",
      "Best training loss: 0.019573893398046494 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.01437841821461916 at learning rate 0.001 and epoch 17\n",
      "Best training loss: 0.013726183213293552 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.013726\n",
      "0.001\n",
      "Best training loss: 0.010281278751790524 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.008551976643502712 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.0072053843177855015 at learning rate 0.001 and epoch 27\n",
      "Best training loss: 0.005391220562160015 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.005391\n",
      "0.001\n",
      "Best training loss: 0.0050595784559845924 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.003388511249795556 at learning rate 0.001 and epoch 35\n",
      "Best training loss: 0.0021603088825941086 at learning rate 0.001 and epoch 40\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.002160\n",
      "0.001\n",
      "Best training loss: 0.0014439255464822054 at learning rate 0.001 and epoch 45\n",
      "Best training loss: 0.0010201125405728817 at learning rate 0.001 and epoch 50\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001020\n",
      "0.001\n",
      "Best training loss: 0.0007457680185325444 at learning rate 0.001 and epoch 55\n",
      "Best training loss: 0.0005583296879194677 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000558\n",
      "0.001\n",
      "Best training loss: 0.0005408104043453932 at learning rate 0.001 and epoch 63\n",
      "Best training loss: 0.0004215054214000702 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.0003665145195554942 at learning rate 0.001 and epoch 68\n",
      "Best training loss: 0.000316429854137823 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000316\n",
      "0.001\n",
      "Best training loss: 0.00025522339274175465 at learning rate 0.001 and epoch 73\n",
      "Best training loss: 0.00023781076015438884 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00018396515224594623 at learning rate 0.001 and epoch 78\n",
      "Best training loss: 0.00018011174688581377 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000180\n",
      "0.001\n",
      "Best training loss: 0.00013694827794097364 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.00013037658936809748 at learning rate 0.001 and epoch 86\n",
      "Best training loss: 0.00010421223851153627 at learning rate 0.001 and epoch 88\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000105\n",
      "0.001\n",
      "Best training loss: 9.523688640911132e-05 at learning rate 0.001 and epoch 91\n",
      "Best training loss: 8.058169623836875e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 7.152317266445607e-05 at learning rate 0.001 and epoch 96\n",
      "Best training loss: 6.305251008598134e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.236993795027956e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000062\n",
      "0.001\n",
      "Best training loss: 5.4873951739864424e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 4.9749360186979175e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 4.858207466895692e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.29111642006319e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 3.9610546082258224e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 3.910942905349657e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 3.8154448702698573e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000038\n",
      "0.001\n",
      "Best training loss: 3.408597331144847e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.179000850650482e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.078863664995879e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.032501263078302e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 2.746763129835017e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 2.578726525825914e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.4721937734284438e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.4406832380918786e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000024\n",
      "0.001\n",
      "Best training loss: 2.2461619664682075e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.2406842617783695e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.1166897568036802e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.0216719349264167e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 1.9974704628111795e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 1.8640124835656025e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 1.839468677644618e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.7620279322727583e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.6834486814332195e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.6592079191468656e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000017\n",
      "0.001\n",
      "Best training loss: 1.5689927749917842e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.5386120139737613e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.4860268493066542e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.4246085811464582e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.4025763448444195e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.3397781913226936e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.309324215981178e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.2735486961901188e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.226258336828323e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.2053608770656865e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000012\n",
      "0.001\n",
      "Best training loss: 1.1624132639553864e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.1353326044627465e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.1085139703936875e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.0730530448199715e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.0547480997047387e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.0236952221021056e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.0007146556745283e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 9.809888069867156e-06 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 9.54158076638123e-06 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 9.379700713907368e-06 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 9.158610737358686e-06 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 8.969803275249433e-06 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 8.811495717964135e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 8.611410521552898e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 8.475267350149807e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 8.307887583214324e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 8.156123840308283e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 8.033089216041844e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 7.879582881287206e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 7.764689144096337e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.64030573918717e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 7.518635356973391e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 7.4174267865600996e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 7.3016494752664585e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 7.207808266684879e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 7.1093259066401515e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.012476089585107e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 6.9308330239437055e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 6.837974979134742e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 6.757790742994985e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.679143552901223e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 6.598776508326409e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 6.528641279146541e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 6.455053608078742e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 6.387909252225654e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 6.322780336631695e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.258004759729374e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.200365533004515e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.139856850495562e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.0839406614832114e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.031033080944326e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 5.977219188935123e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 5.928323844273109e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 5.879612217540853e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 5.833258455822943e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 5.7892734730558e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 5.746327587985434e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 5.707294803869445e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 5.6690214478294365e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 5.633291038975585e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 5.600201802735683e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 5.566671006818069e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 5.5355162658088375e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.505698936758563e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.476786554936552e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.449227955978131e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.4227184591582045e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.3976973504177295e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.372830855776556e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9071, 0.5566, 0.9517, 0.8870, 0.5911, 0.5014, 0.7035, 0.6886,\n",
      "         0.8741, 0.6622, 0.6970, 0.8851, 0.7430, 0.8019, 0.6673, 0.6935, 0.4282,\n",
      "         0.1903, 0.9258, 0.5521, 0.6727, 0.8713, 0.5180, 0.7415, 0.8438, 0.4693,\n",
      "         0.8503, 0.6642, 0.5292, 0.4850, 0.7447, 0.4865, 0.2672, 0.9648, 0.5604,\n",
      "         0.8196, 0.6540, 0.5278, 0.1983, 0.8393, 0.1502, 0.5028, 0.0894, 0.3619]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.372830855776556e-06 from epoch 199\n",
      "MLP predicts 0.761 for case 63 with [0.74]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15777304768562317 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.157773\n",
      "0.001\n",
      "Best training loss: 0.031212788075208664 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.026235077530145645 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.026235\n",
      "0.001\n",
      "Best training loss: 0.01360236294567585 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.008511903695762157 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.008512\n",
      "0.001\n",
      "Best training loss: 0.006012944504618645 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.004593608435243368 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.004594\n",
      "0.001\n",
      "Best training loss: 0.003645465476438403 at learning rate 0.001 and epoch 35\n",
      "Best training loss: 0.0025590036530047655 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.002929\n",
      "0.001\n",
      "Best training loss: 0.0015715195331722498 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0011220278684049845 at learning rate 0.001 and epoch 48\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001872\n",
      "0.001\n",
      "Best training loss: 0.000882812193594873 at learning rate 0.001 and epoch 53\n",
      "Best training loss: 0.0007687184843234718 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.0007181073306128383 at learning rate 0.001 and epoch 58\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001049\n",
      "0.001\n",
      "Best training loss: 0.0005144029855728149 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.00037353564403019845 at learning rate 0.001 and epoch 66\n",
      "Best training loss: 0.00037007348146289587 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000518\n",
      "0.001\n",
      "Best training loss: 0.0002838142972905189 at learning rate 0.001 and epoch 71\n",
      "Best training loss: 0.0002466556616127491 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.0002215905551565811 at learning rate 0.001 and epoch 76\n",
      "Best training loss: 0.00017637317068874836 at learning rate 0.001 and epoch 79\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000242\n",
      "0.001\n",
      "Best training loss: 0.00017555350495968014 at learning rate 0.001 and epoch 81\n",
      "Best training loss: 0.00017427842249162495 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00013371532259043306 at learning rate 0.001 and epoch 84\n",
      "Best training loss: 0.00012397282989695668 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00010509042476769537 at learning rate 0.001 and epoch 89\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000120\n",
      "0.001\n",
      "Best training loss: 9.246172703569755e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 8.410635928157717e-05 at learning rate 0.001 and epoch 94\n",
      "Best training loss: 7.149382872739807e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 6.770234904251993e-05 at learning rate 0.001 and epoch 99\n",
      "Best training loss: 6.598457548534498e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000066\n",
      "0.001\n",
      "Best training loss: 5.673942359862849e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.4673480917699635e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 5.117782347952016e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.5831700845155865e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.4221167627256364e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 4.0660088416188955e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000041\n",
      "0.001\n",
      "Best training loss: 3.742983972188085e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.706724601215683e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.592089706216939e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.2952164474409074e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.083276533288881e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.987588413816411e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.9336051738937385e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.7136919015902095e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000027\n",
      "0.001\n",
      "Best training loss: 2.7098034479422495e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.5601426386856474e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.4524269974790514e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.422069519525394e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.2665615688310936e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.2339216229738668e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.1467843907885253e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.0489449525484815e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.0211531591485254e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.9164528566761874e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000019\n",
      "0.001\n",
      "Best training loss: 1.8726836060523055e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.816727854020428e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.7373240552842617e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.71013471117476e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.6386580682592466e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.593602792127058e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.5564262866973877e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4950382137612905e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4672902580059599e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.4194974937709048e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3790379853162449e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3503975424100645e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.3051152564003132e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.279214757232694e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.245188650500495e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.211362086905865e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1889450433955062e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.155230165750254e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1317810276523232e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.107476964534726e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.079988942365162e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0607001968310215e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0358395229559392e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0154377378057688e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.962731382984202e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.746024261403363e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.584357030689716e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.392601896252017e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.2199206846999e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.069779480341822e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.89592138264561e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.755114322411828e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.607275958638638e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.462066034553573e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.335482561960816e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.19745946500916e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.077345228230115e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.956745321280323e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.837007615307812e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.732171070529148e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.619254120072583e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.517002813983709e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.419902885885676e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.319820724660531e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.230533810798079e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.140059551602462e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.053895387798548e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.97231052981806e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.890589247632306e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.815510460000951e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.739841865055496e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.667365141765913e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.59975148664671e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.531168764922768e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.466977538366336e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.4050145738292485e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.344357643683907e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.287647920544259e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.231880433915649e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.179378488013754e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.128685981821036e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.080060757085448e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.034603757143486e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.9896874518017285e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.9469784901011735e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.9064245760964695e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.866242645424791e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.828318990097614e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.791793228127062e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9072, 0.5569, 0.9513, 0.8868, 0.5916, 0.7408, 0.7028, 0.6886,\n",
      "         0.8743, 0.6625, 0.6967, 0.8849, 0.7430, 0.8014, 0.6673, 0.6936, 0.4284,\n",
      "         0.1905, 0.9250, 0.5522, 0.6726, 0.8710, 0.5178, 0.7416, 0.8441, 0.4694,\n",
      "         0.8502, 0.6637, 0.5289, 0.4847, 0.7444, 0.4865, 0.2676, 0.9645, 0.5604,\n",
      "         0.8195, 0.6541, 0.5272, 0.1982, 0.8390, 0.1504, 0.5028, 0.0894, 0.3617]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.791793228127062e-06 from epoch 199\n",
      "MLP predicts 0.725 for case 66 with [0.5]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15598571300506592 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.155986\n",
      "0.001\n",
      "Best training loss: 0.02991541288793087 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.026537291705608368 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.035587\n",
      "0.001\n",
      "Best training loss: 0.02149980328977108 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.01988658681511879 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.01622702367603779 at learning rate 0.001 and epoch 17\n",
      "Best training loss: 0.011963404715061188 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.011963\n",
      "0.001\n",
      "Best training loss: 0.011840357445180416 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.00736368028447032 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.004640371538698673 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.004640\n",
      "0.001\n",
      "Best training loss: 0.0029621843714267015 at learning rate 0.001 and epoch 35\n",
      "Best training loss: 0.0019585750997066498 at learning rate 0.001 and epoch 40\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.001959\n",
      "0.001\n",
      "Best training loss: 0.0013863140484318137 at learning rate 0.001 and epoch 45\n",
      "Best training loss: 0.0010393309639766812 at learning rate 0.001 and epoch 50\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001039\n",
      "0.001\n",
      "Best training loss: 0.0007989447913132608 at learning rate 0.001 and epoch 55\n",
      "Best training loss: 0.0006989810499362648 at learning rate 0.001 and epoch 58\n",
      "Best training loss: 0.0006196824251674116 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000620\n",
      "0.001\n",
      "Best training loss: 0.000480791786685586 at learning rate 0.001 and epoch 63\n",
      "Best training loss: 0.00047813320998102427 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.0003422591835260391 at learning rate 0.001 and epoch 68\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000363\n",
      "0.001\n",
      "Best training loss: 0.000250829296419397 at learning rate 0.001 and epoch 73\n",
      "Best training loss: 0.0002446126891300082 at learning rate 0.001 and epoch 76\n",
      "Best training loss: 0.0001896814355859533 at learning rate 0.001 and epoch 78\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000208\n",
      "0.001\n",
      "Best training loss: 0.00017321307677775621 at learning rate 0.001 and epoch 81\n",
      "Best training loss: 0.00014673668192699552 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.00012762170808855444 at learning rate 0.001 and epoch 86\n",
      "Best training loss: 0.0001149568342952989 at learning rate 0.001 and epoch 88\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000120\n",
      "0.001\n",
      "Best training loss: 9.69792454270646e-05 at learning rate 0.001 and epoch 91\n",
      "Best training loss: 9.071543900063261e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 7.550303416792303e-05 at learning rate 0.001 and epoch 96\n",
      "Best training loss: 7.198527600849047e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.953167758183554e-05 at learning rate 0.001 and epoch 99\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000071\n",
      "0.001\n",
      "Best training loss: 5.977572072879411e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 5.736849925597198e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.3968862630426884e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 4.795413769898005e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.602392436936498e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.276955587556586e-05 at learning rate 0.001 and epoch 109\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000044\n",
      "0.001\n",
      "Best training loss: 3.8885675166966394e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.7157729821046814e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.4461998438928276e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.182986984029412e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.1345916795544326e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.0303164749057032e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.821542693709489e-05 at learning rate 0.001 and epoch 119\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000028\n",
      "0.001\n",
      "Best training loss: 2.6336238079238683e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.56342191278236e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.496874913049396e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.3430977307725698e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.3254933694261126e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.2012949557392858e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.1292904420988634e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.0840350771322846e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.9716906535904855e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.942612652783282e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000019\n",
      "0.001\n",
      "Best training loss: 1.861734017438721e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.797138429537881e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.7605232642381452e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.680331479292363e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.648387660679873e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.592053558852058e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.538490323582664e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.5092044122866355e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.451078242098447e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.4201950762071647e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3811825738230255e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3382474207901396e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.3129175385984126e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2715551747533027e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2442664228728972e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2148614587204065e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1813656783488113e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1601099686231464e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1291510418232065e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.1055540198867675e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0833573469426483e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.056972541846335e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0384492270532064e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0154254596272949e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.954854249372147e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.773972124094144e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.567307643010281e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.409823178430088e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.227342161466368e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.060343472810928e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.914555110095534e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.748694199312013e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.613219506514724e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.47098544909386e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.33270951261511e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.209942279790994e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.078058272076305e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.963836651470046e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.846978405723348e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.733035090495832e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.631592779944185e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.523340173065662e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.425763669743901e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.331171218538657e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.2366069616691675e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.151565569074592e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.065467343636556e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.98587928127381e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.90879232934094e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.833263796579558e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.763646069884999e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.692363967886195e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.6253446675546e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.562377166119404e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.4996524997695815e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.441155164793599e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.3837223933660425e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.3286170188803226e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.2761596382188145e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.225415745575447e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.177995601319708e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.1309347074711695e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.085929271648638e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.043237135600066e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.001622750773095e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.962349860055838e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.925216100877151e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.889664862479549e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.855863946635509e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9072, 0.5570, 0.9513, 0.8867, 0.5910, 0.7405, 0.5014, 0.6885,\n",
      "         0.8744, 0.6622, 0.6965, 0.8850, 0.7430, 0.8015, 0.6674, 0.6935, 0.4283,\n",
      "         0.1905, 0.9250, 0.5521, 0.6727, 0.8711, 0.5180, 0.7414, 0.8438, 0.4694,\n",
      "         0.8504, 0.6638, 0.5290, 0.4848, 0.7445, 0.4866, 0.2674, 0.9645, 0.5603,\n",
      "         0.8192, 0.6541, 0.5274, 0.1982, 0.8393, 0.1502, 0.5026, 0.0895, 0.3618]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.855863946635509e-06 from epoch 199\n",
      "MLP predicts 0.648 for case 86 with [0.7]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15662012994289398 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.156620\n",
      "0.001\n",
      "Best training loss: 0.02859492413699627 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.021065177395939827 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.056315\n",
      "0.001\n",
      "Best training loss: 0.014636784791946411 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.010412846691906452 at learning rate 0.001 and epoch 17\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.025524\n",
      "0.001\n",
      "Best training loss: 0.007479209452867508 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.005371525418013334 at learning rate 0.001 and epoch 27\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.011427\n",
      "0.001\n",
      "Best training loss: 0.0038099316880106926 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.0026618526317179203 at learning rate 0.001 and epoch 37\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.004814\n",
      "0.001\n",
      "Best training loss: 0.0018612992716953158 at learning rate 0.001 and epoch 42\n",
      "Best training loss: 0.0013289294438436627 at learning rate 0.001 and epoch 47\n",
      "Best training loss: 0.0013180969981476665 at learning rate 0.001 and epoch 49\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002047\n",
      "0.001\n",
      "Best training loss: 0.0009605813538655639 at learning rate 0.001 and epoch 52\n",
      "Best training loss: 0.0009331009932793677 at learning rate 0.001 and epoch 54\n",
      "Best training loss: 0.000700318836607039 at learning rate 0.001 and epoch 57\n",
      "Best training loss: 0.0006726245628669858 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000915\n",
      "0.001\n",
      "Best training loss: 0.0005152958910912275 at learning rate 0.001 and epoch 62\n",
      "Best training loss: 0.0004901195643469691 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00037997556501068175 at learning rate 0.001 and epoch 67\n",
      "Best training loss: 0.00035809745895676315 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000431\n",
      "0.001\n",
      "Best training loss: 0.000280520151136443 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.0002634740376379341 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.00021001353161409497 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00019730263738892972 at learning rate 0.001 and epoch 79\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000217\n",
      "0.001\n",
      "Best training loss: 0.00016001852054614574 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00015015796816442162 at learning rate 0.001 and epoch 84\n",
      "Best training loss: 0.00012372089258860797 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00011590233043534681 at learning rate 0.001 and epoch 89\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000120\n",
      "0.001\n",
      "Best training loss: 9.680871880846098e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 9.051524102687836e-05 at learning rate 0.001 and epoch 94\n",
      "Best training loss: 7.657377136638388e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 7.148436998249963e-05 at learning rate 0.001 and epoch 99\n",
      "Best training loss: 7.137712964322418e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000071\n",
      "0.001\n",
      "Best training loss: 7.00916352798231e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 6.110698450356722e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.697688175132498e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 5.624989717034623e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 5.509749462362379e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.921101208310574e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.5869339373894036e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 4.4940195948584005e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000045\n",
      "0.001\n",
      "Best training loss: 4.3858417484443635e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.995864244643599e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.72488284483552e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.630968421930447e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.539661702234298e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.274861956015229e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.0581679311580956e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.9737306249444373e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000030\n",
      "0.001\n",
      "Best training loss: 2.8940545234945603e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.712896093726158e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.6950900064548478e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.5396564524271525e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.4667548132129014e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.401619167358149e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.273522477480583e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.2430414901464246e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.134720671165269e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 2.073581526929047e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000021\n",
      "0.001\n",
      "Best training loss: 2.0178898921585642e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.9264394722995348e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.893713852041401e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.8144832210964523e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.763805266818963e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.7187434423249215e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.6513376976945437e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.6192572729778476e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.561704084451776e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.5204156625259202e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000015\n",
      "0.001\n",
      "Best training loss: 1.4826042388449423e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.4327612007036805e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.404354134137975e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.360443729936378e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.3268193470139522e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2964212146471255e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.2583266652654856e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.2332211554166861e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.2001714821963105e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.1730177902791183e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000012\n",
      "0.001\n",
      "Best training loss: 1.1477858606667724e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.118789441534318e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0977689271385316e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0717879376898054e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.0497145012777764e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.0293573723174632e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 1.0064973139378708e-05 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.885442523227539e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.682589734438807e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.502176908426918e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 9.332046829513274e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 9.151592166745104e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 9.001121725304984e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.835702828946523e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.686396540724672e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.546946446585935e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.400061233260203e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.273214007203933e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.141520993376616e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 8.019502274692059e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.902820470917504e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.785154593875632e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.679837835894432e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.570907655463088e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.469666343240533e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.374097094725585e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.27773976905155e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.189911229943391e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.102355539245764e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.019492386461934e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.94013579050079e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.862284863018431e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.7912033046013676e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.720127657899866e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.653114269283833e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.589364147657761e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.525703611259814e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.466123522841372e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.408292392734438e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.3533261709380895e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.300202130660182e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.2483222791343e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.199460585776251e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.151894012873527e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.106508863013005e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.063668024580693e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.0220982049941085e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.982430138828931e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.944623808318283e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9072, 0.5571, 0.9515, 0.8871, 0.5912, 0.7411, 0.5014, 0.7032,\n",
      "         0.8744, 0.6623, 0.6964, 0.8852, 0.7429, 0.8015, 0.6674, 0.6936, 0.4283,\n",
      "         0.1906, 0.9247, 0.5520, 0.6726, 0.8710, 0.5179, 0.7417, 0.8439, 0.4693,\n",
      "         0.8504, 0.6638, 0.5291, 0.4848, 0.7446, 0.4864, 0.2675, 0.9648, 0.5602,\n",
      "         0.8193, 0.6540, 0.5274, 0.1984, 0.8392, 0.1501, 0.5027, 0.0894, 0.3617]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.944623808318283e-06 from epoch 199\n",
      "MLP predicts 0.899 for case 69 with [0.69]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15218417346477509 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.152184\n",
      "0.001\n",
      "Best training loss: 0.03058951161801815 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.019354837015271187 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.019355\n",
      "0.001\n",
      "Best training loss: 0.011145779862999916 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.00886810477823019 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.008868\n",
      "0.001\n",
      "Best training loss: 0.00775823462754488 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.005505172070115805 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.006703\n",
      "0.001\n",
      "Best training loss: 0.003153193974867463 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.002158532617613673 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.004459\n",
      "0.001\n",
      "Best training loss: 0.0017469997983425856 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0014184217434376478 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002518\n",
      "0.001\n",
      "Best training loss: 0.0009177410393022001 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0006948928348720074 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.0006712832837365568 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001190\n",
      "0.001\n",
      "Best training loss: 0.0005707203526981175 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.0004434181610122323 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00032221368746832013 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000500\n",
      "0.001\n",
      "Best training loss: 0.0003106816147919744 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00024960891460068524 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.00021338323131203651 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00019995243928860873 at learning rate 0.001 and epoch 79\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000214\n",
      "0.001\n",
      "Best training loss: 0.00015834021905902773 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00014991586795076728 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00012405692541506141 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00011058613745262846 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000111\n",
      "0.001\n",
      "Best training loss: 9.997232700698078e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 8.551203791284934e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 8.140232239384204e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 7.906007522251457e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.82958634570241e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000068\n",
      "0.001\n",
      "Best training loss: 6.631278665736318e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 6.147380190668628e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.5606127716600895e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 5.3997206123312935e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.9140020564664155e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.5753120502922684e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000046\n",
      "0.001\n",
      "Best training loss: 4.4790274841943756e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 4.396731674205512e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 4.009378608316183e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.786440356634557e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.623242082539946e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.597848262870684e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.320906398585066e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 3.296148497611284e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 3.152469435008243e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000032\n",
      "0.001\n",
      "Best training loss: 2.987585321534425e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.9654191166628152e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.783086893032305e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.721660894167144e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.6402265575597994e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.5023593480000272e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.4727674826863222e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.3545604562968947e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.2815060219727457e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 2.2313091903924942e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000022\n",
      "0.001\n",
      "Best training loss: 2.125278115272522e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 2.087072243739385e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 2.011928881984204e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.9425609934842214e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.9039120161323808e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.8272976376465522e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.78660611709347e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.7353626390104182e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.676616375334561e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.644362782826647e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000016\n",
      "0.001\n",
      "Best training loss: 1.5889507267274894e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.549811349832453e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.5136673937377054e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.4664328773505986e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.4372528312378563e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.3978730748931412e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.363237424811814e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.3349438631848898e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.298602182941977e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.2728712135867681e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000013\n",
      "0.001\n",
      "Best training loss: 1.2433817573764827e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.2142852938268334e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.1917300980712753e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.1635855116765015e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.1410679690015968e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.1188847565790638e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 1.0950063369818963e-05 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 1.0759591532405466e-05 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 1.0544906217546668e-05 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 1.0352427125326358e-05 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 1.0173487680731341e-05 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 9.981109542422928e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 9.819385923037771e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 9.647202205087524e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 9.483803296461701e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 9.337203664472327e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 9.178821528621484e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 9.038981261255685e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.90059982339153e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 8.762161996855866e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.637211067252792e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 8.508806786267087e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 8.38929190649651e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 8.273575076600537e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 8.15848761703819e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 8.05371473688865e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.947654921736103e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.846925655030645e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.752133569738362e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.65720415074611e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.569147328467807e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 7.482944965886418e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 7.3989867814816535e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 7.319318683585152e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 7.240934792207554e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 7.166755040088901e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 7.094540251273429e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 7.024349542916752e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.957749519642675e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.8913604991394095e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.828286586824106e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.768041657778667e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.708452474413207e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.651355761277955e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.59597071717144e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.542514256580034e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.4906926127150655e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.440167453547474e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.392116574716056e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9074, 0.5573, 0.9508, 0.8860, 0.5901, 0.7427, 0.5019, 0.7029,\n",
      "         0.6892, 0.6629, 0.6958, 0.8846, 0.7424, 0.8014, 0.6674, 0.6937, 0.4281,\n",
      "         0.1904, 0.9241, 0.5522, 0.6727, 0.8706, 0.5178, 0.7411, 0.8433, 0.4690,\n",
      "         0.8505, 0.6640, 0.5289, 0.4851, 0.7444, 0.4868, 0.2671, 0.9636, 0.5604,\n",
      "         0.8191, 0.6535, 0.5270, 0.1975, 0.8395, 0.1502, 0.5020, 0.0895, 0.3618]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 6.392116574716056e-06 from epoch 199\n",
      "MLP predicts 0.436 for case 72 with [0.88]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1555345356464386 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.155535\n",
      "0.001\n",
      "Best training loss: 0.03285883739590645 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.018132183700799942 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.018132\n",
      "0.001\n",
      "Best training loss: 0.012188980355858803 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.0111484844237566 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.011148\n",
      "0.001\n",
      "Best training loss: 0.008971755392849445 at learning rate 0.001 and epoch 23\n",
      "Best training loss: 0.004810419399291277 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.009028\n",
      "0.001\n",
      "Best training loss: 0.003276023082435131 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.002702763769775629 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.005697\n",
      "0.001\n",
      "Best training loss: 0.0020200912840664387 at learning rate 0.001 and epoch 41\n",
      "Best training loss: 0.0012739357771351933 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002860\n",
      "0.001\n",
      "Best training loss: 0.0010084498208016157 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.000886737834662199 at learning rate 0.001 and epoch 54\n",
      "Best training loss: 0.0008743256330490112 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.0005918872193433344 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001166\n",
      "0.001\n",
      "Best training loss: 0.0004574396298266947 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00040783770964480937 at learning rate 0.001 and epoch 67\n",
      "Best training loss: 0.00037414830876514316 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000439\n",
      "0.001\n",
      "Best training loss: 0.00028449809178709984 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00028084119549021125 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00021734308393206447 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.0001931940932990983 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000193\n",
      "0.001\n",
      "Best training loss: 0.00017486394790466875 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00014374649617820978 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00014264299534261227 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.0001351151877315715 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 0.0001132136894739233 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000113\n",
      "0.001\n",
      "Best training loss: 0.0001009170082397759 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 9.175124432658777e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.876062591094524e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 7.499782805098221e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000075\n",
      "0.001\n",
      "Best training loss: 7.193723286036402e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 6.333507189992815e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 6.126733933342621e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 5.6530505389673635e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 5.183708344702609e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 5.158212297828868e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 5.000565943191759e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000050\n",
      "0.001\n",
      "Best training loss: 4.56027701147832e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 4.279786298866384e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 4.1265611798735335e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 4.078288475284353e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.746401489479467e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.740450119948946e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.548332824721001e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 3.365838711033575e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 3.3447493478888646e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000033\n",
      "0.001\n",
      "Best training loss: 3.119583925581537e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 3.058248330489732e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.960428719234187e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.7976890123682097e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.7677846446749754e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.6269299269188195e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.544401831983123e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.4864588340278715e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.360845792281907e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 2.320155363122467e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000023\n",
      "0.001\n",
      "Best training loss: 2.2312036890070885e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 2.1491718143806793e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 2.1081466911709867e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 2.016469261434395e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.969010554603301e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.9122693629469723e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.8414142687106505e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.8049551727017388e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.7414318790542893e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.6948170014075004e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000017\n",
      "0.001\n",
      "Best training loss: 1.654277730267495e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.5986210200935602e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.5656010873499326e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.5205081581370905e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.4791061403229833e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.4485460269497707e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.4058706256037112e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.3755898180534132e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.3433604181045666e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.3085324098938145e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000013\n",
      "0.001\n",
      "Best training loss: 1.2828968465328217e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.2511144632298965e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.224524385179393e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.1996508874290157e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.17180288725649e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.150194657384418e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 1.1256745892751496e-05 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 1.102903661376331e-05 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 1.0832203770405613e-05 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 1.0608182492433116e-05 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0420251783216372e-05 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 1.0230593943560962e-05 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 1.003876514005242e-05 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 9.873057024378795e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 9.695676453702617e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 9.535312528896611e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 9.380293704452924e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 9.222265362041071e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 9.08338824956445e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 8.938209248299245e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.801088370091747e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 8.674498531036079e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 8.543551302864216e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 8.424988664046396e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 8.308796168421395e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 8.194874681066722e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 8.089773473329842e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.984207513800357e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.885158993303776e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.789435585436877e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.695681233599316e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 7.609181011503097e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 7.5227271736366674e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 7.440634362865239e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 7.362809355981881e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 7.285294486791827e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 7.212551736301975e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 7.141970399970887e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 7.073161214066204e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 7.007969543337822e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.944020242372062e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.882844445499359e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.822852355981013e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.764439149264945e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.7088144533045124e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.6537568272906356e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.600515007448848e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.549407771672122e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.49956473353086e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8316, 0.9072, 0.5568, 0.9513, 0.8868, 0.5912, 0.7417, 0.5013, 0.7028,\n",
      "         0.6886, 0.8743, 0.6963, 0.8847, 0.7427, 0.8006, 0.6674, 0.6938, 0.4279,\n",
      "         0.1906, 0.9238, 0.5519, 0.6727, 0.8708, 0.5179, 0.7414, 0.8434, 0.4691,\n",
      "         0.8506, 0.6639, 0.5289, 0.4853, 0.7447, 0.4870, 0.2679, 0.9646, 0.5601,\n",
      "         0.8185, 0.6540, 0.5277, 0.1982, 0.8385, 0.1503, 0.5024, 0.0894, 0.3614]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 6.49956473353086e-06 from epoch 199\n",
      "MLP predicts 0.426 for case 80 with [0.67]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1557995229959488 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.155800\n",
      "0.001\n",
      "Best training loss: 0.030325185507535934 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.028764301910996437 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.031610\n",
      "0.001\n",
      "Best training loss: 0.02461394853889942 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.01694723591208458 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.010046271607279778 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.010046\n",
      "0.001\n",
      "Best training loss: 0.006270811427384615 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.004115805495530367 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.004116\n",
      "0.001\n",
      "Best training loss: 0.0028142426162958145 at learning rate 0.001 and epoch 35\n",
      "Best training loss: 0.002028759801760316 at learning rate 0.001 and epoch 40\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.002029\n",
      "0.001\n",
      "Best training loss: 0.0015609413385391235 at learning rate 0.001 and epoch 45\n",
      "Best training loss: 0.0013151798630133271 at learning rate 0.001 and epoch 48\n",
      "Best training loss: 0.0012364533031359315 at learning rate 0.001 and epoch 50\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001236\n",
      "0.001\n",
      "Best training loss: 0.0008557758410461247 at learning rate 0.001 and epoch 53\n",
      "Best training loss: 0.0006006580661050975 at learning rate 0.001 and epoch 58\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000758\n",
      "0.001\n",
      "Best training loss: 0.000446517689852044 at learning rate 0.001 and epoch 63\n",
      "Best training loss: 0.0004365702625364065 at learning rate 0.001 and epoch 66\n",
      "Best training loss: 0.00033932531368918717 at learning rate 0.001 and epoch 68\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000427\n",
      "0.001\n",
      "Best training loss: 0.0002915265504270792 at learning rate 0.001 and epoch 71\n",
      "Best training loss: 0.00026013836031779647 at learning rate 0.001 and epoch 73\n",
      "Best training loss: 0.0002053487696684897 at learning rate 0.001 and epoch 76\n",
      "Best training loss: 0.00020155496895313263 at learning rate 0.001 and epoch 78\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000228\n",
      "0.001\n",
      "Best training loss: 0.00015223569062072784 at learning rate 0.001 and epoch 81\n",
      "Best training loss: 0.0001454573794035241 at learning rate 0.001 and epoch 84\n",
      "Best training loss: 0.00011689888924593106 at learning rate 0.001 and epoch 86\n",
      "Best training loss: 0.00010579799709375948 at learning rate 0.001 and epoch 89\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000122\n",
      "0.001\n",
      "Best training loss: 9.173104626825079e-05 at learning rate 0.001 and epoch 91\n",
      "Best training loss: 7.976499910000712e-05 at learning rate 0.001 and epoch 94\n",
      "Best training loss: 7.280171848833561e-05 at learning rate 0.001 and epoch 96\n",
      "Best training loss: 6.186233804328367e-05 at learning rate 0.001 and epoch 99\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000068\n",
      "0.001\n",
      "Best training loss: 5.8153036661678925e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 5.694608626072295e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 4.896696918876842e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 4.6648925490444526e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.423350401339121e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 3.936169741791673e-05 at learning rate 0.001 and epoch 109\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000040\n",
      "0.001\n",
      "Best training loss: 3.763306449400261e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.511878821882419e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.201604704372585e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.1978473998606205e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.053796535823494e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 2.8396778361639008e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.6323041311115958e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.5800098228501156e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000026\n",
      "0.001\n",
      "Best training loss: 2.5039918909897096e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.3369191694655456e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.3314885766012594e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.190131817769725e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.1234494852251373e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.0758321625180542e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 1.9534692910383455e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.9331333533045836e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.842315396061167e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.777124998625368e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000018\n",
      "0.001\n",
      "Best training loss: 1.744338806020096e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.6558098650421016e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.627212986932136e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.569079904584214e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.5124635865504388e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.4848562386760022e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.4226974599296227e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.3928115549788345e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.352306662738556e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.3064484846836422e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000013\n",
      "0.001\n",
      "Best training loss: 1.2832050742872525e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.2386178241285961e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2106024769309442e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.182248888653703e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.1461106623755768e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.125350081565557e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.0938849300146103e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.0694800039345864e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.0476003808435053e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0200422366324347e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 1.0023192771768663e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 9.788620445760898e-06 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 9.583458449924365e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.415202839591075e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.200949534715619e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.046078957908321e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 8.872355465427972e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 8.70311396283796e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 8.5642168414779e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.401052582485136e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 8.269991667475551e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.13245787867345e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 7.995419764483813e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 7.882355930632912e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 7.752354576950893e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 7.640778676432092e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.532785730290925e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.420214387821034e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.324381840589922e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.222815384011483e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.12980863681878e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.041070603008848e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 6.950320766918594e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 6.870436209283071e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 6.786679477954749e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 6.708778528263792e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.636469151999336e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.5617227846814785e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.494623448816128e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.428372671507532e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.36390950603527e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.3031952777237166e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.24253061687341e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.1863806877227034e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.131156624178402e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.078141268517356e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.0291276895441115e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 5.979937213851372e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 5.934060482104542e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 5.890498414373724e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 5.848134151165141e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 5.808344667457277e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 5.769661129306769e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.7327379181515425e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.697278083971469e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.663031060976209e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.630572104564635e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.598326424660627e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.567643256654264e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9070, 0.5567, 0.9510, 0.8866, 0.5913, 0.7399, 0.5010, 0.7032,\n",
      "         0.6884, 0.8743, 0.6625, 0.8849, 0.7429, 0.8015, 0.6674, 0.6934, 0.4282,\n",
      "         0.1903, 0.9252, 0.5519, 0.6726, 0.8708, 0.5180, 0.7413, 0.8439, 0.4693,\n",
      "         0.8504, 0.6637, 0.5290, 0.4847, 0.7444, 0.4866, 0.2674, 0.9645, 0.5603,\n",
      "         0.8194, 0.6542, 0.5272, 0.1985, 0.8392, 0.1502, 0.5026, 0.0895, 0.3617]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.567643256654264e-06 from epoch 199\n",
      "MLP predicts 0.576 for case 81 with [0.7]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15315628051757812 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.153156\n",
      "0.001\n",
      "Best training loss: 0.029286371544003487 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.023023642599582672 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.075701\n",
      "0.001\n",
      "Best training loss: 0.019031399860978127 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.01680869236588478 at learning rate 0.001 and epoch 17\n",
      "Best training loss: 0.011454707011580467 at learning rate 0.001 and epoch 19\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.038574\n",
      "0.001\n",
      "Best training loss: 0.006582802161574364 at learning rate 0.001 and epoch 24\n",
      "Best training loss: 0.004402281250804663 at learning rate 0.001 and epoch 29\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.017092\n",
      "0.001\n",
      "Best training loss: 0.003226796630769968 at learning rate 0.001 and epoch 34\n",
      "Best training loss: 0.0024321814998984337 at learning rate 0.001 and epoch 39\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.006760\n",
      "0.001\n",
      "Best training loss: 0.002190195256844163 at learning rate 0.001 and epoch 41\n",
      "Best training loss: 0.001857343246228993 at learning rate 0.001 and epoch 44\n",
      "Best training loss: 0.0013919782359153032 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002641\n",
      "0.001\n",
      "Best training loss: 0.0009501281892880797 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0006770585896447301 at learning rate 0.001 and epoch 56\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001065\n",
      "0.001\n",
      "Best training loss: 0.0004964434774592519 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.00036707488470710814 at learning rate 0.001 and epoch 66\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000451\n",
      "0.001\n",
      "Best training loss: 0.0002721505588851869 at learning rate 0.001 and epoch 71\n",
      "Best training loss: 0.0002644966298248619 at learning rate 0.001 and epoch 73\n",
      "Best training loss: 0.00020461654639802873 at learning rate 0.001 and epoch 76\n",
      "Best training loss: 0.00018946442287415266 at learning rate 0.001 and epoch 78\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000209\n",
      "0.001\n",
      "Best training loss: 0.0001564609119668603 at learning rate 0.001 and epoch 81\n",
      "Best training loss: 0.00014026813732925802 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.00012077559222234413 at learning rate 0.001 and epoch 86\n",
      "Best training loss: 0.00010625461436575279 at learning rate 0.001 and epoch 88\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000108\n",
      "0.001\n",
      "Best training loss: 9.39634264796041e-05 at learning rate 0.001 and epoch 91\n",
      "Best training loss: 8.200504817068577e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 8.112397335935384e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.35447829356417e-05 at learning rate 0.001 and epoch 96\n",
      "Best training loss: 6.41626538708806e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.199755443958566e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000062\n",
      "0.001\n",
      "Best training loss: 5.7980425481218845e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 5.081041672383435e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 4.8249152314383537e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.6047669457038864e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.0686882130103186e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 3.8225211028475314e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000038\n",
      "0.001\n",
      "Best training loss: 3.6934819945599884e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.640648355940357e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.292990368208848e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.0743245588382706e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 2.991405017382931e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 2.9181923309806734e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.696617229958065e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.513961044314783e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000025\n",
      "0.001\n",
      "Best training loss: 2.4560691599617712e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.3795675588189624e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.2369846192304976e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.224886520707514e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.0878325813100673e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.0432788005564362e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 1.9766072000493295e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.8811058907886036e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.856839116953779e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.7609885617275722e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000018\n",
      "0.001\n",
      "Best training loss: 1.7251250028493814e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.6676389350323007e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.6023490388761275e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.5758716472191736e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.506742319179466e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.4766247659281362e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.430932115908945e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.384495772072114e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.3580392078438308e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.3088180821796414e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000013\n",
      "0.001\n",
      "Best training loss: 1.2839632290706504e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.2470114597817883e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2133824384363834e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.1903640370292123e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.154085748567013e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1330994311720133e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1046824511140585e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.0794817171699833e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.0591714271868113e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.032568616210483e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 1.0152347385883331e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 9.924393452820368e-06 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 9.729968041938264e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.560891157889273e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.355823749501724e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.206901268044021e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.028625754581299e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 8.874363629729487e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 8.729138244234491e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.571288162784185e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.447534128208645e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.303082722704858e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.178789357771166e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.06030129751889e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 7.934393579489551e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 7.828398338460829e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.71315080783097e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.610873126395745e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.50937033444643e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.408003966702381e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.319431915675523e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.2239781729876995e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.138889941415982e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.055364221741911e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 6.972168648644583e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 6.8963631747465115e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.81872643326642e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.747899078618502e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.677095370832831e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.608926014450844e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.545623818965396e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.480564479716122e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.420523732231231e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.3620336732128635e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.305220722424565e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.2514027376892045e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.198602477525128e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.150252829684177e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.1025248214718886e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.0572292568394914e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.014538030285621e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 5.9726844483520836e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 5.933533884672215e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.895275990042137e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.858664735569619e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.82328448217595e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.789318493043538e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.757520284532802e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.726786639570491e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9071, 0.5569, 0.9514, 0.8868, 0.5910, 0.7408, 0.5015, 0.7029,\n",
      "         0.6887, 0.8741, 0.6625, 0.6965, 0.7430, 0.8016, 0.6672, 0.6937, 0.4283,\n",
      "         0.1905, 0.9256, 0.5520, 0.6728, 0.8714, 0.5181, 0.7413, 0.8438, 0.4693,\n",
      "         0.8505, 0.6641, 0.5291, 0.4851, 0.7443, 0.4870, 0.2672, 0.9647, 0.5602,\n",
      "         0.8192, 0.6541, 0.5273, 0.1987, 0.8394, 0.1502, 0.5026, 0.0898, 0.3618]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.726786639570491e-06 from epoch 199\n",
      "MLP predicts 0.929 for case 77 with [0.88]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1550719290971756 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.155072\n",
      "0.001\n",
      "Best training loss: 0.0309432540088892 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.02584257163107395 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.025843\n",
      "0.001\n",
      "Best training loss: 0.013552773743867874 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.00857074186205864 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.008571\n",
      "0.001\n",
      "Best training loss: 0.006088011898100376 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.004638575483113527 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.004639\n",
      "0.001\n",
      "Best training loss: 0.003659743582829833 at learning rate 0.001 and epoch 35\n",
      "Best training loss: 0.002624155255034566 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.002924\n",
      "0.001\n",
      "Best training loss: 0.001617290312424302 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0011470598401501775 at learning rate 0.001 and epoch 48\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001858\n",
      "0.001\n",
      "Best training loss: 0.0008935140212997794 at learning rate 0.001 and epoch 53\n",
      "Best training loss: 0.0007897597970440984 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.0007223312277346849 at learning rate 0.001 and epoch 58\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001044\n",
      "0.001\n",
      "Best training loss: 0.0005276110605336726 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.000381240010028705 at learning rate 0.001 and epoch 66\n",
      "Best training loss: 0.0003778656537178904 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000519\n",
      "0.001\n",
      "Best training loss: 0.0002884969289880246 at learning rate 0.001 and epoch 71\n",
      "Best training loss: 0.0002519962436053902 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.00022474899014923722 at learning rate 0.001 and epoch 76\n",
      "Best training loss: 0.0001798228913685307 at learning rate 0.001 and epoch 79\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000244\n",
      "0.001\n",
      "Best training loss: 0.00017746193043421954 at learning rate 0.001 and epoch 81\n",
      "Best training loss: 0.00017662467143964022 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00013558236241806298 at learning rate 0.001 and epoch 84\n",
      "Best training loss: 0.0001251567155122757 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00010587604629108682 at learning rate 0.001 and epoch 89\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000120\n",
      "0.001\n",
      "Best training loss: 9.283268445869908e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 8.423409599345177e-05 at learning rate 0.001 and epoch 94\n",
      "Best training loss: 7.14234629413113e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 6.751222827006131e-05 at learning rate 0.001 and epoch 99\n",
      "Best training loss: 6.568391836481169e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000066\n",
      "0.001\n",
      "Best training loss: 5.64730835321825e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.434263221104629e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 5.078798494650982e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.548065771814436e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.38423958257772e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 4.026229726150632e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000040\n",
      "0.001\n",
      "Best training loss: 3.7074732972541824e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.6653815186582506e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.555738294380717e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.259676304878667e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.0517319828504696e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.9526227081078105e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.9021106456639245e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.684126593521796e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000027\n",
      "0.001\n",
      "Best training loss: 2.6784236979437992e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.5336641556350514e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.4244520318461582e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.395854244241491e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.242812297481578e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.2089663616498e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.125166247424204e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.0267858417355455e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.999677624553442e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.896954381663818e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000019\n",
      "0.001\n",
      "Best training loss: 1.8521241145208478e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.79820490302518e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.7188403944601305e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.6916523236432113e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.621736191737e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.5760402675368823e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.539830191177316e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4787863619858399e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4509172615362331e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.4042973816685844e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.363627779937815e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.335862725682091e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.291306944040116e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2655340469791554e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2324867384450044e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1988341611868236e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.176990735984873e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1439937225077301e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1207851457584184e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0972859854518902e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.070251801138511e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.051664639817318e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0277386536472477e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0077555089083035e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.894242793961894e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.682544259703718e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.525009772914927e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.3403705250239e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.171550118480809e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.02698957361281e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.85784174897708e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.72113469085889e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.578691449656617e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.437329597654752e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.31553461466683e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.182340025086887e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.066502232395578e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.951090083224699e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.835456017346587e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.735625331406482e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.628934326930903e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.532245490438072e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.4408944783499464e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.3462047112116124e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.261969130922807e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.177042789408006e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.095868568285368e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.019617896730779e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.942766049178317e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.872570793348132e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.802027201047167e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.734417638654122e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.671830760751618e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.607814157177927e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.54845780445612e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.491239673778182e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.435064733523177e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.3823413256614e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.330345968308393e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.2809781411488075e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.233169642655412e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.186438440636266e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.142575784906512e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.099081019783625e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.057340215193108e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.017845862515969e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.978867193334736e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.942004918324528e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.9062326727143954e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8312, 0.9070, 0.5568, 0.9514, 0.8866, 0.5912, 0.7407, 0.5014, 0.7029,\n",
      "         0.6886, 0.8740, 0.6623, 0.6964, 0.8846, 0.8014, 0.6673, 0.6934, 0.4280,\n",
      "         0.1903, 0.9251, 0.5519, 0.6727, 0.8712, 0.5181, 0.7413, 0.8438, 0.4693,\n",
      "         0.8505, 0.6638, 0.5291, 0.4848, 0.7443, 0.4867, 0.2673, 0.9645, 0.5603,\n",
      "         0.8191, 0.6542, 0.5272, 0.1984, 0.8392, 0.1503, 0.5026, 0.0898, 0.3618]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.9062326727143954e-06 from epoch 199\n",
      "MLP predicts 0.574 for case 64 with [0.74]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1539628803730011 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.153963\n",
      "0.001\n",
      "Best training loss: 0.02945931814610958 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.025152819231152534 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.037105\n",
      "0.001\n",
      "Best training loss: 0.019843509420752525 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.014747665263712406 at learning rate 0.001 and epoch 17\n",
      "Best training loss: 0.012788752093911171 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.012789\n",
      "0.001\n",
      "Best training loss: 0.010634366422891617 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.007873695343732834 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.007491248194128275 at learning rate 0.001 and epoch 27\n",
      "Best training loss: 0.004935501608997583 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.004936\n",
      "0.001\n",
      "Best training loss: 0.003103675553575158 at learning rate 0.001 and epoch 35\n",
      "Best training loss: 0.001994755817577243 at learning rate 0.001 and epoch 40\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.001995\n",
      "0.001\n",
      "Best training loss: 0.0013591598253697157 at learning rate 0.001 and epoch 45\n",
      "Best training loss: 0.0009841483552008867 at learning rate 0.001 and epoch 50\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.000984\n",
      "0.001\n",
      "Best training loss: 0.000736325397156179 at learning rate 0.001 and epoch 55\n",
      "Best training loss: 0.0005627732607536018 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000563\n",
      "0.001\n",
      "Best training loss: 0.0005051533808000386 at learning rate 0.001 and epoch 63\n",
      "Best training loss: 0.0004322665627114475 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.00035053069586865604 at learning rate 0.001 and epoch 68\n",
      "Best training loss: 0.0003287480503786355 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000329\n",
      "0.001\n",
      "Best training loss: 0.0002506179444026202 at learning rate 0.001 and epoch 73\n",
      "Best training loss: 0.0002498542598914355 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00018568213272374123 at learning rate 0.001 and epoch 78\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000191\n",
      "0.001\n",
      "Best training loss: 0.00018130055104847997 at learning rate 0.001 and epoch 81\n",
      "Best training loss: 0.0001416952145518735 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.00013154039334040135 at learning rate 0.001 and epoch 86\n",
      "Best training loss: 0.0001101535017369315 at learning rate 0.001 and epoch 88\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000113\n",
      "0.001\n",
      "Best training loss: 9.83957652351819e-05 at learning rate 0.001 and epoch 91\n",
      "Best training loss: 8.668802183819935e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 7.554466719739139e-05 at learning rate 0.001 and epoch 96\n",
      "Best training loss: 6.878491694806144e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.818440306233242e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000068\n",
      "0.001\n",
      "Best training loss: 5.911076732445508e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 5.490994226420298e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.43390451639425e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 5.353568121790886e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.7025747335283086e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.4170981709612533e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.269895362085663e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 4.240023190504871e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000042\n",
      "0.001\n",
      "Best training loss: 3.7915287975920364e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.577703682822175e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.414097591303289e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.400024797883816e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.0943279853090644e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 2.9276467103045434e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.7800662792287767e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.7615251383394934e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000028\n",
      "0.001\n",
      "Best training loss: 2.5582996386219747e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.531266727601178e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.4219576516770758e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.3010810764390044e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.2795975382905453e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.1418627511593513e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.099877383443527e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.0299867173889652e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.9344841348356567e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.9078444893239066e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000019\n",
      "0.001\n",
      "Best training loss: 1.8151979020331055e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.770942435541656e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.720908039715141e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.6483503713971004e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.621715273358859e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.5567238733638078e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.5157023881329224e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4796566574659664e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.424569018126931e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.3984278666612227e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3531770491681527e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3180786481825635e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2896644875581842e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2489644177549053e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2259705727046821e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1927781997655984e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1638901924015954e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1418105714255944e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1107130376331042e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0901197128987405e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0656484846549574e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0418714737170376e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0232499334961176e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.997506822401192e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.821335879678372e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.627867257222533e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.435813808522653e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.283529834647197e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.098360351345036e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.94789172889432e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.79780145623954e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.64120102050947e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.510672159900423e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.366254405700602e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.240090210165363e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.117369361571036e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.992689461389091e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.88659235695377e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.771485798002686e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.667810677958187e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.570378784294007e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.468141575373011e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.378112059086561e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.2857560553529765e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.1984204623731785e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.115260814316571e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.031244876998244e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.9552656896121334e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.8781432673858944e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.804948043281911e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.736454452038743e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.66654295855551e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.601560471608536e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.537265107908752e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.474226211139467e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.414388735720422e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.3555212364008185e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.299866527115228e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.245149052119814e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.192656201164937e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.143044629425276e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.093498086556792e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.046780981705524e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.0022280194971245e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.958529527561041e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.917199359828373e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.877036073798081e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.8389668993186206e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.802265150123276e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9073, 0.5569, 0.9514, 0.8864, 0.5909, 0.7402, 0.5015, 0.7030,\n",
      "         0.6886, 0.8745, 0.6622, 0.6965, 0.8849, 0.7430, 0.6674, 0.6934, 0.4283,\n",
      "         0.1904, 0.9252, 0.5522, 0.6727, 0.8709, 0.5179, 0.7416, 0.8438, 0.4693,\n",
      "         0.8503, 0.6638, 0.5290, 0.4847, 0.7444, 0.4866, 0.2674, 0.9645, 0.5603,\n",
      "         0.8189, 0.6542, 0.5278, 0.1982, 0.8393, 0.1503, 0.5022, 0.0895, 0.3619]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.802265150123276e-06 from epoch 199\n",
      "MLP predicts 0.625 for case 83 with [0.8]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15307164192199707 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.153072\n",
      "0.001\n",
      "Best training loss: 0.031196773052215576 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.02162022888660431 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.021620\n",
      "0.001\n",
      "Best training loss: 0.011995886452496052 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.00908795464783907 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.009088\n",
      "0.001\n",
      "Best training loss: 0.007976476103067398 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.005668953992426395 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.007075\n",
      "0.001\n",
      "Best training loss: 0.003233672585338354 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.002299071056768298 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.004839\n",
      "0.001\n",
      "Best training loss: 0.0019287598552182317 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0013284869492053986 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002643\n",
      "0.001\n",
      "Best training loss: 0.0009058435098268092 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0007259463309310377 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.000597717531491071 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001166\n",
      "0.001\n",
      "Best training loss: 0.00040642375824972987 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.0003072860708925873 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000444\n",
      "0.001\n",
      "Best training loss: 0.00026290712412446737 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.0002450152824167162 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.0001845947263063863 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00017766596283763647 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000178\n",
      "0.001\n",
      "Best training loss: 0.00014117431419435889 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00012377026723697782 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00011286533117527142 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 9.233087621396407e-05 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000092\n",
      "0.001\n",
      "Best training loss: 9.102071635425091e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 8.551398059353232e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 7.227577589219436e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 6.383467552950606e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 5.805488399346359e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000058\n",
      "0.001\n",
      "Best training loss: 5.7941033446695656e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 4.962891398463398e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 4.7062323574209586e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.500546128838323e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 3.9718874177196994e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 3.823767838184722e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000038\n",
      "0.001\n",
      "Best training loss: 3.531834954628721e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.2363575883209705e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.210327122360468e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.107221345999278e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 2.8439984816941433e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 2.6655461624613963e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.5703804567456245e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.538204898883123e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000025\n",
      "0.001\n",
      "Best training loss: 2.339514685445465e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.330931783944834e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.2168556824908592e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.1079917132738046e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.0911182218696922e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 1.958963184733875e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 1.9220351532567292e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.8611912310007028e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.7662698155618273e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.748314025462605e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000017\n",
      "0.001\n",
      "Best training loss: 1.6638770830468275e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.615722430869937e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.5817340681678616e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.5080733646755107e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.4840694348094985e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.4328138604469132e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.3858989404980093e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.3614489944302477e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.3093003872199915e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.2829988008888904e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000013\n",
      "0.001\n",
      "Best training loss: 1.2496099770942237e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.2099163541279268e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.1899853234353941e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.1532655662449542e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.1274655662418809e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1050239663745742e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.0734515853982884e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.0550133993092459e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.0298405868525151e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0068164556287229e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 9.89374802884413e-06 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 9.656509973865468e-06 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 9.491381206316873e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.306659194407985e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.114784006669652e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 8.975211130746175e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 8.794132554612588e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 8.647195500088856e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 8.509608051099349e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.352045369974803e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 8.230932508013211e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.09546691016294e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 7.969156285980716e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 7.857234777475242e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 7.73288502387004e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 7.629913852724712e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.52206688048318e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.416112566716038e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.3248161243100185e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.224169621622423e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.135045962058939e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.049436135275755e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 6.96055303706089e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 6.882478373881895e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 6.802107236580923e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 6.725745151925366e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.6540696934680454e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.581542493222514e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.516065695905127e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.450108685385203e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.386679160641506e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.3283810050052125e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.268753168114927e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.213506821950432e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.160425527923508e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.107743047323311e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.058984126866562e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.011045570630813e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 5.965361651760759e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 5.921858246438205e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 5.879418949916726e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 5.839789082529023e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 5.800835424452089e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.764270554209361e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.730019893235294e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.6966100601130165e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.665162916557165e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.635297384287696e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.606157174042892e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8316, 0.9070, 0.5569, 0.9513, 0.8867, 0.5910, 0.7404, 0.5015, 0.7027,\n",
      "         0.6885, 0.8741, 0.6626, 0.6965, 0.8843, 0.7431, 0.8014, 0.6937, 0.4282,\n",
      "         0.1905, 0.9258, 0.5521, 0.6727, 0.8714, 0.5180, 0.7414, 0.8438, 0.4692,\n",
      "         0.8507, 0.6639, 0.5293, 0.4851, 0.7444, 0.4869, 0.2671, 0.9644, 0.5603,\n",
      "         0.8190, 0.6542, 0.5272, 0.1986, 0.8393, 0.1503, 0.5024, 0.0896, 0.3618]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.606157174042892e-06 from epoch 199\n",
      "MLP predicts 0.528 for case 62 with [0.67]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15693336725234985 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.156933\n",
      "0.001\n",
      "Best training loss: 0.028542693704366684 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.024901602417230606 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.086402\n",
      "0.001\n",
      "Best training loss: 0.023409944027662277 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.01721087284386158 at learning rate 0.001 and epoch 14\n",
      "Best training loss: 0.009153821505606174 at learning rate 0.001 and epoch 19\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.044378\n",
      "0.001\n",
      "Best training loss: 0.00652300613000989 at learning rate 0.001 and epoch 24\n",
      "Best training loss: 0.005467595532536507 at learning rate 0.001 and epoch 29\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.018250\n",
      "0.001\n",
      "Best training loss: 0.004655726253986359 at learning rate 0.001 and epoch 31\n",
      "Best training loss: 0.004622467793524265 at learning rate 0.001 and epoch 34\n",
      "Best training loss: 0.0025955152232199907 at learning rate 0.001 and epoch 36\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.006380\n",
      "0.001\n",
      "Best training loss: 0.001711151679046452 at learning rate 0.001 and epoch 41\n",
      "Best training loss: 0.0012862959410995245 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002176\n",
      "0.001\n",
      "Best training loss: 0.001018162234686315 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0008912677876651287 at learning rate 0.001 and epoch 53\n",
      "Best training loss: 0.000805690186098218 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.0006034316611476243 at learning rate 0.001 and epoch 58\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000801\n",
      "0.001\n",
      "Best training loss: 0.00043387728510424495 at learning rate 0.001 and epoch 63\n",
      "Best training loss: 0.00032162267598323524 at learning rate 0.001 and epoch 68\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000337\n",
      "0.001\n",
      "Best training loss: 0.00024249899433925748 at learning rate 0.001 and epoch 73\n",
      "Best training loss: 0.00023113626230042428 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00018628381076268852 at learning rate 0.001 and epoch 78\n",
      "Best training loss: 0.00016653555212542415 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000167\n",
      "0.001\n",
      "Best training loss: 0.00014498540258500725 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.0001247113395947963 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00011360455391695723 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 9.60065153776668e-05 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000096\n",
      "0.001\n",
      "Best training loss: 9.476520062889904e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 8.928801980800927e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 7.534839096479118e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.193735655164346e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 7.049903069855645e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 5.9917445469181985e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000060\n",
      "0.001\n",
      "Best training loss: 5.5843323934823275e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.5831951613072306e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.421369496616535e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 4.8082609282573685e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.417408854351379e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.230140257277526e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 3.886373815475963e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000039\n",
      "0.001\n",
      "Best training loss: 3.547301821527071e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.363407449796796e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.1670562748331577e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.165286761941388e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 2.889070128730964e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.72051493084291e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.6055131456814706e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000026\n",
      "0.001\n",
      "Best training loss: 2.5682977138785645e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.387011409155093e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.3748403691570275e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.2420719687943347e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.1702287995140068e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.1182846467127092e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.0002287783427164e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.9795219486695714e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.878225702967029e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.8293343600817025e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000018\n",
      "0.001\n",
      "Best training loss: 1.7783237126423046e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.6988669813144952e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.6725425666663796e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.5984001947799698e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.5625500964233652e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.515823259978788e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.4617286979046185e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4355812709254678e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.3805909475195222e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.352355411654571e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3139029761077836e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.275608974538045e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2505714039434679e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2107517250115052e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.1880571946676355e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.156348025688203e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1285343134659342e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1068627827626187e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.0771078450488858e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0580080925137736e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0331855264666956e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0121900231752079e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 9.931623935699463e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.709699952509254e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.551465154800098e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.350930668006185e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.186440365738235e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.028558451973367e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 8.856663043843582e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.721070116735063e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.564181371184532e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.43281941342866e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.298358807223849e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.164411156030837e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.051244549278636e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 7.923968041723128e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.81567632657243e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.706136784690898e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.5991401899955235e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.502846074203262e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.401633411063813e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.313115020224359e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.2215389081975445e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.135808118619025e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.056332378851948e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 6.973735253268387e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.899741492816247e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.825198852311587e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.75471028444008e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.687269888061564e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.620552539970959e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.5596441345405765e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.498014954559039e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.441000550694298e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.386948371073231e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.333485089271562e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.284245500864927e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.2355979935091455e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.189923169586109e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.144960934761912e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.101516191847622e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.060402029106626e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.019293323333841e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.980846253805794e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.943784344708547e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.908033017476555e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.873875124962069e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.840590347361285e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.8087157412956e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9068, 0.5571, 0.9517, 0.8869, 0.5908, 0.7403, 0.5013, 0.7031,\n",
      "         0.6884, 0.8741, 0.6620, 0.6966, 0.8845, 0.7428, 0.8019, 0.6672, 0.4280,\n",
      "         0.1904, 0.9253, 0.5519, 0.6726, 0.8717, 0.5185, 0.7415, 0.8436, 0.4694,\n",
      "         0.8504, 0.6641, 0.5291, 0.4850, 0.7446, 0.4868, 0.2674, 0.9647, 0.5604,\n",
      "         0.8193, 0.6542, 0.5275, 0.1984, 0.8393, 0.1503, 0.5027, 0.0898, 0.3616]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.8087157412956e-06 from epoch 199\n",
      "MLP predicts 1.002 for case 87 with [0.69]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1578555852174759 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.157856\n",
      "0.001\n",
      "Best training loss: 0.033626947551965714 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.01924438588321209 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.019244\n",
      "0.001\n",
      "Best training loss: 0.01252814568579197 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.011172633618116379 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.011173\n",
      "0.001\n",
      "Best training loss: 0.00956667773425579 at learning rate 0.001 and epoch 23\n",
      "Best training loss: 0.005064797587692738 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.009067\n",
      "0.001\n",
      "Best training loss: 0.0033937597181648016 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.002774018794298172 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.005787\n",
      "0.001\n",
      "Best training loss: 0.002099373145028949 at learning rate 0.001 and epoch 41\n",
      "Best training loss: 0.0013137361966073513 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002905\n",
      "0.001\n",
      "Best training loss: 0.0010320640867576003 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0008932737982831895 at learning rate 0.001 and epoch 54\n",
      "Best training loss: 0.0008886383147910237 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.0005899263196624815 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001165\n",
      "0.001\n",
      "Best training loss: 0.0004518555651884526 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.000393155321944505 at learning rate 0.001 and epoch 67\n",
      "Best training loss: 0.00036667438689619303 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000422\n",
      "0.001\n",
      "Best training loss: 0.00027029437478631735 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00026363565120846033 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00020498632511589676 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00017804880917537957 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000178\n",
      "0.001\n",
      "Best training loss: 0.00016405883070547134 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00013093676534481347 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00012173491268185899 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 0.00010233126522507519 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000102\n",
      "0.001\n",
      "Best training loss: 8.945904846768826e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 8.234764391090721e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 6.9124398578424e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.676893099211156e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000067\n",
      "0.001\n",
      "Best training loss: 6.261862290557474e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 5.5249533033929765e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.400404552347027e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.870405973633751e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.4984004489379004e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.436530434759334e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 4.359599552117288e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000044\n",
      "0.001\n",
      "Best training loss: 3.905003177351318e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.693473991006613e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.515584103297442e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.195991666871123e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.1900541216600686e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.0435430744546466e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.8524356821435504e-05 at learning rate 0.001 and epoch 119\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000029\n",
      "0.001\n",
      "Best training loss: 2.654103809618391e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.589454743429087e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.5246346922358498e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.364614192629233e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.3469705411116593e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.228985795227345e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.1452840883284807e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.1086671040393412e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.9924602383980528e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.957445238076616e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000020\n",
      "0.001\n",
      "Best training loss: 1.8878588889492676e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.8085756892105564e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.7807269614422694e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.7008920622174628e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.657667053223122e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.6158561265910976e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.5506540876231156e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.52274451465928e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.470729876018595e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.4283029486250598e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3986033991386648e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3499902706826106e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.3229583601059858e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2876437722297851e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2509677617345005e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2280865121283568e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1923661986656953e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1667998478515074e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1423212527006399e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.1122986506961752e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0923105037363712e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.066894856194267e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0443943210702855e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.025428264256334e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.0022629794548266e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.850019523582887e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.658468115958385e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.468817552260589e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.317846888734493e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.136409062193707e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.985056410892867e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.839716429065447e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.682874067744706e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.553843144909479e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.414424883085303e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.28455995360855e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.165652616298757e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.03925831860397e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.930064384709112e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.81702601670986e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.707099939580075e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.609503427374875e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.50542540117749e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.411021670122864e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.319755241042003e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.22683580534067e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.143001312215347e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.058865776343737e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.978894361964194e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.903261692059459e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.828346158727072e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.759333700756542e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.690497230010806e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.624317393288948e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.5628000811557285e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.500650670204777e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.442280664487043e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.38657320450875e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.3318293541669846e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.28041516392841e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.230261078599142e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.1821829149266705e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.136026513559045e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.0910579122719355e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.048489012755454e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.007183856127085e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.967643573967507e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.9302083172951825e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.89356932323426e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9072, 0.5569, 0.9512, 0.8866, 0.5910, 0.7410, 0.5013, 0.7028,\n",
      "         0.6886, 0.8745, 0.6622, 0.6964, 0.8849, 0.7430, 0.8014, 0.6673, 0.6936,\n",
      "         0.1905, 0.9251, 0.5522, 0.6727, 0.8711, 0.5179, 0.7415, 0.8438, 0.4694,\n",
      "         0.8504, 0.6639, 0.5290, 0.4848, 0.7445, 0.4866, 0.2673, 0.9646, 0.5604,\n",
      "         0.8190, 0.6541, 0.5273, 0.1983, 0.8392, 0.1503, 0.5027, 0.0895, 0.3617]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.89356932323426e-06 from epoch 199\n",
      "MLP predicts 0.404 for case 58 with [0.43]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15940268337726593 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.159403\n",
      "0.001\n",
      "Best training loss: 0.031993746757507324 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.019673049449920654 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.019673\n",
      "0.001\n",
      "Best training loss: 0.011615213006734848 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.00964328646659851 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.009643\n",
      "0.001\n",
      "Best training loss: 0.00866744015365839 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.005537149962037802 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.007573\n",
      "0.001\n",
      "Best training loss: 0.0033174820709973574 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.002390364883467555 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.004999\n",
      "0.001\n",
      "Best training loss: 0.0019869019743055105 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0014450580347329378 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002750\n",
      "0.001\n",
      "Best training loss: 0.0009740149253048003 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0007625313592143357 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.0006719827651977539 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001252\n",
      "0.001\n",
      "Best training loss: 0.0006370139890350401 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.00045765817048959434 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00034271602635271847 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000507\n",
      "0.001\n",
      "Best training loss: 0.0003105615614913404 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00026978322421200573 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.00021635484881699085 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00021624486544169486 at learning rate 0.001 and epoch 79\n",
      "Best training loss: 0.00021234258019831032 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000212\n",
      "0.001\n",
      "Best training loss: 0.00016266903548967093 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.0001479720522183925 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00012806575978174806 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00010921432840405032 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000109\n",
      "0.001\n",
      "Best training loss: 0.00010274579835822806 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 0.00010233895591227338 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 8.451572648482397e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 8.270906982943416e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 7.638079841854051e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.734074850101024e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000067\n",
      "0.001\n",
      "Best training loss: 6.626893446082249e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.9030629927292466e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.446315117296763e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 5.388419231167063e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 5.3007592214271426e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.697769327322021e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.44040160800796e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000044\n",
      "0.001\n",
      "Best training loss: 4.231833008816466e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.816471144091338e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.634303357102908e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.401818321435712e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.144474248983897e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 3.085428397753276e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.9926850402262062e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000030\n",
      "0.001\n",
      "Best training loss: 2.7929821953875944e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.7839925678563304e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.620995837787632e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.5314646336482838e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.4822405976010486e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.3334614525083452e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.3029757358017378e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.2055874069337733e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.115085044351872e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 2.0822026272071525e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000021\n",
      "0.001\n",
      "Best training loss: 1.977050123969093e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.9333585441927426e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.875319503596984e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.797265031200368e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.7664837287156843e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.6963749658316374e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.650491321925074e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.6111212971736677e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.5504794646403752e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.5211994650599081e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000015\n",
      "0.001\n",
      "Best training loss: 1.473192241974175e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.4312718121800572e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.4027966244611889e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.3570940609497484e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.328956386714708e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2962511391378939e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.2607505595951807e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.2370199328870513e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.2038948625558987e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.1785298738686834e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000012\n",
      "0.001\n",
      "Best training loss: 1.1540194464032538e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.125722974393284e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.1058817108278163e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.081036953110015e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.0591395948722493e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.040555889630923e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 1.0181034667766653e-05 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 1.0007620403484907e-05 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.82358142209705e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.639774361858144e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 9.486455383012071e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 9.313391274190508e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 9.165583833237179e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 9.018594028020743e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.868241820891853e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.741530109546147e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.601587069279049e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.47373030410381e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.354961209988687e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 8.229029845097102e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 8.118810001178645e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 8.007669748621993e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.899821866885759e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.800528692314401e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.699477464484517e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.607481620652834e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.5161019594816025e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.427037871821085e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.3458641054457985e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.262991857714951e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.185269168985542e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 7.110955721145729e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 7.0364035309467e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.9671932578785345e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.899048457853496e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.833476163592422e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.770993422833271e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.709662102366565e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.6521347434900235e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.59609895592439e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.542352366523119e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.4910409491858445e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.439266599045368e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.390548605850199e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.343706445477437e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.297378149611177e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.25335997028742e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.21027902525384e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.1689092945016455e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9074, 0.5571, 0.9511, 0.8868, 0.5911, 0.7406, 0.5015, 0.7026,\n",
      "         0.6887, 0.8743, 0.6622, 0.6965, 0.8850, 0.7430, 0.8012, 0.6674, 0.6937,\n",
      "         0.4280, 0.9247, 0.5522, 0.6728, 0.8711, 0.5178, 0.7415, 0.8437, 0.4694,\n",
      "         0.8503, 0.6640, 0.5293, 0.4848, 0.7445, 0.4866, 0.2684, 0.9645, 0.5604,\n",
      "         0.8189, 0.6542, 0.5271, 0.1981, 0.8393, 0.1505, 0.5028, 0.0900, 0.3620]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 6.1689092945016455e-06 from epoch 199\n",
      "MLP predicts 0.536 for case 89 with [0.19]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1516006886959076 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.151601\n",
      "0.001\n",
      "Best training loss: 0.03021981567144394 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.01787579618394375 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.017876\n",
      "0.001\n",
      "Best training loss: 0.010336984880268574 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.008555114269256592 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.008555\n",
      "0.001\n",
      "Best training loss: 0.007735418155789375 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.0043505034409463406 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.006784\n",
      "0.001\n",
      "Best training loss: 0.0024527672212570906 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.0017573290970176458 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.004422\n",
      "0.001\n",
      "Best training loss: 0.0015109608648344874 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0009182582725770772 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002321\n",
      "0.001\n",
      "Best training loss: 0.0005956882378086448 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.00047741178423166275 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.00039460230618715286 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000969\n",
      "0.001\n",
      "Best training loss: 0.00025050368276424706 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00018328535952605307 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000340\n",
      "0.001\n",
      "Best training loss: 0.00016771430091466755 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00014533214562106878 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.0001077566557796672 at learning rate 0.001 and epoch 77\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000117\n",
      "0.001\n",
      "Best training loss: 7.880689372541383e-05 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 7.528508285759017e-05 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 6.262724491534755e-05 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 5.29638709849678e-05 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000053\n",
      "0.001\n",
      "Best training loss: 5.1297560275997967e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 4.040767817059532e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 3.822122744168155e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 3.240465957787819e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000032\n",
      "0.001\n",
      "Best training loss: 2.8864367777714506e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 2.663968552951701e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 2.2915852241567336e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 2.2139245629659854e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000022\n",
      "0.001\n",
      "Best training loss: 2.1335838027880527e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 1.880523814179469e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 1.843157588155009e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 1.7104262951761484e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 1.5763074770802632e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 1.54379758896539e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000015\n",
      "0.001\n",
      "Best training loss: 1.4175782780512236e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 1.3422199117485434e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 1.3158199180907104e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 1.301687825616682e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 1.2048856660840102e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 1.155453355750069e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.1139137313875835e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.112091285904171e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0438931894896086e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.038732716551749e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.0067235962196719e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 9.659636816650163e-06 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 9.629095984564628e-06 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 9.196929568133783e-06 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 9.056937415152788e-06 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 8.879224878910463e-06 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 8.549285666958895e-06 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 8.498247552779503e-06 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 8.221853931900114e-06 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 8.05364743428072e-06 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 7.952929081511684e-06 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 7.702339644310996e-06 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 7.623994406458223e-06 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 7.458420896000462e-06 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 7.300507149921032e-06 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 7.222519343486056e-06 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 7.049834493955132e-06 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 6.96593406246393e-06 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.854834282421507e-06 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 6.724030754412524e-06 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 6.660943199676694e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 6.536830369441304e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 6.453988135035615e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 6.380023023666581e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 6.275029136304511e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 6.215156190592097e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 6.130404017312685e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 6.055751782696461e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 5.9961716942780185e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 5.917076578043634e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 5.864081686013378e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 5.799579867016291e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 5.7364791246072855e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 5.689177669410128e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 5.625946414511418e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 5.576373496296583e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 5.527564553631237e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 5.4734096011088695e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000005\n",
      "0.001\n",
      "Best training loss: 5.430862984212581e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 5.3824251153855585e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 5.3391513574752025e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 5.298732048686361e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 5.255947598925559e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 5.220487309998134e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 5.181645519769518e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 5.14552175445715e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 5.114175110065844e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 5.079405127617065e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000005\n",
      "0.001\n",
      "Best training loss: 5.049319952377118e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 5.019404397899052e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 4.989248282072367e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 4.962370439898223e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 4.934875505568925e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 4.909846211376134e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 4.8852366489882115e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 4.861944944423158e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 4.841459485760424e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 4.820488811674295e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000005\n",
      "0.001\n",
      "Best training loss: 4.800825081474613e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 4.782174983120058e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 4.762656317325309e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 4.744821126223542e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 4.727458872366697e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 4.710413122666068e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 4.694339622801635e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 4.678661298385123e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 4.663496383727761e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8311, 0.9064, 0.5569, 0.9505, 0.8860, 0.5908, 0.7374, 0.5019, 0.7014,\n",
      "         0.6899, 0.8734, 0.6593, 0.6966, 0.8844, 0.7429, 0.8012, 0.6667, 0.6936,\n",
      "         0.4288, 0.1905, 0.5520, 0.6727, 0.8710, 0.5184, 0.7419, 0.8431, 0.4693,\n",
      "         0.8502, 0.6640, 0.5284, 0.4853, 0.7441, 0.4866, 0.2666, 0.9641, 0.5611,\n",
      "         0.8179, 0.6542, 0.5284, 0.1983, 0.8378, 0.1511, 0.5028, 0.0899, 0.3611]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 4.663496383727761e-06 from epoch 199\n",
      "MLP predicts 0.461 for case 78 with [0.94]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15662948787212372 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.156629\n",
      "0.001\n",
      "Best training loss: 0.03409067168831825 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.017150945961475372 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.017151\n",
      "0.001\n",
      "Best training loss: 0.013586055487394333 at learning rate 0.001 and epoch 15\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.013731\n",
      "0.001\n",
      "Best training loss: 0.007551295217126608 at learning rate 0.001 and epoch 23\n",
      "Best training loss: 0.004705033730715513 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.011365\n",
      "0.001\n",
      "Best training loss: 0.003910703118890524 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.003115652361884713 at learning rate 0.001 and epoch 36\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.006686\n",
      "0.001\n",
      "Best training loss: 0.00181808159686625 at learning rate 0.001 and epoch 41\n",
      "Best training loss: 0.0014398948987945914 at learning rate 0.001 and epoch 46\n",
      "Best training loss: 0.0012529848609119654 at learning rate 0.001 and epoch 49\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002954\n",
      "0.001\n",
      "Best training loss: 0.0008155820542015135 at learning rate 0.001 and epoch 54\n",
      "Best training loss: 0.0006479150033555925 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001042\n",
      "0.001\n",
      "Best training loss: 0.0005469584721140563 at learning rate 0.001 and epoch 62\n",
      "Best training loss: 0.0003840975696220994 at learning rate 0.001 and epoch 67\n",
      "Best training loss: 0.00036960409488528967 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000370\n",
      "0.001\n",
      "Best training loss: 0.0002991287619806826 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.0002463693090248853 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00024294620379805565 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00018216983880847692 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000182\n",
      "0.001\n",
      "Best training loss: 0.00016506985411979258 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.00014428577560465783 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00012087864888599142 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 0.00011703583732014522 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000117\n",
      "0.001\n",
      "Best training loss: 0.00011245798668824136 at learning rate 0.001 and epoch 91\n",
      "Best training loss: 9.404941374668851e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 8.336832979694009e-05 at learning rate 0.001 and epoch 96\n",
      "Best training loss: 7.566082786070183e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 7.556282798759639e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000076\n",
      "0.001\n",
      "Best training loss: 6.475929694715887e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 6.161418423289433e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.856149073224515e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 5.196930578676984e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 5.0161179387941957e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.5999473513802513e-05 at learning rate 0.001 and epoch 109\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000047\n",
      "0.001\n",
      "Best training loss: 4.2468454921618104e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 4.1697512642713264e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 4.078797428519465e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.7141740904189646e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.502274921629578e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.340384500916116e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.317878508823924e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 3.058609945583157e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 3.0263614462455735e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000030\n",
      "0.001\n",
      "Best training loss: 2.9024507966823876e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.7359505111235194e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.7201780540053733e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.5547818950144574e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.4828268578858115e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.423044315946754e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.286251401528716e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.2554895622306503e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.158567076548934e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 2.077383396681398e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000021\n",
      "0.001\n",
      "Best training loss: 2.0397696061991155e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.941885238920804e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.9004584828508087e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.8419057596474886e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.7693802874418907e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.738657920213882e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.671527570579201e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.6265359590761364e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.5892223018454388e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.531082125438843e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000015\n",
      "0.001\n",
      "Best training loss: 1.5009067283244804e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.4566681784344837e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.4151454706734512e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.3870146176486742e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.344202973996289e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.3156874956621323e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.284631434828043e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.2498197975219227e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.2268090358702466e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.1952443855989259e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000012\n",
      "0.001\n",
      "Best training loss: 1.1692355656123254e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.1464997442089953e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.1183925380464643e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0978709724440705e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.0747859050752595e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.0522133379708976e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 1.0337799722037744e-05 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 1.0121771992999129e-05 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.94366746454034e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.765134564077016e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 9.579108336765785e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 9.426795259059872e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 9.2555728770094e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 9.10081416805042e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.959985279943794e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.806833648122847e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.67547896632459e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.542168870917521e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.411049748247024e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 8.293981409224216e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 8.172108209691942e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 8.061650987656321e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.953917702252511e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.846686457924079e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.750796612526756e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.652070053154603e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.55943301555817e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.472850938938791e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.384408945654286e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.303368420252809e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.2241105044668075e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 7.1467275120085105e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 7.074869699863484e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 7.003200607869076e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.935223609616514e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.869154731248273e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.804774329793872e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.7444611886458006e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.684411346213892e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.6270417846681084e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.573064638359938e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.519140697491821e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.468359970313031e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.4194300648523495e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.3717625380377285e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.326184575300431e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.281980404310161e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.239549747988349e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.198097253218293e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9071, 0.5570, 0.9513, 0.8868, 0.5908, 0.7414, 0.5015, 0.7023,\n",
      "         0.6889, 0.8743, 0.6616, 0.6963, 0.8848, 0.7428, 0.8014, 0.6673, 0.6936,\n",
      "         0.4280, 0.1905, 0.9247, 0.6726, 0.8707, 0.5176, 0.7414, 0.8438, 0.4692,\n",
      "         0.8503, 0.6641, 0.5288, 0.4851, 0.7444, 0.4866, 0.2676, 0.9648, 0.5605,\n",
      "         0.8191, 0.6541, 0.5277, 0.1982, 0.8394, 0.1503, 0.5030, 0.0893, 0.3616]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 6.198097253218293e-06 from epoch 199\n",
      "MLP predicts 0.294 for case 90 with [0.56]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15708717703819275 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.157087\n",
      "0.001\n",
      "Best training loss: 0.0288404393941164 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.022367985919117928 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.045769\n",
      "0.001\n",
      "Best training loss: 0.016487596556544304 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.011929706670343876 at learning rate 0.001 and epoch 17\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.017973\n",
      "0.001\n",
      "Best training loss: 0.008341135457158089 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.0056351483799517155 at learning rate 0.001 and epoch 27\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.007305\n",
      "0.001\n",
      "Best training loss: 0.003789623035117984 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.0025689247995615005 at learning rate 0.001 and epoch 37\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.002881\n",
      "0.001\n",
      "Best training loss: 0.0018137104343622923 at learning rate 0.001 and epoch 42\n",
      "Best training loss: 0.0013418048620224 at learning rate 0.001 and epoch 47\n",
      "Best training loss: 0.0012266283156350255 at learning rate 0.001 and epoch 50\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001227\n",
      "0.001\n",
      "Best training loss: 0.0010050598066300154 at learning rate 0.001 and epoch 52\n",
      "Best training loss: 0.0008256530854851007 at learning rate 0.001 and epoch 55\n",
      "Best training loss: 0.000747071229852736 at learning rate 0.001 and epoch 57\n",
      "Best training loss: 0.0005716056330129504 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000572\n",
      "0.001\n",
      "Best training loss: 0.0005575411487370729 at learning rate 0.001 and epoch 62\n",
      "Best training loss: 0.00040768299368210137 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.0002949629561044276 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000295\n",
      "0.001\n",
      "Best training loss: 0.00021677385666407645 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00021647616813424975 at learning rate 0.001 and epoch 78\n",
      "Best training loss: 0.0001634436921449378 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000163\n",
      "0.001\n",
      "Best training loss: 0.00015596671437378973 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.00012580423208419234 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00011568557965802029 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 9.814384975470603e-05 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000098\n",
      "0.001\n",
      "Best training loss: 8.796998736215755e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 7.734432438155636e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.722496229689568e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 6.812620995333418e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.135840521892533e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000061\n",
      "0.001\n",
      "Best training loss: 6.030265285517089e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.357488771551289e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 4.90480633743573e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.754009569296613e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.273165905033238e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 3.955412103096023e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000040\n",
      "0.001\n",
      "Best training loss: 3.905820631189272e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.797282261075452e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.456150443525985e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.222688610549085e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.143058711430058e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.068852311116643e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.8310892957961187e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.6533934942563064e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000027\n",
      "0.001\n",
      "Best training loss: 2.5704139261506498e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.5183941033901647e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.351182047277689e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.338003287150059e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.2134932805784047e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.1385223590186797e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.095370837196242e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.978643012989778e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.9541981600923464e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.8685001123230904e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000019\n",
      "0.001\n",
      "Best training loss: 1.805118336051237e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.770122798916418e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.6863008568179794e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.657263965171296e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.5981937394826673e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.5463676390936598e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.515589883638313e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4558258044417016e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4278390153776854e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.3841241525369696e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3427492376649752e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3170769307180308e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2727505236398429e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2470313777157571e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2149465874244925e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1820430700026918e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1597545380936936e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.12735742732184e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.105456340155797e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0805896636156831e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0549805665505119e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0366708920628298e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0118649697687943e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.932348802976776e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.741996109369211e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.538991434965283e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.382440111949109e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.194443919113837e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.039591532200575e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.885231181920972e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.725135558051988e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.596357474743854e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.447376785625238e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.319237167597748e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.197147508326452e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.068270290095825e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.959685717651155e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.843662388040684e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.738378371868748e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.63741263654083e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.534416454291204e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.4449599196668714e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.350438863795716e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.263569386850577e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.181893124652561e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.097240995790344e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.021286364761181e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.9450957198569085e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.872917765576858e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.80459879731643e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.7361702349444386e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.673271855106577e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.60912382954848e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.5481485762575176e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.49052572043729e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.432889676943887e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.378795205819188e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.325929007289233e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.274376119108638e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.225175184226828e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.1767395891365595e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.130825568106957e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.085428140067961e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.041429969627643e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.998882897984004e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.9567955759121105e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.916988811804913e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.8785199144040234e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.841457550559426e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9073, 0.5569, 0.9512, 0.8867, 0.5912, 0.7408, 0.5014, 0.7027,\n",
      "         0.6887, 0.8745, 0.6624, 0.6965, 0.8849, 0.7430, 0.8013, 0.6674, 0.6937,\n",
      "         0.4283, 0.1905, 0.9253, 0.5521, 0.8710, 0.5178, 0.7415, 0.8438, 0.4693,\n",
      "         0.8504, 0.6639, 0.5290, 0.4849, 0.7445, 0.4865, 0.2675, 0.9645, 0.5603,\n",
      "         0.8190, 0.6543, 0.5271, 0.1982, 0.8391, 0.1503, 0.5027, 0.0895, 0.3617]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.841457550559426e-06 from epoch 199\n",
      "MLP predicts 0.874 for case 61 with [0.67]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15332156419754028 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.153322\n",
      "0.001\n",
      "Best training loss: 0.028906770050525665 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.021859673783183098 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.048372\n",
      "0.001\n",
      "Best training loss: 0.015457074157893658 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.010793703608214855 at learning rate 0.001 and epoch 17\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.020018\n",
      "0.001\n",
      "Best training loss: 0.007455618120729923 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.005119567736983299 at learning rate 0.001 and epoch 27\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.008501\n",
      "0.001\n",
      "Best training loss: 0.0035133527126163244 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.002402717247605324 at learning rate 0.001 and epoch 37\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.003437\n",
      "0.001\n",
      "Best training loss: 0.0016800453886389732 at learning rate 0.001 and epoch 42\n",
      "Best training loss: 0.0012159051839262247 at learning rate 0.001 and epoch 47\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001442\n",
      "0.001\n",
      "Best training loss: 0.0008939384715631604 at learning rate 0.001 and epoch 52\n",
      "Best training loss: 0.0006608023541048169 at learning rate 0.001 and epoch 57\n",
      "Best training loss: 0.0006534550921060145 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000653\n",
      "0.001\n",
      "Best training loss: 0.0004909681156277657 at learning rate 0.001 and epoch 62\n",
      "Best training loss: 0.00045228138333186507 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.00036283681401982903 at learning rate 0.001 and epoch 67\n",
      "Best training loss: 0.0003164976369589567 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000316\n",
      "0.001\n",
      "Best training loss: 0.0002675191208254546 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.0002259317843709141 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00019963951490353793 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.000166100260685198 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000166\n",
      "0.001\n",
      "Best training loss: 0.00015148671809583902 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.0001250575587619096 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00011625853221630678 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 9.59258759394288e-05 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000096\n",
      "0.001\n",
      "Best training loss: 9.01240055100061e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 7.461407949449494e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.04109261278063e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 6.955347635084763e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 5.8757446822710335e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000059\n",
      "0.001\n",
      "Best training loss: 5.554594099521637e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.3913270676275715e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 4.6804627345409244e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.423220525495708e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.248629556968808e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 3.770184048335068e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000038\n",
      "0.001\n",
      "Best training loss: 3.560951154213399e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.403022128622979e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.386406388017349e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.069060403504409e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 2.8958740585949272e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.7643362045637332e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.741251046245452e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.5274164727306925e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000025\n",
      "0.001\n",
      "Best training loss: 2.517086613806896e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.387337553955149e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.282366949657444e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.2531012291437946e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.109426168317441e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.0841864170506597e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 1.994369631574955e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.9126458937535062e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.8840766642824747e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.7839161955635063e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000018\n",
      "0.001\n",
      "Best training loss: 1.752929892973043e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.690746285021305e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.6274256267934106e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.5990868632798083e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.5295569028239697e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.4995932360761799e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.4533501598634757e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4048571756575257e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.3799026419292204e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.3293447409523651e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000013\n",
      "0.001\n",
      "Best training loss: 1.3018691788602155e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.2678319762926549e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2302804861974437e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2077408428012859e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.1712455489032436e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1477314728836063e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1210513548576273e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.092348884412786e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.0735619071056135e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0460406883794349e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 1.0262293471896555e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.005874673865037e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 9.833804142544977e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.670846338849515e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.464251888857689e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.29984980757581e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.13400526769692e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 8.958332728070673e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 8.823084499454126e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.659618288220372e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.52225912240101e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.390308721573092e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.248596714111045e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.131327376759145e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.00224097474711e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 7.887022547947709e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.775542144372594e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.661494237254374e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.563950020994525e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.45830220694188e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.3625660661491565e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.271786216733744e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.178113719419343e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.094996817613719e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.010261924733641e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 6.930609288247069e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.853880222479347e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.777330781915225e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.707951797579881e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.636739271925762e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.57014697935665e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.507632861030288e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.44401779936743e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.385257165675284e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.327238679659786e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.271758593356935e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.218551789061166e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.166940238472307e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.1188106883491855e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.071506959415274e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.027335984981619e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 5.985382813378237e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 5.943640189798316e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.904602858208818e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.867374966328498e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.831432645209134e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.7975321396952495e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.764503839600366e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.733502803195734e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9072, 0.5567, 0.9510, 0.8866, 0.5910, 0.7404, 0.5014, 0.7027,\n",
      "         0.6886, 0.8743, 0.6622, 0.6963, 0.8847, 0.7431, 0.8012, 0.6673, 0.6933,\n",
      "         0.4283, 0.1903, 0.9253, 0.5522, 0.6727, 0.5178, 0.7415, 0.8436, 0.4692,\n",
      "         0.8505, 0.6638, 0.5290, 0.4849, 0.7443, 0.4866, 0.2671, 0.9645, 0.5603,\n",
      "         0.8192, 0.6541, 0.5269, 0.1984, 0.8392, 0.1504, 0.5026, 0.0895, 0.3616]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.733502803195734e-06 from epoch 199\n",
      "MLP predicts 0.715 for case 1 with [0.87]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15782485902309418 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.157825\n",
      "0.001\n",
      "Best training loss: 0.02882845513522625 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.023271026089787483 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.043183\n",
      "0.001\n",
      "Best training loss: 0.01735990308225155 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.012437884695827961 at learning rate 0.001 and epoch 17\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.016417\n",
      "0.001\n",
      "Best training loss: 0.00876037310808897 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.006094136741012335 at learning rate 0.001 and epoch 27\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.006746\n",
      "0.001\n",
      "Best training loss: 0.004237104207277298 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.002939307363703847 at learning rate 0.001 and epoch 37\n",
      "Best training loss: 0.0027004277799278498 at learning rate 0.001 and epoch 40\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.002700\n",
      "0.001\n",
      "Best training loss: 0.002082493156194687 at learning rate 0.001 and epoch 42\n",
      "Best training loss: 0.0017429945291951299 at learning rate 0.001 and epoch 45\n",
      "Best training loss: 0.001521526137366891 at learning rate 0.001 and epoch 47\n",
      "Best training loss: 0.0011666915379464626 at learning rate 0.001 and epoch 50\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001167\n",
      "0.001\n",
      "Best training loss: 0.0011297138407826424 at learning rate 0.001 and epoch 52\n",
      "Best training loss: 0.000805108284112066 at learning rate 0.001 and epoch 55\n",
      "Best training loss: 0.000573539815377444 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000574\n",
      "0.001\n",
      "Best training loss: 0.00041637138929218054 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.0003039861039724201 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000304\n",
      "0.001\n",
      "Best training loss: 0.00030136617715470493 at learning rate 0.001 and epoch 73\n",
      "Best training loss: 0.00022488748072646558 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00021005059534218162 at learning rate 0.001 and epoch 78\n",
      "Best training loss: 0.0001698113774182275 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000170\n",
      "0.001\n",
      "Best training loss: 0.0001517984492238611 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.00013046337699051946 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00011313694267300889 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 0.00010153871699003503 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000102\n",
      "0.001\n",
      "Best training loss: 8.662074105814099e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 7.98914406914264e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 6.762585690012202e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.332451448542997e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000063\n",
      "0.001\n",
      "Best training loss: 6.268807919695973e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 6.239447247935459e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.3630068578058854e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.0611419283086434e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.897494363831356e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.89594203827437e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.3065971112810075e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.0737155359238386e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000041\n",
      "0.001\n",
      "Best training loss: 3.890857988153584e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.4948476240970194e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.308041414129548e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.1412953831022605e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.129431206616573e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.865618807845749e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.8583537641679868e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.7105179469799623e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000027\n",
      "0.001\n",
      "Best training loss: 2.572713310655672e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.554201455495786e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.3758093448122963e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.3430544388247654e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.2487376554636285e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.139544812962413e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.1140434910194017e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.9944822270190343e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.9545523173292167e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.888842962216586e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000019\n",
      "0.001\n",
      "Best training loss: 1.8055587133858353e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.779365811671596e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.6961404980975203e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.6561407392146066e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.6100591892609373e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.546492421766743e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.5199798326648306e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4624726645706687e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4268525774241425e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.3915756426285952e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.344107022305252e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3203105481807142e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2784353202732746e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.248091120942263e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2214824891998433e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.185322707897285e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1638471733022016e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1335543604218401e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1084048310294747e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0867621313082054e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0595208550512325e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.041263749357313e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0180434401263483e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.974773092835676e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.802301974559668e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.589050932845566e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.430194040760398e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.252178642782383e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.082232281798497e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.936115591495764e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.76973535923753e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.635490303277038e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.491267180943396e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.352687473234255e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.233092557929922e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.098321814031806e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.983526302268729e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.868154170864727e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.753530553600285e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.652163731108885e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.545487733295886e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.450185421475908e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.354816261795349e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.261469818331534e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.1777835728426e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.089879545674194e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.00984082868672e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.9330721998994704e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.85628765495494e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.785994628444314e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.716217285429593e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.650866453128401e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.587759344256483e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.526466222567251e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.4700570874265395e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.413563369278563e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.360372481140075e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.3101206251303665e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.26092150923796e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.215353550942382e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.1709997680736706e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.12867779636872e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.08812069913256e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.049078820069553e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.012440280755982e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.97634516452672e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.942389179836027e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.910431809752481e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.8789951253857e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9071, 0.5569, 0.9513, 0.8866, 0.5910, 0.7408, 0.5014, 0.7029,\n",
      "         0.6886, 0.8743, 0.6623, 0.6965, 0.8848, 0.7430, 0.8015, 0.6674, 0.6937,\n",
      "         0.4283, 0.1904, 0.9252, 0.5521, 0.6727, 0.8711, 0.7416, 0.8439, 0.4694,\n",
      "         0.8504, 0.6639, 0.5290, 0.4848, 0.7445, 0.4866, 0.2673, 0.9645, 0.5604,\n",
      "         0.8190, 0.6542, 0.5275, 0.1983, 0.8394, 0.1503, 0.5026, 0.0896, 0.3617]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.8789951253857e-06 from epoch 199\n",
      "MLP predicts 0.652 for case 2 with [0.52]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15457426011562347 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.154574\n",
      "0.001\n",
      "Best training loss: 0.031185537576675415 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.023245470598340034 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.023245\n",
      "0.001\n",
      "Best training loss: 0.01240972988307476 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.008464992977678776 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.008465\n",
      "0.001\n",
      "Best training loss: 0.006573508959263563 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.005358397029340267 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.005358\n",
      "0.001\n",
      "Best training loss: 0.003860319498926401 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.002266721101477742 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.003585\n",
      "0.001\n",
      "Best training loss: 0.001571565750055015 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0012539083836600184 at learning rate 0.001 and epoch 48\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002201\n",
      "0.001\n",
      "Best training loss: 0.0010693742660805583 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0010467622196301818 at learning rate 0.001 and epoch 53\n",
      "Best training loss: 0.000701000913977623 at learning rate 0.001 and epoch 56\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001147\n",
      "0.001\n",
      "Best training loss: 0.0005166246555745602 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.00050726457266137 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.0004046211834065616 at learning rate 0.001 and epoch 66\n",
      "Best training loss: 0.0003342519630677998 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000525\n",
      "0.001\n",
      "Best training loss: 0.0003211408038623631 at learning rate 0.001 and epoch 71\n",
      "Best training loss: 0.00023740626056678593 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.00023227873316500336 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00018034760432783514 at learning rate 0.001 and epoch 79\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000232\n",
      "0.001\n",
      "Best training loss: 0.00016177845827769488 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00014250796812120825 at learning rate 0.001 and epoch 84\n",
      "Best training loss: 0.00011952668864978477 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00011411866580601782 at learning rate 0.001 and epoch 89\n",
      "Best training loss: 0.00011356752656865865 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000114\n",
      "0.001\n",
      "Best training loss: 9.240396320819855e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 9.149662946583703e-05 at learning rate 0.001 and epoch 94\n",
      "Best training loss: 8.417866774834692e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.343625475186855e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 7.302156154764816e-05 at learning rate 0.001 and epoch 99\n",
      "Best training loss: 6.463052704930305e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000065\n",
      "0.001\n",
      "Best training loss: 5.9223482821835205e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.8212255680700764e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 5.1126226026099175e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.8172649258049205e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.643072679755278e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.132469621254131e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000041\n",
      "0.001\n",
      "Best training loss: 3.9354046748485416e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.704725531861186e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.389601261005737e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.369776459294371e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.232572635170072e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.018792631337419e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.8123626179876737e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000028\n",
      "0.001\n",
      "Best training loss: 2.7428863177192397e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.6723817427409813e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.503739233361557e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.487918573024217e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.3571637939312495e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.2735946913599037e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.232721817563288e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.1082989405840635e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.0749474060721695e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.9959848941653036e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000020\n",
      "0.001\n",
      "Best training loss: 1.9172935935785063e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.884816629171837e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.7982973076868802e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.7583010048838332e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.7066568034351803e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.6411340766353533e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.6123794921441004e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.5518395230174065e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.511793198005762e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.4766433196200524e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000015\n",
      "0.001\n",
      "Best training loss: 1.4249520063458476e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3976175978314131e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.356208485958632e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.3203453818277922e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2932649951835629e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2544690434879158e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.2297488865442574e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1996800822089426e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1696914953063242e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.1482938134577125e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.1189405995537527e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.096926280297339e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.074896408681525e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0502637451281771e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.0319272405467927e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.0098793609358836e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.908177162287757e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.732663784234319e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.537608093523886e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.38361063163029e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 9.211275028064847e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 9.050419066625182e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.910054930311162e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.75267869560048e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.620144399174023e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.48617037263466e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.35238188301446e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.234494089265354e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.109552254609298e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.99718782218406e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.887235369707923e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.77714558353182e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.679506779822987e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.578258191642817e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.483977697120281e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.395731245196657e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.3055143730016425e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.22293725630152e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.14095631337841e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.0607134148303885e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.984906576690264e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.9093684942345135e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.838624358351808e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.768953426217195e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.701595793856541e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.637812475673854e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.57337341181119e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.512749223475112e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.454465165006695e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.39688960291096e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.342293090710882e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.288943495746935e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.238090463739354e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.1893365455034655e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.142378424556227e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.097743607824668e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.054375717212679e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.0129900703032035e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.9735780268965755e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9072, 0.5569, 0.9512, 0.8868, 0.5909, 0.7411, 0.5014, 0.7020,\n",
      "         0.6884, 0.8743, 0.6623, 0.6961, 0.8850, 0.7428, 0.8014, 0.6673, 0.6935,\n",
      "         0.4283, 0.1904, 0.9254, 0.5522, 0.6726, 0.8709, 0.5177, 0.8435, 0.4693,\n",
      "         0.8500, 0.6638, 0.5289, 0.4849, 0.7444, 0.4865, 0.2677, 0.9648, 0.5601,\n",
      "         0.8188, 0.6541, 0.5277, 0.1980, 0.8388, 0.1502, 0.5028, 0.0894, 0.3618]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.9735780268965755e-06 from epoch 199\n",
      "MLP predicts 0.485 for case 3 with [0.74]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15296056866645813 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.152961\n",
      "0.001\n",
      "Best training loss: 0.03145148605108261 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.017993943765759468 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.017994\n",
      "0.001\n",
      "Best training loss: 0.012037722393870354 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.011058244854211807 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.011058\n",
      "0.001\n",
      "Best training loss: 0.009279957041144371 at learning rate 0.001 and epoch 23\n",
      "Best training loss: 0.005040759686380625 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.009087\n",
      "0.001\n",
      "Best training loss: 0.0034576025791466236 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.00284401117824018 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.005782\n",
      "0.001\n",
      "Best training loss: 0.002171769505366683 at learning rate 0.001 and epoch 41\n",
      "Best training loss: 0.0013667350867763162 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002896\n",
      "0.001\n",
      "Best training loss: 0.0010532928863540292 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0009142383933067322 at learning rate 0.001 and epoch 54\n",
      "Best training loss: 0.0008913752972148359 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.0005976291722618043 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001162\n",
      "0.001\n",
      "Best training loss: 0.00045523239532485604 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00039725477108731866 at learning rate 0.001 and epoch 67\n",
      "Best training loss: 0.00037057558074593544 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000424\n",
      "0.001\n",
      "Best training loss: 0.0002749348641373217 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00026666236226446927 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00020943177514709532 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00018113698752131313 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000181\n",
      "0.001\n",
      "Best training loss: 0.0001679324050201103 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00013384157500695437 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00012352435442153364 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 0.00010484481754247099 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000105\n",
      "0.001\n",
      "Best training loss: 9.109682287089527e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 8.44007299747318e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.061640644678846e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.841339927632362e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000068\n",
      "0.001\n",
      "Best training loss: 6.365353328874335e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 5.657767178490758e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.5291402532020584e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.96843276778236e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.6139008190948516e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.517060369835235e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 4.462702418095432e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000045\n",
      "0.001\n",
      "Best training loss: 3.9993934478843585e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.7950961996102706e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.5954049963038415e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.287714935140684e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.267067950218916e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.135408405796625e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.9337898013181984e-05 at learning rate 0.001 and epoch 119\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000029\n",
      "0.001\n",
      "Best training loss: 2.742171818681527e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.6660443836590275e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.6089197490364313e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.4457038307446055e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.4238273908849806e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.3122374841477722e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.2210411771084182e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.187053360103164e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.0711975594167598e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 2.0310344552854076e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000020\n",
      "0.001\n",
      "Best training loss: 1.965685441973619e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.882436845335178e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.8537841242505237e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.775347482180223e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.7276242942898534e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.6872629203135148e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.6200139725697227e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.5896976037765853e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.5383951904368587e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.492357023380464e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000015\n",
      "0.001\n",
      "Best training loss: 1.4622664821217768e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.4124808330961969e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.3825326277583372e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.347265515505569e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.3081367796985433e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2838375369028654e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.247465843334794e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.2194702321721707e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1942828678002115e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.1625606930465437e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000012\n",
      "0.001\n",
      "Best training loss: 1.1408770660636947e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.1146677024953533e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0901334462687373e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0701885912567377e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.0458245924382936e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.0269674021401443e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 1.0071238648379222e-05 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.867387234407943e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.705991033115424e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.517053513263818e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 9.353792847832665e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 9.201594366459176e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 9.035552466230001e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.898237865651026e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.7533044279553e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.615291335445363e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.49165917315986e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.36032813822385e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.244995115092024e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 8.129025445668958e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 8.014973900571931e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.913679837656673e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.80684786150232e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.708566045039333e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.615192771481816e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.519971859437646e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.433531663991744e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.3469855124130845e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.263549832714489e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.1848421612230595e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.105998065526364e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 7.032891517155804e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.960985956538934e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.891135399200721e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.825868240412092e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.76011222822126e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.698385277559282e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.6390211941325106e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.580136869160924e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.52424978397903e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.469583695434267e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.416553333110642e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.3657425926066935e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.315998234640574e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.2685694501851685e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.222584943316178e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.178128387546167e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.135911462479271e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.094735454098554e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9072, 0.5568, 0.9514, 0.8870, 0.5906, 0.7412, 0.5011, 0.7028,\n",
      "         0.6887, 0.8744, 0.6615, 0.6962, 0.8849, 0.7428, 0.8015, 0.6672, 0.6937,\n",
      "         0.4281, 0.1907, 0.9248, 0.5521, 0.6726, 0.8713, 0.5175, 0.7414, 0.4693,\n",
      "         0.8503, 0.6639, 0.5292, 0.4849, 0.7447, 0.4866, 0.2673, 0.9644, 0.5602,\n",
      "         0.8185, 0.6542, 0.5273, 0.1982, 0.8396, 0.1500, 0.5029, 0.0896, 0.3614]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 6.094735454098554e-06 from epoch 199\n",
      "MLP predicts 0.482 for case 6 with [0.85]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1583087295293808 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.158309\n",
      "0.001\n",
      "Best training loss: 0.027432061731815338 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.020201286301016808 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.054319\n",
      "0.001\n",
      "Best training loss: 0.014622219838202 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.010499527677893639 at learning rate 0.001 and epoch 17\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.023015\n",
      "0.001\n",
      "Best training loss: 0.007195201236754656 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.004910526797175407 at learning rate 0.001 and epoch 27\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.009934\n",
      "0.001\n",
      "Best training loss: 0.003424186259508133 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.00237340503372252 at learning rate 0.001 and epoch 37\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.004120\n",
      "0.001\n",
      "Best training loss: 0.0016393618425354362 at learning rate 0.001 and epoch 42\n",
      "Best training loss: 0.0011707599041983485 at learning rate 0.001 and epoch 47\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001714\n",
      "0.001\n",
      "Best training loss: 0.0008558061672374606 at learning rate 0.001 and epoch 52\n",
      "Best training loss: 0.0006267176358960569 at learning rate 0.001 and epoch 57\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000756\n",
      "0.001\n",
      "Best training loss: 0.0004617005761247128 at learning rate 0.001 and epoch 62\n",
      "Best training loss: 0.0003417625557631254 at learning rate 0.001 and epoch 67\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000356\n",
      "0.001\n",
      "Best training loss: 0.0002531101054046303 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00025059108156710863 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.000189599406439811 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.0001810290414141491 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000181\n",
      "0.001\n",
      "Best training loss: 0.0001442936045350507 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00013368001964408904 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00011104250734206289 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00010082233347930014 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000101\n",
      "0.001\n",
      "Best training loss: 8.652600809000432e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 7.757458661217242e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 6.824119918746874e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 6.75323826726526e-05 at learning rate 0.001 and epoch 99\n",
      "Best training loss: 6.0660404415102676e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000061\n",
      "0.001\n",
      "Best training loss: 5.424216215033084e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.280147888697684e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 4.7954017645679414e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.343657201388851e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.184546196484007e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 3.842588921543211e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000038\n",
      "0.001\n",
      "Best training loss: 3.517120785545558e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.3607972000027075e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.120444671367295e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 2.880207830457948e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.8523327273433097e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.741319804044906e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.5700679543660954e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000026\n",
      "0.001\n",
      "Best training loss: 2.3890615921118297e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.3492488253396004e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.267181480419822e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.1453892259160057e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.1303774701664224e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.006517388508655e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.96423407032853e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.9044318833039142e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.815498217183631e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000018\n",
      "0.001\n",
      "Best training loss: 1.7920365280588157e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.7071961337933317e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.668101867835503e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.6207366570597515e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.5558629456791095e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.531670022814069e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.47050677696825e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.436225193174323e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4002736861584708e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.3518491869035643e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3282162399264053e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.284552399738459e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2557607078633737e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2265584700799081e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.1904892744496465e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1698371963575482e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1369125786586665e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1129491213068832e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.0900824236159679e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0622557056194637e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0438087883812841e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0192216905124951e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 9.995164873544127e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.805664376472123e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.592064998287242e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.437220796826296e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.242428859579377e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.079414667212404e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 8.927882845455315e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.758461262914352e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.625532245787326e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.475109098071698e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.341257853317074e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.214604349632282e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.081448868324514e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 7.97126631368883e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.848680979805067e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.738401109236293e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.635572728759144e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.526424269599374e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.431603080476634e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.333402209042106e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.24161009202362e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.154223567340523e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.065892987156985e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 6.98694111633813e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.905027476022951e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.828614914411446e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.7568416852736846e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.683954779873602e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.617975941480836e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.552932063641492e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.491498425020836e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.433381713577546e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.3762563513591886e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.323389243334532e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.270949597819708e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.221643616299843e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.174603640829446e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.127803771960316e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.083809694246156e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.040850166755263e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 5.999342192808399e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.959605914540589e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.921431693423074e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.885115569981281e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.849364242749289e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.815457825519843e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.783526376035297e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9070, 0.5570, 0.9514, 0.8863, 0.5914, 0.7403, 0.5021, 0.7025,\n",
      "         0.6885, 0.8744, 0.6632, 0.6967, 0.8849, 0.7432, 0.8013, 0.6671, 0.6935,\n",
      "         0.4281, 0.1906, 0.9255, 0.5525, 0.6727, 0.8712, 0.5180, 0.7413, 0.8436,\n",
      "         0.8506, 0.6637, 0.5288, 0.4851, 0.7445, 0.4870, 0.2679, 0.9645, 0.5605,\n",
      "         0.8188, 0.6540, 0.5274, 0.1984, 0.8393, 0.1503, 0.5025, 0.0896, 0.3618]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.783526376035297e-06 from epoch 199\n",
      "MLP predicts 1.102 for case 9 with [0.47]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15248523652553558 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.152485\n",
      "0.001\n",
      "Best training loss: 0.030738510191440582 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.020244736224412918 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.020245\n",
      "0.001\n",
      "Best training loss: 0.01183214783668518 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.009335828945040703 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.009336\n",
      "0.001\n",
      "Best training loss: 0.00807933695614338 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.005959574598819017 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.006953\n",
      "0.001\n",
      "Best training loss: 0.003426681039854884 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.002335513709113002 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.004630\n",
      "0.001\n",
      "Best training loss: 0.0018693313468247652 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0014699370367452502 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002588\n",
      "0.001\n",
      "Best training loss: 0.0009426515316590667 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0007130175945349038 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.0006522265612147748 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001192\n",
      "0.001\n",
      "Best training loss: 0.000589356932323426 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.00043346575694158673 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00031939923064783216 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000482\n",
      "0.001\n",
      "Best training loss: 0.00029309309320524335 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00025029759854078293 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.0002019436506088823 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00020084439893253148 at learning rate 0.001 and epoch 79\n",
      "Best training loss: 0.00019888039969373494 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000199\n",
      "0.001\n",
      "Best training loss: 0.00015095464186742902 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00013757037231698632 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00011878516670549288 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00010120699153048918 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000101\n",
      "0.001\n",
      "Best training loss: 9.549270907882601e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 9.494984988123178e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 7.83381619839929e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.704812014708295e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 7.078295311657712e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.261153612285852e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000063\n",
      "0.001\n",
      "Best training loss: 6.190407293615863e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.4847630963195115e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.087378667667508e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 5.016473005525768e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.9640406359685585e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.384481508168392e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.164737038081512e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000042\n",
      "0.001\n",
      "Best training loss: 3.952058978029527e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.5791130358120427e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.4206477721454576e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.191314317518845e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 2.9633831218234263e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.9011163860559464e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.8262127671041526e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000028\n",
      "0.001\n",
      "Best training loss: 2.6334915673942305e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.6272216928191483e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.480298098817002e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.3896765924291685e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.3505712306359783e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.2093729057814926e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.1787893274449743e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.0928599042235874e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.003248482651543e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.9751107174670324e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000020\n",
      "0.001\n",
      "Best training loss: 1.877277645689901e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.833363603509497e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.782484287105035e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.706627881503664e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.6781572412583046e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.6137626516865566e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.567961589898914e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.5330286259995773e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4750032278243452e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.4465579624811653e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.402861835231306e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3615335774375126e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.3353510439628735e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2921337656734977e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2644970411201939e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2345914001343772e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.200091719510965e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1778240150306374e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1468608136055991e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.122038156609051e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.09955335574341e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.072394570655888e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0533005479373969e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0301797374268062e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.0088045200973283e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.91340584732825e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.699549991637468e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.532097465125844e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.359809155284893e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.18217938306043e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 9.037425115820952e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.873777915141545e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.731457455724012e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.594813152740244e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.451628673356026e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.331745448231231e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.202485332731158e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.082351087068673e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.973931133165024e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.857479431550018e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.756046215945389e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.654617547814269e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.554595867986791e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.463782367267413e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.370927960437257e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.285529591172235e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.202399501693435e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.120640475477558e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.046901373541914e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.972154096729355e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.902145742060384e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.836293323431164e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.769892479496775e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.708610271743964e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.6486500145401806e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.590496468561469e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.535908141813707e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.481895979959518e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.431003839679761e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.381392722687451e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.3332754507428035e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.288348231464624e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.243890766199911e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.201827090990264e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.161345936561702e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.121187652752269e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.083276275603566e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.046976523066405e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.011953701090533e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9070, 0.5567, 0.9507, 0.8867, 0.5906, 0.7410, 0.5016, 0.7029,\n",
      "         0.6885, 0.8740, 0.6624, 0.6965, 0.8845, 0.7429, 0.8012, 0.6672, 0.6936,\n",
      "         0.4281, 0.1905, 0.9251, 0.5518, 0.6726, 0.8716, 0.5181, 0.7410, 0.8436,\n",
      "         0.4694, 0.6637, 0.5291, 0.4849, 0.7444, 0.4869, 0.2673, 0.9646, 0.5602,\n",
      "         0.8188, 0.6539, 0.5277, 0.1986, 0.8395, 0.1503, 0.5019, 0.0897, 0.3617]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 6.011953701090533e-06 from epoch 199\n",
      "MLP predicts 0.548 for case 10 with [0.85]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1570194810628891 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.157019\n",
      "0.001\n",
      "Best training loss: 0.02864505723118782 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.021555863320827484 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.052137\n",
      "0.001\n",
      "Best training loss: 0.015049267560243607 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.010491286404430866 at learning rate 0.001 and epoch 17\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.022560\n",
      "0.001\n",
      "Best training loss: 0.007306928280740976 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.005051448941230774 at learning rate 0.001 and epoch 27\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.009774\n",
      "0.001\n",
      "Best training loss: 0.0034616119228303432 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.002352586016058922 at learning rate 0.001 and epoch 37\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.003972\n",
      "0.001\n",
      "Best training loss: 0.0016188824083656073 at learning rate 0.001 and epoch 42\n",
      "Best training loss: 0.0011490901233628392 at learning rate 0.001 and epoch 47\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001636\n",
      "0.001\n",
      "Best training loss: 0.0008309833356179297 at learning rate 0.001 and epoch 52\n",
      "Best training loss: 0.0006071487441658974 at learning rate 0.001 and epoch 57\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000716\n",
      "0.001\n",
      "Best training loss: 0.00044822346535511315 at learning rate 0.001 and epoch 62\n",
      "Best training loss: 0.00033089896896854043 at learning rate 0.001 and epoch 67\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000335\n",
      "0.001\n",
      "Best training loss: 0.00024461050634272397 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00023564528964925557 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00018354292842559516 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00017091170593630522 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000171\n",
      "0.001\n",
      "Best training loss: 0.0001404458744218573 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.0001273755042348057 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00010897333413595334 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 9.711005986901e-05 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000097\n",
      "0.001\n",
      "Best training loss: 8.553013321943581e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 7.538893260061741e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 6.777545058866963e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 6.763615238014609e-05 at learning rate 0.001 and epoch 99\n",
      "Best training loss: 5.945925659034401e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000059\n",
      "0.001\n",
      "Best training loss: 5.420496017904952e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.3131094318814576e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 4.750055813929066e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.3724732677219436e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.23222008976154e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 3.8410169509006664e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000038\n",
      "0.001\n",
      "Best training loss: 3.5596644011093304e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.525159991113469e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.410361023270525e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.139685941278003e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 2.9247319616843015e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.868656702048611e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.787363155221101e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.5975074095185846e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000026\n",
      "0.001\n",
      "Best training loss: 2.4310127628268674e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.372155722696334e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.308300099684857e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.1750800442532636e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.1604912035400048e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.043821405095514e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.9888911992893554e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.939485991897527e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.843276731960941e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000018\n",
      "0.001\n",
      "Best training loss: 1.8191612980444916e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.7386686522513628e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.6909907571971416e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.6497986507602036e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.580411299073603e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.554901245981455e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.4965232367103454e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.456705740565667e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4241461940400768e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.3728908015764318e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3482731446856633e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3063084224995691e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2738292753056157e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2467025953810662e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2089470146747772e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1873035873577464e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1554768207133748e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1292071576463059e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1074508620367851e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.078628974937601e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0596064385026693e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0356391612731386e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0143535291717853e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.960748684534337e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.741309440869372e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.580795449437574e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.390384548169095e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.218008926836774e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.068960025615525e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.895162864064332e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.758186595514417e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.609807082393672e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.469132808386348e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.343073204741813e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.205760423152242e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.090714800346177e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.968066711327992e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.852256203477737e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.748409188934602e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.636444934178144e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.538837962783873e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.439554337906884e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.34371542421286e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.255650871229591e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.165324859670363e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.0840483203937765e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.002005531830946e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.923137334524654e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.850246791145764e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.7753785515378695e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.706477506668307e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.638895683863666e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.572846814378863e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.5105787143693306e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.448898147937143e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.3910201788530685e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.334200406854507e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.279490662564058e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.227868198038777e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.176362148835324e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.128071618149988e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.081329502194421e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.0355641835485585e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.9919043451373e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.949420028628083e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.9088320085720625e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.869234428246273e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.831379894516431e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.795420747745084e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9070, 0.5569, 0.9514, 0.8867, 0.5910, 0.7416, 0.5014, 0.7034,\n",
      "         0.6884, 0.8740, 0.6626, 0.6965, 0.8847, 0.7431, 0.8017, 0.6672, 0.6935,\n",
      "         0.4281, 0.1905, 0.9252, 0.5517, 0.6727, 0.8713, 0.5182, 0.7414, 0.8438,\n",
      "         0.4693, 0.8507, 0.5290, 0.4849, 0.7444, 0.4867, 0.2670, 0.9645, 0.5603,\n",
      "         0.8193, 0.6543, 0.5271, 0.1985, 0.8393, 0.1503, 0.5027, 0.0896, 0.3618]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.795420747745084e-06 from epoch 199\n",
      "MLP predicts 0.88 for case 11 with [0.66]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1575045883655548 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.157505\n",
      "0.001\n",
      "Best training loss: 0.0330740362405777 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.020231708884239197 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.020232\n",
      "0.001\n",
      "Best training loss: 0.012201054953038692 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.010123305022716522 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.010123\n",
      "0.001\n",
      "Best training loss: 0.009069632738828659 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.005461449734866619 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.007899\n",
      "0.001\n",
      "Best training loss: 0.0033176897559314966 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.0024644003715366125 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.005188\n",
      "0.001\n",
      "Best training loss: 0.0023795675951987505 at learning rate 0.001 and epoch 41\n",
      "Best training loss: 0.002101064193993807 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0013631280744448304 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002781\n",
      "0.001\n",
      "Best training loss: 0.0009639266063459218 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0007864279905334115 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.0006203251541592181 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001209\n",
      "0.001\n",
      "Best training loss: 0.00043808933696709573 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00043370004277676344 at learning rate 0.001 and epoch 67\n",
      "Best training loss: 0.00033868374885059893 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000464\n",
      "0.001\n",
      "Best training loss: 0.0002824115799739957 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00027155622956342995 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.0002024550485657528 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00019115350733045489 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000191\n",
      "0.001\n",
      "Best training loss: 0.00015652579895686358 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00013517348270397633 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.0001253628870472312 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00010195389040745795 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000102\n",
      "0.001\n",
      "Best training loss: 0.00010102717351401225 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 9.355543443234637e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 8.039415843086317e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.059321069391444e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.47890628897585e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000065\n",
      "0.001\n",
      "Best training loss: 6.427034531952813e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.528792826225981e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.258192686596885e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.995494236936793e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.446959792403504e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.277759217075072e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000043\n",
      "0.001\n",
      "Best training loss: 3.947013829019852e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.6342691601021215e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.5833189031109214e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.4810287615982816e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.193864904460497e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 2.9982958949403837e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.8827042115153745e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.847247196768876e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000028\n",
      "0.001\n",
      "Best training loss: 2.633695476106368e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.613113792904187e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.493417377991136e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.3686456188443117e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.3452957975678146e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.2030711988918483e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.1542140530073084e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.0877432689303532e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.9811748643405735e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.9557850464479998e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000020\n",
      "0.001\n",
      "Best training loss: 1.8641860151547007e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.806100772228092e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.766692912497092e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.6845871869008988e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.6532987501705065e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.596396032255143e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.5415176676469855e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.5117550901777577e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4533902685798239e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.4206976629793644e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3826503163727466e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3369901353144087e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.3123936696501914e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.27102539408952e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2401103958836757e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2138511920056771e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1776628525694832e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1551910574780777e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1264841305091977e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.099374821933452e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.078677178156795e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0514793757465668e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0315418876416516e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0101384759764187e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.877063348540105e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.709624464449007e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.501260137767531e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.326788131147623e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.164903531200252e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.981306564237457e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.835573680698872e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.67669950821437e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.526310011802707e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.393331881961785e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.248080121120438e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.124272426357493e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.997850843821652e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.87359294918133e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.766005182929803e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.650231054867618e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.546391771029448e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.447707957908278e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.346062375290785e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.256731805682648e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.1666659096081275e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.081259809638141e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.000933692324907e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.920469331817003e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.848072189313825e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.775400834158063e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.705719442834379e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.64184744891827e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.576555279025342e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.515961558761774e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.4577325247228146e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.399958238034742e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.346550435409881e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.294030299613951e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.243772986636031e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.1958608057466336e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.1490786720241886e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.104793556005461e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.0610541368077975e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.019427473802352e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.980104106129147e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.940907612966839e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.903631517867325e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.868027074029669e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.833185696246801e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8313, 0.9071, 0.5570, 0.9513, 0.8867, 0.5910, 0.7406, 0.5015, 0.7029,\n",
      "         0.6886, 0.8741, 0.6622, 0.6964, 0.8848, 0.7430, 0.8015, 0.6673, 0.6935,\n",
      "         0.4282, 0.1902, 0.9250, 0.5519, 0.6727, 0.8712, 0.5181, 0.7414, 0.8437,\n",
      "         0.4694, 0.8504, 0.6639, 0.4846, 0.7441, 0.4863, 0.2672, 0.9646, 0.5603,\n",
      "         0.8191, 0.6541, 0.5272, 0.1982, 0.8394, 0.1503, 0.5027, 0.0894, 0.3617]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.833185696246801e-06 from epoch 199\n",
      "MLP predicts 0.462 for case 12 with [0.53]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15796107053756714 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.157961\n",
      "0.001\n",
      "Best training loss: 0.028655827045440674 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.02283463068306446 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.044609\n",
      "0.001\n",
      "Best training loss: 0.016677066683769226 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.011706018820405006 at learning rate 0.001 and epoch 17\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.017375\n",
      "0.001\n",
      "Best training loss: 0.008080889470875263 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.005510937422513962 at learning rate 0.001 and epoch 27\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.007133\n",
      "0.001\n",
      "Best training loss: 0.0037736992817372084 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.0025981341022998095 at learning rate 0.001 and epoch 37\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.002847\n",
      "0.001\n",
      "Best training loss: 0.001845000428147614 at learning rate 0.001 and epoch 42\n",
      "Best training loss: 0.0018401220440864563 at learning rate 0.001 and epoch 45\n",
      "Best training loss: 0.001357140950858593 at learning rate 0.001 and epoch 47\n",
      "Best training loss: 0.0012292085448279977 at learning rate 0.001 and epoch 50\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001229\n",
      "0.001\n",
      "Best training loss: 0.0010158121585845947 at learning rate 0.001 and epoch 52\n",
      "Best training loss: 0.0008445497951470315 at learning rate 0.001 and epoch 55\n",
      "Best training loss: 0.0007633977802470326 at learning rate 0.001 and epoch 57\n",
      "Best training loss: 0.0005994049715809524 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000599\n",
      "0.001\n",
      "Best training loss: 0.0005765591049566865 at learning rate 0.001 and epoch 62\n",
      "Best training loss: 0.0004353768890723586 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.00043362320866435766 at learning rate 0.001 and epoch 67\n",
      "Best training loss: 0.0003201868967153132 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000320\n",
      "0.001\n",
      "Best training loss: 0.0002392017631791532 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00018241413636133075 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000182\n",
      "0.001\n",
      "Best training loss: 0.00017555007070768625 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.00014144084707368165 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00013141680392436683 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 0.00011099022231064737 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000111\n",
      "0.001\n",
      "Best training loss: 0.00011095001536887139 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 0.0001006212187348865 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 8.791637810645625e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 8.666601206641644e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 7.839522004360333e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 7.017439202172682e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000070\n",
      "0.001\n",
      "Best training loss: 6.835245585534722e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 6.204052624525502e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.647162106470205e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 5.440395398181863e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.975483534508385e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.581532994052395e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000046\n",
      "0.001\n",
      "Best training loss: 4.547446587821469e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 4.380222162581049e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 4.0411498048342764e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.7505440559471026e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.6773388274013996e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.562659185263328e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.319468305562623e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 3.319205643492751e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 3.098247179877944e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000031\n",
      "0.001\n",
      "Best training loss: 3.015549373230897e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.9347917006816715e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.7588284865487367e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.733635119511746e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.586595837783534e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.5085251763812266e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.4446464522043243e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.3182197764981538e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.284254878759384e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 2.1813029889017344e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000022\n",
      "0.001\n",
      "Best training loss: 2.113095615641214e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 2.062991916318424e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.9700208213180304e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.9331584553583525e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.8607550373417325e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.8037408153759316e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.761822568369098e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.6939658962655813e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.659637018747162e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.60595154738985e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000016\n",
      "0.001\n",
      "Best training loss: 1.5596733646816574e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.525884545117151e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.4750684385944624e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.4442665815295186e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.4045838724996429e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.3672512068296783e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.3385263628151733e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.3002359992242418e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.273621637665201e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.2424332453520037e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000012\n",
      "0.001\n",
      "Best training loss: 1.2124805834901053e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.1889633242390119e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.1592425835260656e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.1365372301952448e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.112363679567352e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.0881340131163597e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 1.0682223546609748e-05 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 1.0452976312080864e-05 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 1.0264694537909236e-05 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 1.00699962786166e-05 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 9.877005140879191e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 9.714059160614852e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 9.53126436797902e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 9.373367902298924e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 9.218096238328144e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 9.061120181286242e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.923635505198035e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.778659321251325e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.647802133054938e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 8.519405128026847e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.391732990276068e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 8.27731128083542e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 8.15776911622379e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 8.048597010201775e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.943979653646238e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.839234967832454e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.743930837023072e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.648538485227618e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.558677680208348e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.471721346519189e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.386098332062829e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 7.307182841032045e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 7.227940386655973e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 7.1525796556670684e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 7.080133855197346e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 7.0078299359011e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.940416369616287e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.8743538577109575e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.810622380726272e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.749145995854633e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.689337169518694e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.632607437495608e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.576833129656734e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.52362905384507e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.473060693679145e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.422787464543944e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.374951226462144e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.328707058855798e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.284211849560961e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9070, 0.5570, 0.9512, 0.8865, 0.5909, 0.7416, 0.5014, 0.7027,\n",
      "         0.6886, 0.8744, 0.6624, 0.6964, 0.8850, 0.7431, 0.8015, 0.6675, 0.6934,\n",
      "         0.4282, 0.1904, 0.9246, 0.5520, 0.6726, 0.8712, 0.5180, 0.7415, 0.8436,\n",
      "         0.4693, 0.8503, 0.6639, 0.5290, 0.7448, 0.4868, 0.2681, 0.9647, 0.5603,\n",
      "         0.8187, 0.6541, 0.5274, 0.1982, 0.8394, 0.1504, 0.5026, 0.0897, 0.3616]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 6.284211849560961e-06 from epoch 199\n",
      "MLP predicts 0.73 for case 13 with [0.48]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1556876301765442 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.155688\n",
      "0.001\n",
      "Best training loss: 0.029727155342698097 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.02556859701871872 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.037614\n",
      "0.001\n",
      "Best training loss: 0.02018139138817787 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.014985429123044014 at learning rate 0.001 and epoch 17\n",
      "Best training loss: 0.01297088898718357 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.012971\n",
      "0.001\n",
      "Best training loss: 0.010814495384693146 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.007986444048583508 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.007640142925083637 at learning rate 0.001 and epoch 27\n",
      "Best training loss: 0.004995317198336124 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.004995\n",
      "0.001\n",
      "Best training loss: 0.003131200559437275 at learning rate 0.001 and epoch 35\n",
      "Best training loss: 0.00201623747125268 at learning rate 0.001 and epoch 40\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.002016\n",
      "0.001\n",
      "Best training loss: 0.0013819187879562378 at learning rate 0.001 and epoch 45\n",
      "Best training loss: 0.0010038246400654316 at learning rate 0.001 and epoch 50\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001004\n",
      "0.001\n",
      "Best training loss: 0.0007527886191383004 at learning rate 0.001 and epoch 55\n",
      "Best training loss: 0.0007501873187720776 at learning rate 0.001 and epoch 58\n",
      "Best training loss: 0.0005758620682172477 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000576\n",
      "0.001\n",
      "Best training loss: 0.0005037057562731206 at learning rate 0.001 and epoch 63\n",
      "Best training loss: 0.000441558426246047 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.00034936724114231765 at learning rate 0.001 and epoch 68\n",
      "Best training loss: 0.000335296499542892 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000335\n",
      "0.001\n",
      "Best training loss: 0.0002502122661098838 at learning rate 0.001 and epoch 73\n",
      "Best training loss: 0.00018585333600640297 at learning rate 0.001 and epoch 78\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000194\n",
      "0.001\n",
      "Best training loss: 0.00017949429457075894 at learning rate 0.001 and epoch 81\n",
      "Best training loss: 0.0001421658816980198 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.00013045454397797585 at learning rate 0.001 and epoch 86\n",
      "Best training loss: 0.00011081321281380951 at learning rate 0.001 and epoch 88\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000115\n",
      "0.001\n",
      "Best training loss: 9.791270713321865e-05 at learning rate 0.001 and epoch 91\n",
      "Best training loss: 8.738260657992214e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 7.542975072283298e-05 at learning rate 0.001 and epoch 96\n",
      "Best training loss: 6.942484469618648e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.892844976391643e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000069\n",
      "0.001\n",
      "Best training loss: 5.9225258155493066e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 5.548599801841192e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.434607737697661e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 5.409508230513893e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.7277317207772285e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.469186023925431e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.2836818465730175e-05 at learning rate 0.001 and epoch 109\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000043\n",
      "0.001\n",
      "Best training loss: 3.823071892838925e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.6224268114892766e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.434247992117889e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.1258994567906484e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.124650538666174e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.964277155115269e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.8009459128952585e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.7899304768652655e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000028\n",
      "0.001\n",
      "Best training loss: 2.5856961656245403e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.5505580197204836e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.4502560336259194e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.3200847863336094e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.301249696756713e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.164104262192268e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.1158484742045403e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.0514646166702732e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.9507191609591246e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.924807656905614e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000019\n",
      "0.001\n",
      "Best training loss: 1.8335054846829735e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.784869709808845e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.7381205907440744e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.6627309378236532e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.6355661500710994e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.5717261703684926e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.5276653357432224e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4932239537301939e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4364748494699597e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.4093948266236112e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3649439097207505e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3274834600451868e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2995021279493812e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2576637345773634e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2333580343693029e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2003760275547393e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1697618901962414e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1475603969302028e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.115821760322433e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.094275194191141e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0699411177483853e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0451510206621606e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0264858246955555e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0027776625065599e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.846320608630776e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.65561866905773e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.459024113311898e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.306316314905416e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.121379662246909e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.967884241428692e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.81880350789288e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.659626473672688e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.52902030601399e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.385005457967054e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.255790817202069e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.131836693792138e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.00345151219517e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.893947440607008e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.77637160354061e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.667717000003904e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.567737156932708e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.462569556082599e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.3692108344403096e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.274860763573088e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.184324658737751e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.099563845258672e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.013782123976853e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.9362604335765354e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.858422693767352e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.784603101550601e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.7163118728785776e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.647323971264996e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.584097718587145e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.523088359244866e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.463565114245284e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.408349236153299e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.354076504067052e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.3032275647856295e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.2540343606087845e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.207023034221493e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.163343641674146e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.119622412370518e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.078706519474508e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.0398378991521895e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.001680958434008e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.965689069853397e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.930869974690722e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.8975369938707445e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.865043021913152e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9070, 0.5569, 0.9513, 0.8865, 0.5910, 0.7406, 0.5014, 0.7029,\n",
      "         0.6885, 0.8743, 0.6624, 0.6965, 0.8850, 0.7430, 0.8015, 0.6674, 0.6935,\n",
      "         0.4283, 0.1904, 0.9252, 0.5521, 0.6727, 0.8711, 0.5181, 0.7415, 0.8438,\n",
      "         0.4694, 0.8504, 0.6639, 0.5288, 0.4847, 0.4865, 0.2674, 0.9646, 0.5603,\n",
      "         0.8191, 0.6540, 0.5273, 0.1983, 0.8393, 0.1503, 0.5026, 0.0896, 0.3618]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.865043021913152e-06 from epoch 199\n",
      "MLP predicts 0.701 for case 15 with [0.75]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15800780057907104 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.158008\n",
      "0.001\n",
      "Best training loss: 0.032573603093624115 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.021553779020905495 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.021554\n",
      "0.001\n",
      "Best training loss: 0.01213144138455391 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.009240801446139812 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.009241\n",
      "0.001\n",
      "Best training loss: 0.007893570698797703 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.0060866898857057095 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.006789\n",
      "0.001\n",
      "Best training loss: 0.003422388806939125 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.0022878742311149836 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.004571\n",
      "0.001\n",
      "Best training loss: 0.0018199676414951682 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0015020824503153563 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002596\n",
      "0.001\n",
      "Best training loss: 0.0009530275710858405 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0007101947558112442 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.000678693875670433 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001214\n",
      "0.001\n",
      "Best training loss: 0.0005796629120595753 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.00044330599484965205 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00032002784428186715 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000500\n",
      "0.001\n",
      "Best training loss: 0.00030507889459840953 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.0002473719068802893 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.00020757903985213488 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00019772852829191834 at learning rate 0.001 and epoch 79\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000208\n",
      "0.001\n",
      "Best training loss: 0.00015303061809390783 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00014333584113046527 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00011912699119420722 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00010435061267344281 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000104\n",
      "0.001\n",
      "Best training loss: 9.528637019684538e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 7.985195406945422e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.687134348088875e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 7.323931640712544e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.323975685518235e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000063\n",
      "0.001\n",
      "Best training loss: 6.193573790369555e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.624954792438075e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.1100720156682655e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.986118074157275e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.457586328499019e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.1748211515368894e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000042\n",
      "0.001\n",
      "Best training loss: 4.044494198751636e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 4.0126840758603066e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.6131590604782104e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.4283035347471014e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.243287210352719e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 2.9759663448203355e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.9467822969309054e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.8327251129667275e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000028\n",
      "0.001\n",
      "Best training loss: 2.658365156094078e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.65206163021503e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.4832583221723326e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.4167464289348572e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.356868571951054e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.218542795162648e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.1960731828585267e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.0920057067996822e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.0161216525593773e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.980025990633294e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000020\n",
      "0.001\n",
      "Best training loss: 1.878165676316712e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.8431097487336956e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.7804924937081523e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.711139702820219e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.681433423073031e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.6114508980535902e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.5727213394711725e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.5312485629692674e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4749807633052114e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.448410239390796e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3998127542436123e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.36278395075351e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.3339688848645892e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2902802154712845e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.264942875423003e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2316756510699634e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1993285625067074e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1762967005779501e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1439986337791197e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.1211499440832995e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.096612322726287e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0700426173571032e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.051165054377634e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.026586141961161e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.0062745786854066e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.87796920526307e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.664307071943767e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.502022294327617e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.320267963630613e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.14864540391136e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.999038072943222e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.830952538119163e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.691788025316782e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.546763638150878e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.403715582971927e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.281991540570743e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.147099833877292e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.028279808058869e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.914522029750515e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.795814781275112e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.692124199820682e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.585473667859333e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.485376499971608e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.3914766289817635e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.296417152247159e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.210679086711025e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.1244821810978465e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.042318884487031e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.967024546611356e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.889003543619765e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.817032954131719e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.747476618329529e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.6785337367036846e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.614548055949854e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.551614660565974e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.4920182012428995e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.434856913983822e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.379631486197468e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.328274139377754e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.277824923017761e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.23028563495609e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.185517577250721e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.140923687780742e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.099180154706119e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.059148745407583e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.0202555687283166e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.9839635468961205e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.9490694184205495e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.915574092796305e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9072, 0.5570, 0.9513, 0.8867, 0.5909, 0.7407, 0.5014, 0.7027,\n",
      "         0.6886, 0.8743, 0.6622, 0.6964, 0.8849, 0.7430, 0.8015, 0.6674, 0.6935,\n",
      "         0.4283, 0.1904, 0.9252, 0.5522, 0.6726, 0.8710, 0.5179, 0.7415, 0.8437,\n",
      "         0.4693, 0.8503, 0.6639, 0.5291, 0.4848, 0.7445, 0.2674, 0.9646, 0.5603,\n",
      "         0.8191, 0.6542, 0.5275, 0.1984, 0.8392, 0.1503, 0.5026, 0.0896, 0.3617]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.915574092796305e-06 from epoch 199\n",
      "MLP predicts 0.621 for case 16 with [0.49]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15911002457141876 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.159110\n",
      "0.001\n",
      "Best training loss: 0.03439967334270477 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.01801416091620922 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.018014\n",
      "0.001\n",
      "Best training loss: 0.013122654519975185 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.012736024335026741 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.012736\n",
      "0.001\n",
      "Best training loss: 0.008106719702482224 at learning rate 0.001 and epoch 23\n",
      "Best training loss: 0.004628256894648075 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.010558\n",
      "0.001\n",
      "Best training loss: 0.0035439557395875454 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.0032409837003797293 at learning rate 0.001 and epoch 36\n",
      "Best training loss: 0.0031638655345886946 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.006334\n",
      "0.001\n",
      "Best training loss: 0.0016851958353072405 at learning rate 0.001 and epoch 41\n",
      "Best training loss: 0.0012108990922570229 at learning rate 0.001 and epoch 46\n",
      "Best training loss: 0.001167637761682272 at learning rate 0.001 and epoch 49\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002832\n",
      "0.001\n",
      "Best training loss: 0.0010759852593764663 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0006776315858587623 at learning rate 0.001 and epoch 54\n",
      "Best training loss: 0.0005051103653386235 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000962\n",
      "0.001\n",
      "Best training loss: 0.00045088783372193575 at learning rate 0.001 and epoch 62\n",
      "Best training loss: 0.00042847570148296654 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00029079479281790555 at learning rate 0.001 and epoch 67\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000298\n",
      "0.001\n",
      "Best training loss: 0.00021734641632065177 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00018210751295555383 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00017607220797799528 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.0001276594994124025 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000128\n",
      "0.001\n",
      "Best training loss: 0.0001201973864226602 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 9.959921590052545e-05 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 8.374144817935303e-05 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 8.127361070364714e-05 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000081\n",
      "0.001\n",
      "Best training loss: 8.091221388895065e-05 at learning rate 0.001 and epoch 91\n",
      "Best training loss: 6.380819104379043e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 5.774764213128947e-05 at learning rate 0.001 and epoch 96\n",
      "Best training loss: 5.130144927534275e-05 at learning rate 0.001 and epoch 98\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000053\n",
      "0.001\n",
      "Best training loss: 4.393225026433356e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 4.208268001093529e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 4.057878686580807e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 3.509144517011009e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 3.454253601375967e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 3.1398732971865684e-05 at learning rate 0.001 and epoch 109\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000033\n",
      "0.001\n",
      "Best training loss: 2.8817888960475102e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 2.8273238058318384e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 2.5263196221203543e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 2.3985496227396652e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 2.3016638806439005e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.0911551473545842e-05 at learning rate 0.001 and epoch 119\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000021\n",
      "0.001\n",
      "Best training loss: 2.0078303350601345e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 1.8862299839383923e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 1.763231739460025e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 1.7305632354691625e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 1.6924430383369327e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 1.5861076462897472e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.583855009812396e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.5064479157445021e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.4546915735991206e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000015\n",
      "0.001\n",
      "Best training loss: 1.43670067700441e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.3598887562693562e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.3425010365608614e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.2996306395507418e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.2484978469728958e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.2356897059362382e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.1840108527394477e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.1581495527934749e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.1347128747729585e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.0923892659775447e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0774980182759464e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.0455938536324538e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.0188909982389305e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.0026390555140097e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 9.718545697978698e-06 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 9.561163096805103e-06 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 9.352221240988001e-06 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 9.11961069505196e-06 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 8.994205018098e-06 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 8.773658009886276e-06 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.618180800112896e-06 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 8.479655662085861e-06 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 8.290709047287237e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 8.174911272362806e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 8.023170266824309e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 7.882628779043444e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 7.774051482556388e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 7.63116440793965e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 7.527200068579987e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 7.413311777781928e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.295079740288202e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 7.207193903013831e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 7.096233275660779e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 7.003015525697265e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 6.917638074810384e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 6.820013368269429e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 6.742901859979611e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 6.659577593381982e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 6.57932923786575e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 6.509173999802442e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.432485406548949e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 6.366857633111067e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 6.300084351096302e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 6.2330586843017954e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 6.175334874569671e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 6.1123182604205795e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.054668119759299e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.001881502015749e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 5.946666988165816e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 5.897874416405102e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 5.8495770645095035e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 5.802778105135076e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 5.759732175647514e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 5.717062322219135e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 5.678294655808713e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 5.640118160954444e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 5.602819783234736e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 5.568830601987429e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 5.534083811653545e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 5.5015830184856895e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 5.4712190831196494e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 5.440813310997328e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 5.412489826994715e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.384842552302871e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.357620921131456e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.33145021108794e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.305962531565456e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.281917310639983e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.258196779323043e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9071, 0.5570, 0.9511, 0.8868, 0.5907, 0.7400, 0.5016, 0.7028,\n",
      "         0.6890, 0.8750, 0.6623, 0.6961, 0.8846, 0.7430, 0.8017, 0.6675, 0.6937,\n",
      "         0.4289, 0.1910, 0.9256, 0.5528, 0.6727, 0.8718, 0.5180, 0.7412, 0.8440,\n",
      "         0.4695, 0.8504, 0.6638, 0.5295, 0.4855, 0.7440, 0.4868, 0.9641, 0.5604,\n",
      "         0.8191, 0.6545, 0.5274, 0.1985, 0.8400, 0.1503, 0.5025, 0.0894, 0.3620]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.258196779323043e-06 from epoch 199\n",
      "MLP predicts 0.49 for case 20 with [0.26]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1528467833995819 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.152847\n",
      "0.001\n",
      "Best training loss: 0.0288712065666914 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.022162016481161118 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.046149\n",
      "0.001\n",
      "Best training loss: 0.015850795432925224 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.011073635891079903 at learning rate 0.001 and epoch 17\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.018507\n",
      "0.001\n",
      "Best training loss: 0.007621187251061201 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.005197274964302778 at learning rate 0.001 and epoch 27\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.007716\n",
      "0.001\n",
      "Best training loss: 0.0035490556620061398 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.00241629546508193 at learning rate 0.001 and epoch 37\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.003075\n",
      "0.001\n",
      "Best training loss: 0.0016850574174895883 at learning rate 0.001 and epoch 42\n",
      "Best training loss: 0.0012238590279594064 at learning rate 0.001 and epoch 47\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001289\n",
      "0.001\n",
      "Best training loss: 0.0009074904373846948 at learning rate 0.001 and epoch 52\n",
      "Best training loss: 0.0008668192895129323 at learning rate 0.001 and epoch 55\n",
      "Best training loss: 0.0006777045200578868 at learning rate 0.001 and epoch 57\n",
      "Best training loss: 0.0006010692450217903 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000601\n",
      "0.001\n",
      "Best training loss: 0.0005096131353639066 at learning rate 0.001 and epoch 62\n",
      "Best training loss: 0.0004256096144672483 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.00038107510772533715 at learning rate 0.001 and epoch 67\n",
      "Best training loss: 0.000305005261907354 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000305\n",
      "0.001\n",
      "Best training loss: 0.0002839936933014542 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.00022292092035058886 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00021364906569942832 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.0001674143859418109 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000167\n",
      "0.001\n",
      "Best training loss: 0.00016299269918818027 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00012835282541345805 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00012553640408441424 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.0001233787479577586 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 0.00010002535418607295 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000100\n",
      "0.001\n",
      "Best training loss: 9.769710595719516e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 9.347740706289187e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 7.890254346420988e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.660395203856751e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 7.216895028250292e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.283123366301879e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000063\n",
      "0.001\n",
      "Best training loss: 6.062269676476717e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.671434701071121e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.050210529589094e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.839424946112558e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.522570452536456e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.0953349525807425e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000041\n",
      "0.001\n",
      "Best training loss: 3.905047196894884e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.6577210266841576e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.352300700498745e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.3383697882527485e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.1825627957005054e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.995865543198306e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.991967812704388e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.772197149170097e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000028\n",
      "0.001\n",
      "Best training loss: 2.731357017182745e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.6277117285644636e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.487153142283205e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.464984754624311e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.3180113203125075e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.2696993255522102e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.1939127691439353e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.0894640329061076e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.0615274479496293e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.9586545022320934e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000020\n",
      "0.001\n",
      "Best training loss: 1.9109300410491414e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.8556316717877053e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.777062425389886e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.746709313010797e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.6751955627114512e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.6329551726812497e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.589671774127055e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.5313080439227633e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.5033471754577477e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.450940817449009e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000015\n",
      "0.001\n",
      "Best training loss: 1.4148602531349752e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3818133993481752e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.3375650269153994e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.3120627954776864e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2740159036184195e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.244260147359455e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.2172909919172525e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1839192666229792e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1621179510257207e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.1329318112984765e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.1085057849413715e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.087140026356792e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0611372999846935e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0422582818137016e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.0200371434621047e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.0000469046644866e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.822260835790075e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.621228855394293e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.463437891099602e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.285258784075268e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 9.121367838815786e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.97567042557057e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.812815394776408e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.67757717060158e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.536415407434106e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.401007107750047e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.278048881038558e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.148459528456442e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.03549301053863e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.918970368336886e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.807957445038483e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.707323675276712e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.601388915645657e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.506550900870934e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.412490504066227e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.320539225474931e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.235219982248964e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.149370503611863e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.0704782046959735e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.992504040681524e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.9172510848147795e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.847244094387861e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.775832844141405e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.709733042953303e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.6454822444939055e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.583062713616528e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.524451691802824e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.466624199674698e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.411890808522003e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.358154678309802e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.306214345386252e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.257225322769955e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.208252216310939e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.161982128105592e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.117652446846478e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.074120392440818e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.032648798282025e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.99218628849485e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.953117579338141e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8313, 0.9071, 0.5566, 0.9512, 0.8863, 0.5912, 0.7406, 0.5014, 0.7023,\n",
      "         0.6881, 0.8738, 0.6624, 0.6964, 0.8848, 0.7429, 0.8014, 0.6673, 0.6934,\n",
      "         0.4282, 0.1905, 0.9250, 0.5523, 0.6725, 0.8709, 0.5182, 0.7418, 0.8436,\n",
      "         0.4694, 0.8504, 0.6638, 0.5290, 0.4845, 0.7444, 0.4866, 0.2678, 0.5602,\n",
      "         0.8188, 0.6541, 0.5274, 0.1983, 0.8388, 0.1505, 0.5025, 0.0894, 0.3616]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.953117579338141e-06 from epoch 199\n",
      "MLP predicts 0.759 for case 25 with [0.97]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15644623339176178 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.156446\n",
      "0.001\n",
      "Best training loss: 0.0291850995272398 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.023138605058193207 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.044471\n",
      "0.001\n",
      "Best training loss: 0.01696072146296501 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.011996012181043625 at learning rate 0.001 and epoch 17\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.017246\n",
      "0.001\n",
      "Best training loss: 0.008339149877429008 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.005737762898206711 at learning rate 0.001 and epoch 27\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.007115\n",
      "0.001\n",
      "Best training loss: 0.003951879218220711 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.0027160695753991604 at learning rate 0.001 and epoch 37\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.002831\n",
      "0.001\n",
      "Best training loss: 0.001913028652779758 at learning rate 0.001 and epoch 42\n",
      "Best training loss: 0.0018133653793483973 at learning rate 0.001 and epoch 45\n",
      "Best training loss: 0.0013946493854746222 at learning rate 0.001 and epoch 47\n",
      "Best training loss: 0.001201472245156765 at learning rate 0.001 and epoch 50\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001201\n",
      "0.001\n",
      "Best training loss: 0.0010362564353272319 at learning rate 0.001 and epoch 52\n",
      "Best training loss: 0.0008197937859222293 at learning rate 0.001 and epoch 55\n",
      "Best training loss: 0.0007738703861832619 at learning rate 0.001 and epoch 57\n",
      "Best training loss: 0.0005780418287031353 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000578\n",
      "0.001\n",
      "Best training loss: 0.0004164941783528775 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.00030323045211844146 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000303\n",
      "0.001\n",
      "Best training loss: 0.00022457422164734453 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.0002209699887316674 at learning rate 0.001 and epoch 78\n",
      "Best training loss: 0.0001702338777249679 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000170\n",
      "0.001\n",
      "Best training loss: 0.0001600760588189587 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.0001313141401624307 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.0001191888441098854 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 0.00010252257925458252 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000103\n",
      "0.001\n",
      "Best training loss: 9.096543362829834e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 8.086119487415999e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 8.066809823503718e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 7.070806896081194e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.424708408303559e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000064\n",
      "0.001\n",
      "Best training loss: 6.308838055701926e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.586805127677508e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.151304503669962e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 5.145793329575099e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.987483043805696e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.4776414142688736e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.1651241190265864e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000042\n",
      "0.001\n",
      "Best training loss: 4.085359978489578e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.99260752601549e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.633653250290081e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.398700209800154e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.298070441815071e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.231114169466309e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.9823422664776444e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.7998245059279725e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000028\n",
      "0.001\n",
      "Best training loss: 2.702425445022527e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.6526768124313094e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.4781653337413445e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.4582397600170225e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.334718737984076e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.249917815788649e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.2067795725888573e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.0853231035289355e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.0552717614918947e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.9696042727446184e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000020\n",
      "0.001\n",
      "Best training loss: 1.899396738735959e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.86327251867624e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.7761338312993757e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.742688073136378e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.6830517779453658e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.62641463248292e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.5941217498038895e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.5320287275244482e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.5006549801910296e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.456202517147176e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000015\n",
      "0.001\n",
      "Best training loss: 1.411623725289246e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3846081856172532e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.338690344709903e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.3105022844683845e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.277744559047278e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2426409739418887e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.2190619600005448e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1853309842990711e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1614321920205839e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.1355973583704326e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.108121796278283e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.088435328711057e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0623627531458624e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.042034818965476e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.0219193427474238e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.999632311519235e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.829989721765742e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.630824024497997e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.46266209211899e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.298988516093232e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 9.125879842031281e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.985108252090868e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.824982614896726e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.683125088282395e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.549751328246202e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.407979294133838e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.28705833555432e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.159991011780221e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.04262890596874e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.931420441309456e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.817866389814299e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.717985681665596e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.613621619384503e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.515988727391232e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.424820068990812e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.33153183318791e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.247847406688379e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.1645390562480316e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.0850296651769895e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.010043191257864e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.935671990504488e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.866688181617064e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.79754521115683e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.7312439568922855e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.6687612161331344e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.605872385989642e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.547112661792198e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.490237865364179e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.435135219362564e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.382419542205753e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.3305556068371516e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.2814306147629395e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.233446129044751e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.1874216044088826e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.14395958109526e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.100291557231685e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.058487088012043e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.018131443852326e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.9791213971038815e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9073, 0.5571, 0.9512, 0.8868, 0.5911, 0.7411, 0.5016, 0.7027,\n",
      "         0.6887, 0.8745, 0.6627, 0.6964, 0.8849, 0.7429, 0.8014, 0.6674, 0.6938,\n",
      "         0.4282, 0.1905, 0.9250, 0.5521, 0.6726, 0.8710, 0.5178, 0.7414, 0.8439,\n",
      "         0.4693, 0.8504, 0.6638, 0.5291, 0.4849, 0.7445, 0.4865, 0.2678, 0.9645,\n",
      "         0.8190, 0.6541, 0.5273, 0.1981, 0.8390, 0.1504, 0.5030, 0.0894, 0.3616]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.9791213971038815e-06 from epoch 199\n",
      "MLP predicts 0.655 for case 26 with [0.56]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1537029892206192 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.153703\n",
      "0.001\n",
      "Best training loss: 0.030797243118286133 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.022742334753274918 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.022742\n",
      "0.001\n",
      "Best training loss: 0.012054849416017532 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.008220575749874115 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.008221\n",
      "0.001\n",
      "Best training loss: 0.0064076767303049564 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.005242064595222473 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.005242\n",
      "0.001\n",
      "Best training loss: 0.003722734749317169 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.002171852393075824 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.003500\n",
      "0.001\n",
      "Best training loss: 0.0014998405240476131 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.001201279112137854 at learning rate 0.001 and epoch 48\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002149\n",
      "0.001\n",
      "Best training loss: 0.0010278939735144377 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0010076049948111176 at learning rate 0.001 and epoch 53\n",
      "Best training loss: 0.0006708255968987942 at learning rate 0.001 and epoch 56\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001121\n",
      "0.001\n",
      "Best training loss: 0.0004952053423039615 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.0004901806823909283 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00039020818076096475 at learning rate 0.001 and epoch 66\n",
      "Best training loss: 0.0003226483822800219 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000513\n",
      "0.001\n",
      "Best training loss: 0.00031122067593969405 at learning rate 0.001 and epoch 71\n",
      "Best training loss: 0.00022922440257389098 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.00022522255312651396 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.0001743363100104034 at learning rate 0.001 and epoch 79\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000226\n",
      "0.001\n",
      "Best training loss: 0.0001565569982631132 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00013787455100100487 at learning rate 0.001 and epoch 84\n",
      "Best training loss: 0.00011545378220034763 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00011036661453545094 at learning rate 0.001 and epoch 89\n",
      "Best training loss: 0.00010992127499775961 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000110\n",
      "0.001\n",
      "Best training loss: 8.908577001420781e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 8.836865163175389e-05 at learning rate 0.001 and epoch 94\n",
      "Best training loss: 8.1195670645684e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.065082900226116e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 7.037538307486102e-05 at learning rate 0.001 and epoch 99\n",
      "Best training loss: 6.212312291609123e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000062\n",
      "0.001\n",
      "Best training loss: 5.68356153962668e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.593808964476921e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 4.895143865724094e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.607100709108636e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.441127020982094e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 3.938266672776081e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000039\n",
      "0.001\n",
      "Best training loss: 3.747743176063523e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.524169005686417e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.2155712688108906e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.201155050192028e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.065776763833128e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.8579837817233056e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.6579284167382866e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000027\n",
      "0.001\n",
      "Best training loss: 2.59372936852742e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.524910269130487e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.360691360081546e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.3482140022679232e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.219558518845588e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.1400772311608307e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.101157770084683e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.9797251297859475e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.949488250829745e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.8723756511462852e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000019\n",
      "0.001\n",
      "Best training loss: 1.796910146367736e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.766290188243147e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.6818954463815317e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.6445959772681817e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.5944255210342817e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.5314562915591523e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.5049143257783726e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4462507351709064e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4086424926063046e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.375200918118935e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3256148122309241e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3002754712942988e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.260455246665515e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2266780686331913e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2010897989966907e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1640144293778576e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1409871149226092e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1121298484795261e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.0836523870239034e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.063505987985991e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0352921890444122e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0145355190616101e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 9.934765330399387e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.700795999378897e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.527935617370531e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.317654075857718e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.138354471360799e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 8.971562238002662e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 8.786806574789807e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.644439731142484e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.483155397698283e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.33416106615914e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.204758159990888e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.058461389737204e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 7.936820111353882e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 7.813734555384144e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.691314749536105e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.5845409810426645e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.470807304343907e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.3690225690370426e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.268683020811295e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.168511729105376e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.0806240728416014e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 6.98793610354187e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 6.902901077410206e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 6.823154762969352e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.740879143762868e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.666800345556112e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.592942554561887e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.521644536405802e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.453638889070135e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.385574124578852e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.322428362182109e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.259576366574038e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.198919436428696e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.142562597233336e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.085739187255967e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.0320753618725576e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 5.979825346003054e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 5.928048267378472e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 5.878915544599295e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 5.831089765706565e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 5.78533854422858e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.741632321587531e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.698947461496573e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.658721420331858e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.619378043775214e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.5815380619606e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.54515145267942e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9072, 0.5568, 0.9513, 0.8866, 0.5912, 0.7391, 0.5011, 0.7031,\n",
      "         0.6880, 0.8743, 0.6612, 0.6968, 0.8844, 0.7430, 0.8012, 0.6673, 0.6932,\n",
      "         0.4282, 0.1905, 0.9247, 0.5522, 0.6727, 0.8712, 0.5180, 0.7410, 0.8433,\n",
      "         0.4691, 0.8502, 0.6638, 0.5293, 0.4852, 0.7444, 0.4865, 0.2675, 0.9645,\n",
      "         0.5604, 0.6541, 0.5266, 0.1985, 0.8392, 0.1494, 0.5029, 0.0895, 0.3619]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.54515145267942e-06 from epoch 199\n",
      "MLP predicts 0.532 for case 27 with [0.82]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15583188831806183 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.155832\n",
      "0.001\n",
      "Best training loss: 0.03276403248310089 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.019821738824248314 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.019822\n",
      "0.001\n",
      "Best training loss: 0.012238954193890095 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.010463833808898926 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.010464\n",
      "0.001\n",
      "Best training loss: 0.010112299583852291 at learning rate 0.001 and epoch 23\n",
      "Best training loss: 0.009561615996062756 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.005177622195333242 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.008388\n",
      "0.001\n",
      "Best training loss: 0.0032722805626690388 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.0025676845107227564 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.005450\n",
      "0.001\n",
      "Best training loss: 0.002158320276066661 at learning rate 0.001 and epoch 41\n",
      "Best training loss: 0.0012862340081483126 at learning rate 0.001 and epoch 46\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002813\n",
      "0.001\n",
      "Best training loss: 0.000967981934081763 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0009121780749410391 at learning rate 0.001 and epoch 54\n",
      "Best training loss: 0.0008205064805224538 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.0005789635470137 at learning rate 0.001 and epoch 59\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001162\n",
      "0.001\n",
      "Best training loss: 0.00042647935333661735 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.0003922925388906151 at learning rate 0.001 and epoch 67\n",
      "Best training loss: 0.0003397673135623336 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000425\n",
      "0.001\n",
      "Best training loss: 0.00026136849191971123 at learning rate 0.001 and epoch 72\n",
      "Best training loss: 0.0001928555138874799 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00017369458510074764 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000174\n",
      "0.001\n",
      "Best training loss: 0.00015236271428875625 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.0001250591012649238 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00012312184844631702 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00011860975064337254 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 9.627468534745276e-05 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000096\n",
      "0.001\n",
      "Best training loss: 8.605735638411716e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 7.704022573307157e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 6.5825050114654e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.256060441955924e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000063\n",
      "0.001\n",
      "Best training loss: 6.03803237027023e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 5.22848276887089e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.0812552217394114e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.664605876314454e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.24386998929549e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.11879263992887e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000041\n",
      "0.001\n",
      "Best training loss: 3.715770799317397e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.482363172224723e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.363245923537761e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.333309723529965e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.027097409358248e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 2.8724813091685064e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.715656410146039e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.7133148250868544e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000027\n",
      "0.001\n",
      "Best training loss: 2.507343378965743e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.46734925894998e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.3855465769884177e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.2421903850045055e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.2294783775578253e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.1041938452981412e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.0391405996633694e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 1.995823549805209e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.885686106106732e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.8601194824441336e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000019\n",
      "0.001\n",
      "Best training loss: 1.7848464267444797e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.7173695596284233e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.689651980996132e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.6110816432046704e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.5765575881232508e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.531764428364113e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.4729089343745727e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4478493540082127e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.395213621435687e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.3592342838819604e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3287754882185254e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.2829212209908292e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2594253348652273e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2233761481184047e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.1906960935448296e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1685201570799109e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1339740012772381e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1115956112917047e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.087060445570387e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0597108484944329e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0413840755063575e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0166319043491967e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 9.96769085759297e-06 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 9.78398384177126e-06 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.568674613547046e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.413777661393397e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.228530871041585e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.05877459445037e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 8.91872332431376e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.749566404731013e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.615281331003644e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.477712981402874e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.335948223248124e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.218744369514752e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.088717549981084e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 7.973160791152623e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.861991434765514e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.745692528260406e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.646753147128038e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.5404545896162745e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.4406912062841e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.350223768298747e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.2530528996139765e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.166901013988536e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.082192041707458e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 6.9981906563043594e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.921716249053134e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.844130439276341e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.771915650460869e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.701307029288728e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.631363703490933e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.567001491930569e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.501820280391257e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.439864137064433e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.381861112458864e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.323283287201775e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.269048753893003e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.216779638634762e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.165694685478229e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.117958946560975e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.071621101000346e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.027751169312978e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 5.9852495724044275e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.944087661191588e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.9051467360404786e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.866823812539224e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.830166628584266e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.795651759399334e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.761772172263591e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9071, 0.5570, 0.9513, 0.8866, 0.5906, 0.7410, 0.5013, 0.7028,\n",
      "         0.6888, 0.8743, 0.6624, 0.6965, 0.8849, 0.7430, 0.8015, 0.6673, 0.6936,\n",
      "         0.4283, 0.1905, 0.9253, 0.5521, 0.6726, 0.8711, 0.5180, 0.7415, 0.8439,\n",
      "         0.4693, 0.8503, 0.6638, 0.5290, 0.4849, 0.7443, 0.4864, 0.2670, 0.9646,\n",
      "         0.5603, 0.8191, 0.5270, 0.1981, 0.8393, 0.1502, 0.5027, 0.0895, 0.3617]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.761772172263591e-06 from epoch 199\n",
      "MLP predicts 0.522 for case 29 with [0.66]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15751369297504425 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.157514\n",
      "0.001\n",
      "Best training loss: 0.03126559779047966 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.026459859684109688 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.026460\n",
      "0.001\n",
      "Best training loss: 0.013843472115695477 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.008650392293930054 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.008650\n",
      "0.001\n",
      "Best training loss: 0.006034699734300375 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.004514565225690603 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.004515\n",
      "0.001\n",
      "Best training loss: 0.0035140998661518097 at learning rate 0.001 and epoch 35\n",
      "Best training loss: 0.0026443114038556814 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.002794\n",
      "0.001\n",
      "Best training loss: 0.0015874267555773258 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.001100702560506761 at learning rate 0.001 and epoch 48\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001789\n",
      "0.001\n",
      "Best training loss: 0.0008488342282362282 at learning rate 0.001 and epoch 53\n",
      "Best training loss: 0.0007785194902680814 at learning rate 0.001 and epoch 56\n",
      "Best training loss: 0.0006858452688902617 at learning rate 0.001 and epoch 58\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001016\n",
      "0.001\n",
      "Best training loss: 0.0005112160579301417 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.0003648085694294423 at learning rate 0.001 and epoch 66\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000508\n",
      "0.001\n",
      "Best training loss: 0.0002743766235653311 at learning rate 0.001 and epoch 71\n",
      "Best training loss: 0.0002451233158353716 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.00021363646374084055 at learning rate 0.001 and epoch 76\n",
      "Best training loss: 0.0001737062557367608 at learning rate 0.001 and epoch 79\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000240\n",
      "0.001\n",
      "Best training loss: 0.00016931438585743308 at learning rate 0.001 and epoch 81\n",
      "Best training loss: 0.00013065476377960294 at learning rate 0.001 and epoch 84\n",
      "Best training loss: 0.0001227158063556999 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00010215373185928911 at learning rate 0.001 and epoch 89\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000118\n",
      "0.001\n",
      "Best training loss: 9.094367123907432e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 8.156186959240586e-05 at learning rate 0.001 and epoch 94\n",
      "Best training loss: 6.99658558005467e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 6.56457559671253e-05 at learning rate 0.001 and epoch 99\n",
      "Best training loss: 6.49096691631712e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000065\n",
      "0.001\n",
      "Best training loss: 5.535191303351894e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.306569073582068e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 5.019806121708825e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.4628559408010915e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.299209103919566e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 3.9793765608919784e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000040\n",
      "0.001\n",
      "Best training loss: 3.644046591944061e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.633222877397202e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.498787191347219e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.220951839466579e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.0046292522456497e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 2.9279031878104433e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.8644595658988692e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.6533349227975123e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000027\n",
      "0.001\n",
      "Best training loss: 2.5007413569255732e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.4055607354966924e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.371793561906088e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.2198772057890892e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.1938963982393034e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.103040969814174e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.0128734831814654e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.9850253011099994e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.881262141978368e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000019\n",
      "0.001\n",
      "Best training loss: 1.8430921045364812e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.785201493476052e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.710017750156112e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.684119888523128e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.6125297406688333e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.571349821460899e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.5330857422668487e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4737295714439824e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.4473738701781258e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.399009306624066e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.3609378584078513e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.3320604011823889e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2876110304205213e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2630630408239085e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2286248420423362e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.196271477965638e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.174007684312528e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1405884833948221e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1181688932992984e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0934596502920613e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0666945854609367e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0476884199306369e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0228405699308496e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0031067176896613e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.838017831498291e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.623906407796312e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.463750757277012e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.270287591789383e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.100837814912666e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.949624316301197e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.776087270234711e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.636749953438994e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.487986633554101e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.344540219695773e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.217403774324339e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.078995961113833e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.959752110764384e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.838147212169133e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.71908071328653e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.6146889114170335e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.5025122896477114e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.4020745159941725e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.3061314651567955e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.208523584267823e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.120907412172528e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.031973837001715e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 6.948817826923914e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.869061508041341e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.790284714952577e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.719241810060339e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.648612270510057e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.581818524864502e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.519732778542675e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.456697974499548e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.397758170351153e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.339428182400297e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.282189588091569e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.228000074770534e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.174721420393325e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.1241498769959435e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.075059445720399e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.027390554663725e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 5.982159109407803e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 5.937122750765411e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.893877641938161e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.8522809922578745e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.811273240396986e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.7728793763089925e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.735905233450467e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9072, 0.5569, 0.9510, 0.8875, 0.5922, 0.7413, 0.5014, 0.7030,\n",
      "         0.6888, 0.8744, 0.6623, 0.6965, 0.8849, 0.7433, 0.8010, 0.6673, 0.6937,\n",
      "         0.4284, 0.1904, 0.9247, 0.5519, 0.6726, 0.8714, 0.5180, 0.7413, 0.8439,\n",
      "         0.4693, 0.8502, 0.6637, 0.5294, 0.4848, 0.7446, 0.4867, 0.2675, 0.9645,\n",
      "         0.5603, 0.8194, 0.6543, 0.1981, 0.8399, 0.1503, 0.5028, 0.0895, 0.3617]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.735905233450467e-06 from epoch 199\n",
      "MLP predicts 0.635 for case 32 with [0.52]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15914949774742126 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.159149\n",
      "0.001\n",
      "Best training loss: 0.03191576525568962 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.021484320983290672 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.021484\n",
      "0.001\n",
      "Best training loss: 0.011799664236605167 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.0087220324203372 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.008722\n",
      "0.001\n",
      "Best training loss: 0.007233846466988325 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.006666984874755144 at learning rate 0.001 and epoch 28\n",
      "Best training loss: 0.006105523556470871 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.006106\n",
      "0.001\n",
      "Best training loss: 0.0037073728162795305 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.0023248225916177034 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.004060\n",
      "0.001\n",
      "Best training loss: 0.0016958724008873105 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0013806765200570226 at learning rate 0.001 and epoch 48\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002391\n",
      "0.001\n",
      "Best training loss: 0.001077597145922482 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0007290343637578189 at learning rate 0.001 and epoch 56\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001212\n",
      "0.001\n",
      "Best training loss: 0.0005498583777807653 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.0005217718426138163 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.0004376150027383119 at learning rate 0.001 and epoch 66\n",
      "Best training loss: 0.00035298053990118206 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000553\n",
      "0.001\n",
      "Best training loss: 0.0003503944317344576 at learning rate 0.001 and epoch 71\n",
      "Best training loss: 0.0002560919092502445 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.0002467504527885467 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00019665033323690295 at learning rate 0.001 and epoch 79\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000247\n",
      "0.001\n",
      "Best training loss: 0.00017401306831743568 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.0001560164091642946 at learning rate 0.001 and epoch 84\n",
      "Best training loss: 0.00012980939936824143 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.0001250202622031793 at learning rate 0.001 and epoch 89\n",
      "Best training loss: 0.00012245072866789997 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000122\n",
      "0.001\n",
      "Best training loss: 0.00010101923544425517 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 0.00010028616088675335 at learning rate 0.001 and epoch 94\n",
      "Best training loss: 9.153768769465387e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 8.070292096817866e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 8.01521455287002e-05 at learning rate 0.001 and epoch 99\n",
      "Best training loss: 7.084455864969641e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000071\n",
      "0.001\n",
      "Best training loss: 6.531753024319187e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 6.518706504721195e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 6.402398139471188e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 5.637895446852781e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 5.325693928170949e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 5.1119397539878264e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.5754368329653516e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000046\n",
      "0.001\n",
      "Best training loss: 4.358984006103128e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 4.0994680603034794e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.764317443710752e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.729738455149345e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.588126855902374e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.354067666805349e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 3.13041964545846e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000031\n",
      "0.001\n",
      "Best training loss: 3.0474318919004872e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.9717635698034428e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.7890917408512905e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.7663250875775702e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.627315916470252e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.532861617510207e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.487182973709423e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.3529368263552897e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.3127109670895152e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 2.2278296455624513e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000022\n",
      "0.001\n",
      "Best training loss: 2.1404317521955818e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 2.1028063201811165e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 2.0091296391910873e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.9627470464911312e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.9059069018112496e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.833616988733411e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.8003087461693212e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.734297620714642e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.688806150923483e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.6494394003530033e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000016\n",
      "0.001\n",
      "Best training loss: 1.592366606928408e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.5607625755365007e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.5149405953707173e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.4744750842510257e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.4435477169172373e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.4004926924826577e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.3720031347475015e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.338210222456837e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.3044091247138567e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.2797639101336244e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000013\n",
      "0.001\n",
      "Best training loss: 1.2468554814404342e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.2215072274557315e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.1962925782427192e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.1684856872307137e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.1473448466858827e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.1225027265027165e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 1.1006782187905628e-05 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 1.0804091289173812e-05 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 1.0582777576928493e-05 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 1.0405029570392799e-05 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 1.0209506399405655e-05 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 1.0026881682279054e-05 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 9.866302207228728e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 9.688037607702427e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 9.53594280872494e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 9.383265023643617e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 9.230453542841133e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 9.094686902244575e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.95173980097752e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 8.821592018648516e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.693962627148721e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 8.566070391680114e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 8.451701432932168e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 8.333363439305685e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 8.222948963521048e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 8.119160156638827e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 8.013120350369718e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.915856258478016e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.819629900041036e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.7260929174372e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.638223905814812e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 7.550925602117786e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 7.46913110560854e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 7.3882806645997334e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 7.310239652724704e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 7.236736109916819e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 7.163157079048688e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 7.09358664607862e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 7.026490038697375e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.960379778320203e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.897472303535324e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.836325610493077e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.777044291084167e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.719889825035352e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.664620286755962e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.61176045468892e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.559986559295794e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.510068033094285e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.4627115534676705e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9075, 0.5572, 0.9510, 0.8864, 0.5915, 0.7420, 0.5014, 0.7030,\n",
      "         0.6892, 0.8745, 0.6623, 0.6960, 0.8848, 0.7427, 0.8011, 0.6673, 0.6937,\n",
      "         0.4280, 0.1903, 0.9245, 0.5521, 0.6727, 0.8706, 0.5176, 0.7417, 0.8439,\n",
      "         0.4692, 0.8505, 0.6639, 0.5291, 0.4848, 0.7444, 0.4875, 0.2685, 0.9646,\n",
      "         0.5604, 0.8186, 0.6541, 0.5276, 0.8390, 0.1504, 0.5027, 0.0898, 0.3620]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 6.4627115534676705e-06 from epoch 199\n",
      "MLP predicts 0.51 for case 34 with [0.19]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15468807518482208 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.154688\n",
      "0.001\n",
      "Best training loss: 0.029537400230765343 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.024966513738036156 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.038552\n",
      "0.001\n",
      "Best training loss: 0.019472584128379822 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.014360643923282623 at learning rate 0.001 and epoch 17\n",
      "Best training loss: 0.013617492280900478 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.013617\n",
      "0.001\n",
      "Best training loss: 0.010296666994690895 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.008453605696558952 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.007236637640744448 at learning rate 0.001 and epoch 27\n",
      "Best training loss: 0.005309774074703455 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.005310\n",
      "0.001\n",
      "Best training loss: 0.005089079029858112 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.0033183088526129723 at learning rate 0.001 and epoch 35\n",
      "Best training loss: 0.0021046584006398916 at learning rate 0.001 and epoch 40\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.002105\n",
      "0.001\n",
      "Best training loss: 0.0014052374754101038 at learning rate 0.001 and epoch 45\n",
      "Best training loss: 0.0009945021010935307 at learning rate 0.001 and epoch 50\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.000995\n",
      "0.001\n",
      "Best training loss: 0.000730914412997663 at learning rate 0.001 and epoch 55\n",
      "Best training loss: 0.0005525373271666467 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000553\n",
      "0.001\n",
      "Best training loss: 0.000523783266544342 at learning rate 0.001 and epoch 63\n",
      "Best training loss: 0.00042156942072324455 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.00035826428211294115 at learning rate 0.001 and epoch 68\n",
      "Best training loss: 0.00031986759859137237 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000320\n",
      "0.001\n",
      "Best training loss: 0.0002532326616346836 at learning rate 0.001 and epoch 73\n",
      "Best training loss: 0.00024352075706701726 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.0001860827615018934 at learning rate 0.001 and epoch 78\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000187\n",
      "0.001\n",
      "Best training loss: 0.00018458101840224117 at learning rate 0.001 and epoch 81\n",
      "Best training loss: 0.00014118957915343344 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.00013321932055987418 at learning rate 0.001 and epoch 86\n",
      "Best training loss: 0.00010946019028779119 at learning rate 0.001 and epoch 88\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000111\n",
      "0.001\n",
      "Best training loss: 9.925824997480959e-05 at learning rate 0.001 and epoch 91\n",
      "Best training loss: 8.61051375977695e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 7.60061593609862e-05 at learning rate 0.001 and epoch 96\n",
      "Best training loss: 6.840803689556196e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.779393152100965e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000068\n",
      "0.001\n",
      "Best training loss: 5.943890573689714e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 5.478038656292483e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.349111233954318e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.7332167014246807e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 4.421948688104749e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.317459024605341e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 4.2544408643152565e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000043\n",
      "0.001\n",
      "Best training loss: 3.8199716072995216e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 3.591819040593691e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.453491081018001e-05 at learning rate 0.001 and epoch 114\n",
      "Best training loss: 3.4211392630822957e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.119201210211031e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 2.9433631425490603e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.8099151677452028e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.781559669529088e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000028\n",
      "0.001\n",
      "Best training loss: 2.5771150831133127e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.5562101654941216e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.4356959329452366e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.3223628886626102e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.29597972065676e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.1558751541306265e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.1185063815210015e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.041430889221374e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 1.9499357222230174e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 1.9213295672670938e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000019\n",
      "0.001\n",
      "Best training loss: 1.8260598153574392e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.785125641617924e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.730738949845545e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.6600331946392544e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.63276035891613e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.565695674798917e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.526777123217471e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.4881626157148276e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.433819215890253e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.4075986655370798e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000014\n",
      "0.001\n",
      "Best training loss: 1.360783790005371e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.326831534242956e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.2967740985914133e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.2560062714328524e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.2329007404332515e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.1983537660853472e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.1698090929712635e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.1465551324363332e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.1152026672789361e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.0945173926302232e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000011\n",
      "0.001\n",
      "Best training loss: 1.0689816008380149e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.0453577488078736e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.0260006092721596e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0021050911745988e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 9.844585292739794e-06 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 9.644225428928621e-06 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 9.451624464418273e-06 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.294626579503529e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.105495337280445e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 8.953464202932082e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.797584087005816e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 8.640019586891867e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 8.507261554768775e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.360402716789395e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.23363006929867e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.106745553959627e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 7.980153895914555e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 7.871329216868617e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 7.752801138849463e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 7.647212441952433e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.546324013674166e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.443421054631472e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.352693501161411e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.259632639033953e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.173111953306943e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.090415238053538e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.008428838162217e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 6.934593329788186e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 6.8592621573770884e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 6.788474365748698e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.722325906594051e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 6.654811386397341e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.592626959900372e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.5317067310388666e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.472679160651751e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.416400538000744e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.361080068018055e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.3094958022702485e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.258289431571029e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.209328148543136e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000006\n",
      "0.001\n",
      "Best training loss: 6.162872068671277e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.116636541264597e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.073311396903591e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.031719749444164e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 5.991182661091443e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 5.952503215667093e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 5.915112978982506e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 5.87930298934225e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 5.844513452757383e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9070, 0.5570, 0.9514, 0.8867, 0.5909, 0.7407, 0.5015, 0.7027,\n",
      "         0.6887, 0.8742, 0.6624, 0.6964, 0.8847, 0.7431, 0.8014, 0.6674, 0.6936,\n",
      "         0.4282, 0.1905, 0.9253, 0.5522, 0.6727, 0.8712, 0.5181, 0.7414, 0.8437,\n",
      "         0.4694, 0.8504, 0.6639, 0.5290, 0.4850, 0.7445, 0.4867, 0.2672, 0.9646,\n",
      "         0.5603, 0.8192, 0.6542, 0.5273, 0.1984, 0.1504, 0.5026, 0.0896, 0.3617]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 5.844513452757383e-06 from epoch 199\n",
      "MLP predicts 0.816 for case 41 with [0.84]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.1593247801065445 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.159325\n",
      "0.001\n",
      "Best training loss: 0.031747013330459595 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.020609157159924507 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.020609\n",
      "0.001\n",
      "Best training loss: 0.011423768475651741 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.00884668156504631 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.008847\n",
      "0.001\n",
      "Best training loss: 0.007655304856598377 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.006270522717386484 at learning rate 0.001 and epoch 28\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.006593\n",
      "0.001\n",
      "Best training loss: 0.003585604950785637 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.002329221460968256 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.004344\n",
      "0.001\n",
      "Best training loss: 0.0017480360111221671 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0016707824543118477 at learning rate 0.001 and epoch 46\n",
      "Best training loss: 0.0014476292999461293 at learning rate 0.001 and epoch 48\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002487\n",
      "0.001\n",
      "Best training loss: 0.001017680624499917 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0007180240354500711 at learning rate 0.001 and epoch 56\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001234\n",
      "0.001\n",
      "Best training loss: 0.0005647436482831836 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.0005004869308322668 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.00046042518806643784 at learning rate 0.001 and epoch 66\n",
      "Best training loss: 0.0003483366745058447 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000549\n",
      "0.001\n",
      "Best training loss: 0.00025908590760082006 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.0002392391616012901 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.0002024487912422046 at learning rate 0.001 and epoch 79\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000241\n",
      "0.001\n",
      "Best training loss: 0.00017159082926809788 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00016210235480684787 at learning rate 0.001 and epoch 84\n",
      "Best training loss: 0.000130454936879687 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.0001302170567214489 at learning rate 0.001 and epoch 89\n",
      "Best training loss: 0.00012084321497241035 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000121\n",
      "0.001\n",
      "Best training loss: 0.00010308424680260941 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 9.142851922661066e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 8.309360418934375e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 8.29069031169638e-05 at learning rate 0.001 and epoch 99\n",
      "Best training loss: 7.157302025007084e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000072\n",
      "0.001\n",
      "Best training loss: 6.74652328598313e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 6.541510083479807e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.744424561271444e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 5.499631879501976e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 5.1602568419184536e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.687835098593496e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000047\n",
      "0.001\n",
      "Best training loss: 4.492066000238992e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 4.1624807636253536e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.86645806429442e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.78147087758407e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.6880443076370284e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.4203025279566646e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 3.4193530154880136e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 3.21692532452289e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000032\n",
      "0.001\n",
      "Best training loss: 3.096934960922226e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 3.0460727430181578e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.851930730685126e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.8172373276902363e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.696504452615045e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.578730163804721e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.542244146752637e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.4062384909484535e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.3525431970483623e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 2.279728687426541e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000023\n",
      "0.001\n",
      "Best training loss: 2.1790596292703412e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 2.142504308721982e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 2.0515211872407235e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.9941859136451967e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.9439470634097233e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.8654878658708185e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.829457687563263e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.766446075635031e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.7133479559561238e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.6766303815529682e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000017\n",
      "0.001\n",
      "Best training loss: 1.6169457012438215e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.5817975508980453e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.5384823200292885e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.49322704601218e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.4627510608988814e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.4190640285960399e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.3872168892703485e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.3546413356380071e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.318000977335032e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.2924634575028904e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000013\n",
      "0.001\n",
      "Best training loss: 1.2592598977789748e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.2312046237639152e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.2063080248481128e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.1767115211114287e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.1542937500053085e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.129426254919963e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 1.1056652510887943e-05 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 1.0853264939214569e-05 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 1.0623623893479817e-05 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 1.043317843141267e-05 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 1.0236037269351073e-05 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 1.0040021152235568e-05 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 9.874236639006995e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 9.689925718703307e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 9.527383554086555e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 9.371719897899311e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 9.209049494529609e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 9.067224709724542e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.920651453081518e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 8.782701115706004e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.652899850858375e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 8.520403753209393e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 8.401230843446683e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 8.280252586700954e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 8.16402734926669e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 8.056763363128994e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.946568075567484e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.844164429116063e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.744482900307048e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.644965080544353e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.551750059064943e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 7.459645530616399e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 7.372183972620405e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 7.286852905963315e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 7.203185305115767e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 7.124789135559695e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 7.046970040391898e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.972727987886174e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.902404948050389e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.832861799921375e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.767980266886298e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.7060063884127885e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.646148449362954e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.5894396357180085e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.53495044389274e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.483076504082419e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.4321525314881e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.383153049682733e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.336695605568821e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8316, 0.9074, 0.5569, 0.9509, 0.8862, 0.5911, 0.7410, 0.5019, 0.7034,\n",
      "         0.6883, 0.8745, 0.6626, 0.6963, 0.8847, 0.7426, 0.8012, 0.6674, 0.6938,\n",
      "         0.4283, 0.1904, 0.9238, 0.5523, 0.6726, 0.8707, 0.5180, 0.7416, 0.8444,\n",
      "         0.4691, 0.8507, 0.6641, 0.5289, 0.4849, 0.7443, 0.4869, 0.2684, 0.9643,\n",
      "         0.5606, 0.8200, 0.6539, 0.5278, 0.1979, 0.8398, 0.5027, 0.0895, 0.3619]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 6.336695605568821e-06 from epoch 199\n",
      "MLP predicts 0.604 for case 44 with [0.15]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15805049240589142 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.158050\n",
      "0.001\n",
      "Best training loss: 0.028850026428699493 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.023584138602018356 at learning rate 0.001 and epoch 7\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.041908\n",
      "0.001\n",
      "Best training loss: 0.017522843554615974 at learning rate 0.001 and epoch 12\n",
      "Best training loss: 0.012398581951856613 at learning rate 0.001 and epoch 17\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.015480\n",
      "0.001\n",
      "Best training loss: 0.008599251508712769 at learning rate 0.001 and epoch 22\n",
      "Best training loss: 0.005887520499527454 at learning rate 0.001 and epoch 27\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.006103\n",
      "0.001\n",
      "Best training loss: 0.004052756354212761 at learning rate 0.001 and epoch 32\n",
      "Best training loss: 0.003799652447924018 at learning rate 0.001 and epoch 35\n",
      "Best training loss: 0.002810637466609478 at learning rate 0.001 and epoch 37\n",
      "Best training loss: 0.002383653772994876 at learning rate 0.001 and epoch 40\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.002384\n",
      "0.001\n",
      "Best training loss: 0.0020063959527760744 at learning rate 0.001 and epoch 42\n",
      "Best training loss: 0.001547188381664455 at learning rate 0.001 and epoch 45\n",
      "Best training loss: 0.001482193823903799 at learning rate 0.001 and epoch 47\n",
      "Best training loss: 0.0010537932394072413 at learning rate 0.001 and epoch 50\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.001054\n",
      "0.001\n",
      "Best training loss: 0.0007433234131895006 at learning rate 0.001 and epoch 55\n",
      "Best training loss: 0.0005405092961154878 at learning rate 0.001 and epoch 60\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.000541\n",
      "0.001\n",
      "Best training loss: 0.0004003901267424226 at learning rate 0.001 and epoch 65\n",
      "Best training loss: 0.0002986801555380225 at learning rate 0.001 and epoch 70\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000299\n",
      "0.001\n",
      "Best training loss: 0.000295635691145435 at learning rate 0.001 and epoch 73\n",
      "Best training loss: 0.00022581557277590036 at learning rate 0.001 and epoch 75\n",
      "Best training loss: 0.00021117056894581765 at learning rate 0.001 and epoch 78\n",
      "Best training loss: 0.0001741163432598114 at learning rate 0.001 and epoch 80\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000174\n",
      "0.001\n",
      "Best training loss: 0.00015644344966858625 at learning rate 0.001 and epoch 83\n",
      "Best training loss: 0.00013627736188936979 at learning rate 0.001 and epoch 85\n",
      "Best training loss: 0.00011906919826287776 at learning rate 0.001 and epoch 88\n",
      "Best training loss: 0.00010757785639725626 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000108\n",
      "0.001\n",
      "Best training loss: 9.257138299290091e-05 at learning rate 0.001 and epoch 93\n",
      "Best training loss: 8.552183135179803e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.307835039682686e-05 at learning rate 0.001 and epoch 98\n",
      "Best training loss: 6.83349571772851e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000068\n",
      "0.001\n",
      "Best training loss: 6.78570504533127e-05 at learning rate 0.001 and epoch 101\n",
      "Best training loss: 6.715639756293967e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.840930316480808e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.4967516916804016e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 5.3464445954887196e-05 at learning rate 0.001 and epoch 106\n",
      "Best training loss: 5.310404594638385e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.718404306913726e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.450876440387219e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000045\n",
      "0.001\n",
      "Best training loss: 4.277723564882763e-05 at learning rate 0.001 and epoch 111\n",
      "Best training loss: 4.252350117894821e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.8495418266393244e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.63500184903387e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.474341792752966e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.443252353463322e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.172102879034355e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 3.1629871955374256e-05 at learning rate 0.001 and epoch 119\n",
      "Best training loss: 2.9961067411932163e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000030\n",
      "0.001\n",
      "Best training loss: 2.861076791305095e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.8290864065638743e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.643087827891577e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.608779504953418e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.499471156625077e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.390461668255739e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.3550297555630095e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.2276302843238227e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.185627272410784e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 2.1081497834529728e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000021\n",
      "0.001\n",
      "Best training loss: 2.022852459049318e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.9891529518645257e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.8987198927788995e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.8563032426754944e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.8010598068940453e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.734626857796684e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.702258305158466e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.638195499253925e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.599907955096569e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.5573577911709435e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000016\n",
      "0.001\n",
      "Best training loss: 1.5064362742123194e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.4779921912122518e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.4305505828815512e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.3973706700198818e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.365065236313967e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.3255369594844524e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.3001753359276336e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.265160881303018e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.2373061508696992e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.2111442629247904e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000012\n",
      "0.001\n",
      "Best training loss: 1.1806772818090394e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.1591724614845589e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.1320145858917385e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.1088418432336766e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.08799049485242e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.0637969353410881e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 1.0451490197738167e-05 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 1.0240907613479067e-05 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 1.0048472177004442e-05 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.873549970507156e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 9.682195923232939e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 9.525804671284277e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 9.355157089885324e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 9.196028258884326e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 9.052859240910038e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.89716284291353e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.763612640905194e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.62721662997501e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.495636393490713e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 8.375542165595107e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 8.25081042421516e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 8.138531484291889e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 8.024816452234518e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.91549973655492e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.815217941242736e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.711381840636022e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.616772109031444e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.524437933170702e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.4338850026833825e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.349776751652826e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.26651569493697e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 7.188840299932053e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 7.113176252460107e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 7.040144282655092e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.9722482294309884e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.904247584316181e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.840823061793344e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.779925570299383e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.719858447468141e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.663140993623529e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.607552677451167e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.554457741003716e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.502498308691429e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.452534762502182e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.405147360055707e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.357961410685675e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.312895038718125e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.269064670050284e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.225674951565452e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8315, 0.9072, 0.5570, 0.9513, 0.8863, 0.5907, 0.7415, 0.5016, 0.7030,\n",
      "         0.6887, 0.8747, 0.6627, 0.6963, 0.8849, 0.7428, 0.8022, 0.6673, 0.6939,\n",
      "         0.4280, 0.1904, 0.9246, 0.5521, 0.6727, 0.8709, 0.5178, 0.7413, 0.8435,\n",
      "         0.4694, 0.8511, 0.6638, 0.5289, 0.4849, 0.7445, 0.4868, 0.2675, 0.9644,\n",
      "         0.5609, 0.8187, 0.6540, 0.5280, 0.1981, 0.8393, 0.1504, 0.0896, 0.3619]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.0857]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 6.225674951565452e-06 from epoch 199\n",
      "MLP predicts 0.859 for case 45 with [0.5]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15939219295978546 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.159392\n",
      "0.001\n",
      "Best training loss: 0.03145836666226387 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.020824914798140526 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.020825\n",
      "0.001\n",
      "Best training loss: 0.011648782528936863 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.008927978575229645 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.008928\n",
      "0.001\n",
      "Best training loss: 0.007569370325654745 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.006629565265029669 at learning rate 0.001 and epoch 28\n",
      "Best training loss: 0.006413021590560675 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.006413\n",
      "0.001\n",
      "Best training loss: 0.0037574779707938433 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.0023786164820194244 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.004188\n",
      "0.001\n",
      "Best training loss: 0.0017304858192801476 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.001401693793013692 at learning rate 0.001 and epoch 48\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002425\n",
      "0.001\n",
      "Best training loss: 0.001054513966664672 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0007140669040381908 at learning rate 0.001 and epoch 56\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001215\n",
      "0.001\n",
      "Best training loss: 0.0005447887815535069 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.0005033620400354266 at learning rate 0.001 and epoch 64\n",
      "Best training loss: 0.0004396993317641318 at learning rate 0.001 and epoch 66\n",
      "Best training loss: 0.0003435950493440032 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000547\n",
      "0.001\n",
      "Best training loss: 0.00025255701621063054 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.00023824871459510177 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.0001959862420335412 at learning rate 0.001 and epoch 79\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000240\n",
      "0.001\n",
      "Best training loss: 0.00016869048704393208 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.00015616882592439651 at learning rate 0.001 and epoch 84\n",
      "Best training loss: 0.00012658171181101352 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00012508216605056077 at learning rate 0.001 and epoch 89\n",
      "Best training loss: 0.00011800650099758059 at learning rate 0.001 and epoch 90\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000118\n",
      "0.001\n",
      "Best training loss: 9.91457054624334e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 8.847221033647656e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.961598021211103e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 6.887255585752428e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000069\n",
      "0.001\n",
      "Best training loss: 6.462620513048023e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 6.31781731499359e-05 at learning rate 0.001 and epoch 103\n",
      "Best training loss: 5.5153788707684726e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 5.275401417748071e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.972419992554933e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.5004093408351764e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000045\n",
      "0.001\n",
      "Best training loss: 4.316400736570358e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 4.006202289019711e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.7145659007364884e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.6461213312577456e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.54928633896634e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.292059409432113e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 3.0956238333601505e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000031\n",
      "0.001\n",
      "Best training loss: 2.987186780956108e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.9369961339398287e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.7479483833303675e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.7199104806641117e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.5997120246756822e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.4892582587199286e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.455420144542586e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.3227899873745628e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.2750740754418075e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 2.2037151211407036e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000022\n",
      "0.001\n",
      "Best training loss: 2.1078345525893383e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 2.074404983432032e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.985463677556254e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.9324848835822195e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.8840068150893785e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.8085926058120094e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.77545371116139e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.7140684576588683e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.664054798311554e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.629032522032503e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000016\n",
      "0.001\n",
      "Best training loss: 1.571374014019966e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.5385161532321945e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.4964321053412277e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.4532828572555445e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.4243662008084357e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.3820896128891036e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.3520854736270849e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.32075983856339e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.2857140973210335e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.2615771083801519e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000013\n",
      "0.001\n",
      "Best training loss: 1.2296986824367195e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.2032567610731348e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.1796093531302176e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.1514292964420747e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.1303415703878272e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.106517174775945e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 1.0839649803529028e-05 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 1.0645892871252727e-05 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 1.0425250366097316e-05 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 1.0243846190860495e-05 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 1.005408375931438e-05 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 9.865730135061312e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 9.705493539513554e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 9.526348549115937e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 9.368457540404052e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 9.217130354954861e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 9.059735930350143e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.922537745092995e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.780408279562835e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 8.6468589870492e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000009\n",
      "0.001\n",
      "Best training loss: 8.521685231244192e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 8.394481483264826e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 8.2807382568717e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 8.165128747350536e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 8.055183570832014e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.954114153108094e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.850366273487452e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.754960279271472e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.662494681426324e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.571356036351062e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 7.486637059628265e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 7.402096343867015e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 7.322470992221497e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 7.245387678267434e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 7.169415766838938e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 7.097898105712375e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 7.026653747743694e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.95864446242922e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.894650596223073e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.8306276261864696e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.769737865397474e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.710642537655076e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.652793217654107e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.597654191864422e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.544098141603172e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.492802640423179e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.442634457926033e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.3938477978808805e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.347356702463003e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8316, 0.9075, 0.5572, 0.9512, 0.8862, 0.5910, 0.7409, 0.5017, 0.7028,\n",
      "         0.6887, 0.8743, 0.6628, 0.6962, 0.8848, 0.7425, 0.8013, 0.6675, 0.6938,\n",
      "         0.4282, 0.1909, 0.9241, 0.5525, 0.6727, 0.8709, 0.5178, 0.7416, 0.8435,\n",
      "         0.4692, 0.8505, 0.6638, 0.5294, 0.4854, 0.7448, 0.4870, 0.2684, 0.9649,\n",
      "         0.5604, 0.8189, 0.6540, 0.5278, 0.1983, 0.8392, 0.1503, 0.5026, 0.3616]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.3590]]], device='cuda:0')\n",
      "Testing on best loss: 6.347356702463003e-06 from epoch 199\n",
      "MLP predicts 0.442 for case 46 with [0.09]\n",
      "Using constant kernel and channel sizes\n",
      "Creating 1 batchs of size 45 from 45 training cases\n",
      "Model contains 272145082 trainable parameters\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (fc2): Linear(in_features=11664, out_features=11664, bias=True)\n",
      "  (r): Linear(in_features=23329, out_features=1, bias=True)\n",
      ")\n",
      "Best training loss: 0.15847821533679962 at learning rate 0.001 and epoch 0\n",
      "Train Epoch: 0 [1/1 (0%)]\tLoss: 0.158478\n",
      "0.001\n",
      "Best training loss: 0.03177472949028015 at learning rate 0.001 and epoch 2\n",
      "Best training loss: 0.02346927300095558 at learning rate 0.001 and epoch 10\n",
      "Train Epoch: 10 [1/1 (0%)]\tLoss: 0.023469\n",
      "0.001\n",
      "Best training loss: 0.012428593821823597 at learning rate 0.001 and epoch 15\n",
      "Best training loss: 0.008472765795886517 at learning rate 0.001 and epoch 20\n",
      "Train Epoch: 20 [1/1 (0%)]\tLoss: 0.008473\n",
      "0.001\n",
      "Best training loss: 0.006614453624933958 at learning rate 0.001 and epoch 25\n",
      "Best training loss: 0.005416369065642357 at learning rate 0.001 and epoch 30\n",
      "Train Epoch: 30 [1/1 (0%)]\tLoss: 0.005416\n",
      "0.001\n",
      "Best training loss: 0.0038807031232863665 at learning rate 0.001 and epoch 33\n",
      "Best training loss: 0.0022701735142618418 at learning rate 0.001 and epoch 38\n",
      "Train Epoch: 40 [1/1 (0%)]\tLoss: 0.003615\n",
      "0.001\n",
      "Best training loss: 0.0015659892233088613 at learning rate 0.001 and epoch 43\n",
      "Best training loss: 0.0012486863415688276 at learning rate 0.001 and epoch 48\n",
      "Train Epoch: 50 [1/1 (0%)]\tLoss: 0.002218\n",
      "0.001\n",
      "Best training loss: 0.0010822556214407086 at learning rate 0.001 and epoch 51\n",
      "Best training loss: 0.0010436690645292401 at learning rate 0.001 and epoch 53\n",
      "Best training loss: 0.0007066549733281136 at learning rate 0.001 and epoch 56\n",
      "Train Epoch: 60 [1/1 (0%)]\tLoss: 0.001160\n",
      "0.001\n",
      "Best training loss: 0.0005183998146094382 at learning rate 0.001 and epoch 61\n",
      "Best training loss: 0.000404657592298463 at learning rate 0.001 and epoch 66\n",
      "Best training loss: 0.00034043207415379584 at learning rate 0.001 and epoch 69\n",
      "Train Epoch: 70 [1/1 (0%)]\tLoss: 0.000534\n",
      "0.001\n",
      "Best training loss: 0.00032061536330729723 at learning rate 0.001 and epoch 71\n",
      "Best training loss: 0.00024016298993956298 at learning rate 0.001 and epoch 74\n",
      "Best training loss: 0.00023800718190614134 at learning rate 0.001 and epoch 77\n",
      "Best training loss: 0.00018134951824322343 at learning rate 0.001 and epoch 79\n",
      "Train Epoch: 80 [1/1 (0%)]\tLoss: 0.000237\n",
      "0.001\n",
      "Best training loss: 0.00016506866086274385 at learning rate 0.001 and epoch 82\n",
      "Best training loss: 0.0001428667746949941 at learning rate 0.001 and epoch 84\n",
      "Best training loss: 0.00012142170453444123 at learning rate 0.001 and epoch 87\n",
      "Best training loss: 0.00011441602691775188 at learning rate 0.001 and epoch 89\n",
      "Train Epoch: 90 [1/1 (0%)]\tLoss: 0.000116\n",
      "0.001\n",
      "Best training loss: 9.35680145630613e-05 at learning rate 0.001 and epoch 92\n",
      "Best training loss: 9.193067671731114e-05 at learning rate 0.001 and epoch 94\n",
      "Best training loss: 8.60181316966191e-05 at learning rate 0.001 and epoch 95\n",
      "Best training loss: 7.42467018426396e-05 at learning rate 0.001 and epoch 97\n",
      "Best training loss: 7.360444578807801e-05 at learning rate 0.001 and epoch 99\n",
      "Best training loss: 6.592196587007493e-05 at learning rate 0.001 and epoch 100\n",
      "Train Epoch: 100 [1/1 (0%)]\tLoss: 0.000066\n",
      "0.001\n",
      "Best training loss: 5.987792610540055e-05 at learning rate 0.001 and epoch 102\n",
      "Best training loss: 5.8897861890727654e-05 at learning rate 0.001 and epoch 104\n",
      "Best training loss: 5.2074385166633874e-05 at learning rate 0.001 and epoch 105\n",
      "Best training loss: 4.875795275438577e-05 at learning rate 0.001 and epoch 107\n",
      "Best training loss: 4.7482513764407486e-05 at learning rate 0.001 and epoch 108\n",
      "Best training loss: 4.715176692116074e-05 at learning rate 0.001 and epoch 109\n",
      "Best training loss: 4.2036881495732814e-05 at learning rate 0.001 and epoch 110\n",
      "Train Epoch: 110 [1/1 (0%)]\tLoss: 0.000042\n",
      "0.001\n",
      "Best training loss: 3.9898914110381156e-05 at learning rate 0.001 and epoch 112\n",
      "Best training loss: 3.785845547099598e-05 at learning rate 0.001 and epoch 113\n",
      "Best training loss: 3.4480850445106626e-05 at learning rate 0.001 and epoch 115\n",
      "Best training loss: 3.446206756052561e-05 at learning rate 0.001 and epoch 116\n",
      "Best training loss: 3.285888669779524e-05 at learning rate 0.001 and epoch 117\n",
      "Best training loss: 3.0848208552924916e-05 at learning rate 0.001 and epoch 118\n",
      "Best training loss: 2.8646940336329862e-05 at learning rate 0.001 and epoch 120\n",
      "Train Epoch: 120 [1/1 (0%)]\tLoss: 0.000029\n",
      "0.001\n",
      "Best training loss: 2.8088115868740715e-05 at learning rate 0.001 and epoch 121\n",
      "Best training loss: 2.724308433244005e-05 at learning rate 0.001 and epoch 122\n",
      "Best training loss: 2.5594783437554725e-05 at learning rate 0.001 and epoch 123\n",
      "Best training loss: 2.5458844902459532e-05 at learning rate 0.001 and epoch 124\n",
      "Best training loss: 2.4050765205174685e-05 at learning rate 0.001 and epoch 125\n",
      "Best training loss: 2.329747258045245e-05 at learning rate 0.001 and epoch 126\n",
      "Best training loss: 2.2820029698777944e-05 at learning rate 0.001 and epoch 127\n",
      "Best training loss: 2.15676445805002e-05 at learning rate 0.001 and epoch 128\n",
      "Best training loss: 2.1267162082949653e-05 at learning rate 0.001 and epoch 129\n",
      "Best training loss: 2.0404795577633195e-05 at learning rate 0.001 and epoch 130\n",
      "Train Epoch: 130 [1/1 (0%)]\tLoss: 0.000020\n",
      "0.001\n",
      "Best training loss: 1.9661800251924433e-05 at learning rate 0.001 and epoch 131\n",
      "Best training loss: 1.930592770804651e-05 at learning rate 0.001 and epoch 132\n",
      "Best training loss: 1.841586890805047e-05 at learning rate 0.001 and epoch 133\n",
      "Best training loss: 1.804587009246461e-05 at learning rate 0.001 and epoch 134\n",
      "Best training loss: 1.7478734662290663e-05 at learning rate 0.001 and epoch 135\n",
      "Best training loss: 1.6838996089063585e-05 at learning rate 0.001 and epoch 136\n",
      "Best training loss: 1.654432708164677e-05 at learning rate 0.001 and epoch 137\n",
      "Best training loss: 1.5910094589344226e-05 at learning rate 0.001 and epoch 138\n",
      "Best training loss: 1.5527399227721617e-05 at learning rate 0.001 and epoch 139\n",
      "Best training loss: 1.5145124962145928e-05 at learning rate 0.001 and epoch 140\n",
      "Train Epoch: 140 [1/1 (0%)]\tLoss: 0.000015\n",
      "0.001\n",
      "Best training loss: 1.4627198652306106e-05 at learning rate 0.001 and epoch 141\n",
      "Best training loss: 1.4352069229062181e-05 at learning rate 0.001 and epoch 142\n",
      "Best training loss: 1.391212845192058e-05 at learning rate 0.001 and epoch 143\n",
      "Best training loss: 1.3562126696342602e-05 at learning rate 0.001 and epoch 144\n",
      "Best training loss: 1.3271685020299628e-05 at learning rate 0.001 and epoch 145\n",
      "Best training loss: 1.2875761058239732e-05 at learning rate 0.001 and epoch 146\n",
      "Best training loss: 1.262998648599023e-05 at learning rate 0.001 and epoch 147\n",
      "Best training loss: 1.2308579243835993e-05 at learning rate 0.001 and epoch 148\n",
      "Best training loss: 1.2009600141027477e-05 at learning rate 0.001 and epoch 149\n",
      "Best training loss: 1.1785316019086167e-05 at learning rate 0.001 and epoch 150\n",
      "Train Epoch: 150 [1/1 (0%)]\tLoss: 0.000012\n",
      "0.001\n",
      "Best training loss: 1.1480275134090334e-05 at learning rate 0.001 and epoch 151\n",
      "Best training loss: 1.125896869780263e-05 at learning rate 0.001 and epoch 152\n",
      "Best training loss: 1.102346050174674e-05 at learning rate 0.001 and epoch 153\n",
      "Best training loss: 1.0774349902931135e-05 at learning rate 0.001 and epoch 154\n",
      "Best training loss: 1.0583962648524903e-05 at learning rate 0.001 and epoch 155\n",
      "Best training loss: 1.0353089237469248e-05 at learning rate 0.001 and epoch 156\n",
      "Best training loss: 1.0161063073610421e-05 at learning rate 0.001 and epoch 157\n",
      "Best training loss: 9.97446204564767e-06 at learning rate 0.001 and epoch 158\n",
      "Best training loss: 9.774003046914004e-06 at learning rate 0.001 and epoch 159\n",
      "Best training loss: 9.615072485757992e-06 at learning rate 0.001 and epoch 160\n",
      "Train Epoch: 160 [1/1 (0%)]\tLoss: 0.000010\n",
      "0.001\n",
      "Best training loss: 9.43314444157295e-06 at learning rate 0.001 and epoch 161\n",
      "Best training loss: 9.269421752833296e-06 at learning rate 0.001 and epoch 162\n",
      "Best training loss: 9.121524271904491e-06 at learning rate 0.001 and epoch 163\n",
      "Best training loss: 8.958052603702527e-06 at learning rate 0.001 and epoch 164\n",
      "Best training loss: 8.82109634403605e-06 at learning rate 0.001 and epoch 165\n",
      "Best training loss: 8.67950075189583e-06 at learning rate 0.001 and epoch 166\n",
      "Best training loss: 8.541743227397092e-06 at learning rate 0.001 and epoch 167\n",
      "Best training loss: 8.418344805249944e-06 at learning rate 0.001 and epoch 168\n",
      "Best training loss: 8.288054232252762e-06 at learning rate 0.001 and epoch 169\n",
      "Best training loss: 8.17271302366862e-06 at learning rate 0.001 and epoch 170\n",
      "Train Epoch: 170 [1/1 (0%)]\tLoss: 0.000008\n",
      "0.001\n",
      "Best training loss: 8.057294508034829e-06 at learning rate 0.001 and epoch 171\n",
      "Best training loss: 7.94429888628656e-06 at learning rate 0.001 and epoch 172\n",
      "Best training loss: 7.843677849450614e-06 at learning rate 0.001 and epoch 173\n",
      "Best training loss: 7.737643500149716e-06 at learning rate 0.001 and epoch 174\n",
      "Best training loss: 7.640392141183838e-06 at learning rate 0.001 and epoch 175\n",
      "Best training loss: 7.5472903517948e-06 at learning rate 0.001 and epoch 176\n",
      "Best training loss: 7.452989848388825e-06 at learning rate 0.001 and epoch 177\n",
      "Best training loss: 7.367139915004373e-06 at learning rate 0.001 and epoch 178\n",
      "Best training loss: 7.280284535227111e-06 at learning rate 0.001 and epoch 179\n",
      "Best training loss: 7.196906608442077e-06 at learning rate 0.001 and epoch 180\n",
      "Train Epoch: 180 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 7.117609584383899e-06 at learning rate 0.001 and epoch 181\n",
      "Best training loss: 7.038566764094867e-06 at learning rate 0.001 and epoch 182\n",
      "Best training loss: 6.964872682146961e-06 at learning rate 0.001 and epoch 183\n",
      "Best training loss: 6.891448265378131e-06 at learning rate 0.001 and epoch 184\n",
      "Best training loss: 6.821336683060508e-06 at learning rate 0.001 and epoch 185\n",
      "Best training loss: 6.756840321031632e-06 at learning rate 0.001 and epoch 186\n",
      "Best training loss: 6.692283022857737e-06 at learning rate 0.001 and epoch 187\n",
      "Best training loss: 6.631993528571911e-06 at learning rate 0.001 and epoch 188\n",
      "Best training loss: 6.573687642230652e-06 at learning rate 0.001 and epoch 189\n",
      "Best training loss: 6.5155772972502746e-06 at learning rate 0.001 and epoch 190\n",
      "Train Epoch: 190 [1/1 (0%)]\tLoss: 0.000007\n",
      "0.001\n",
      "Best training loss: 6.460574695665855e-06 at learning rate 0.001 and epoch 191\n",
      "Best training loss: 6.407044111256255e-06 at learning rate 0.001 and epoch 192\n",
      "Best training loss: 6.356023277476197e-06 at learning rate 0.001 and epoch 193\n",
      "Best training loss: 6.306401701294817e-06 at learning rate 0.001 and epoch 194\n",
      "Best training loss: 6.257794666453265e-06 at learning rate 0.001 and epoch 195\n",
      "Best training loss: 6.211516847542953e-06 at learning rate 0.001 and epoch 196\n",
      "Best training loss: 6.165246759337606e-06 at learning rate 0.001 and epoch 197\n",
      "Best training loss: 6.120667421782855e-06 at learning rate 0.001 and epoch 198\n",
      "Best training loss: 6.077782927604858e-06 at learning rate 0.001 and epoch 199\n",
      "Predicted tensor([[0.8314, 0.9072, 0.5572, 0.9511, 0.8871, 0.5910, 0.7414, 0.5016, 0.7024,\n",
      "         0.6889, 0.8742, 0.6627, 0.6963, 0.8849, 0.7431, 0.8011, 0.6675, 0.6935,\n",
      "         0.4285, 0.1906, 0.9252, 0.5521, 0.6726, 0.8711, 0.5177, 0.7414, 0.8440,\n",
      "         0.4693, 0.8504, 0.6638, 0.5291, 0.4849, 0.7444, 0.4865, 0.2682, 0.9646,\n",
      "         0.5604, 0.8187, 0.6542, 0.5275, 0.1983, 0.8390, 0.1505, 0.5029, 0.0895]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>) for tensor([[[0.8308]],\n",
      "\n",
      "        [[0.9091]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.9524]],\n",
      "\n",
      "        [[0.8889]],\n",
      "\n",
      "        [[0.5926]],\n",
      "\n",
      "        [[0.7368]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.7037]],\n",
      "\n",
      "        [[0.6866]],\n",
      "\n",
      "        [[0.8788]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.8831]],\n",
      "\n",
      "        [[0.7442]],\n",
      "\n",
      "        [[0.8036]],\n",
      "\n",
      "        [[0.6667]],\n",
      "\n",
      "        [[0.6914]],\n",
      "\n",
      "        [[0.4286]],\n",
      "\n",
      "        [[0.1875]],\n",
      "\n",
      "        [[0.9394]],\n",
      "\n",
      "        [[0.5556]],\n",
      "\n",
      "        [[0.6724]],\n",
      "\n",
      "        [[0.8730]],\n",
      "\n",
      "        [[0.5172]],\n",
      "\n",
      "        [[0.7436]],\n",
      "\n",
      "        [[0.8462]],\n",
      "\n",
      "        [[0.4677]],\n",
      "\n",
      "        [[0.8529]],\n",
      "\n",
      "        [[0.6615]],\n",
      "\n",
      "        [[0.5312]],\n",
      "\n",
      "        [[0.4833]],\n",
      "\n",
      "        [[0.7455]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.2609]],\n",
      "\n",
      "        [[0.9677]],\n",
      "\n",
      "        [[0.5593]],\n",
      "\n",
      "        [[0.8250]],\n",
      "\n",
      "        [[0.6552]],\n",
      "\n",
      "        [[0.5227]],\n",
      "\n",
      "        [[0.1944]],\n",
      "\n",
      "        [[0.8393]],\n",
      "\n",
      "        [[0.1471]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.0857]]], device='cuda:0')\n",
      "Testing on best loss: 6.077782927604858e-06 from epoch 199\n",
      "MLP predicts 0.609 for case 52 with [0.36]\n"
     ]
    }
   ],
   "source": [
    "for j in np.arange(len(subsc)):\n",
    "    test_id = subsc[j]\n",
    "    test_index = subsc == test_id\n",
    "    train_index = subsc != test_id\n",
    "    X_test = X_img[test_index,:,:]\n",
    "    X_train = X_img[train_index,:,:]\n",
    "    y_train = per_change[train_index]\n",
    "    y_test = per_change[test_index]\n",
    "    # Cross validation\n",
    "    cvn = 4\n",
    "    # ADD PRE-UPDRS as a residual connection to the last linear layer\n",
    "    # MLP\n",
    "    encoder = Net(in_size=X_train.shape[-1],n_channels=4)\n",
    "    yt, encoder, _ = train_model(X_all=X_train,\n",
    "                     y_all=y_train,\n",
    "                     u_all = pre_updrs_off[train_index],\n",
    "                     model=encoder,\n",
    "                     X_test=X_test,\n",
    "                     u_test = pre_updrs_off[test_index],\n",
    "                     warm_start_weights=[],\n",
    "                     early_stopping=[],\n",
    "                     tol=1e-6,\n",
    "                     lr=1e-3,\n",
    "                     lr_decay=None,\n",
    "                     alpha=1e-2,\n",
    "                     reg_type='l2',\n",
    "                     thresh=[],\n",
    "                     num_epochs=int(200),\n",
    "                     batch_size=len(X_img)-1,\n",
    "                     case_id=str(int(subsc[j])),\n",
    "                     num_neighbors=0,\n",
    "                     random_val=True,\n",
    "                     verbose=2,\n",
    "                     save_state=False)\n",
    "\n",
    "    results_bls[j] = yt.detach().cpu().numpy()\n",
    "    print('MLP predicts',str(np.round(yt.item(),3)),\n",
    "            'for case',str(int(subsc[j])),'with',str(np.round(per_change[j],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 540x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_bls[results_bls>1] = 1\n",
    "util.eval_prediction(np.vstack((pre_imp.T,\n",
    "                               results_bls.T,\n",
    "                               )),\n",
    "                               per_change.T,\n",
    "                               ['LCT',\n",
    "                                'MLP 4 channels \\n 200 iterations \\n $\\\\alpha_{L_2} = 10^{-2}$ \\n $\\gamma=10^{-3}$',\n",
    "                                ],(15,10))\n",
    "plt.ylim([0,1.75])\n",
    "plt.xlim([0,1.75])\n",
    "plt.style.use('default')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
