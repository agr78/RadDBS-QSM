{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary data notebook for\n",
    "# NIH: Imaging Guided Intervention Surgery Study Section\n",
    "\n",
    "# Exploratory aim: evaluate presurgical scans between STN and GPi targets\n",
    "#   Given retrospective GPi acquisitions?\n",
    "#   Search for radiomic differentiators for STN versus GPi selection in presurgical scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import nibabel as nib\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score\n",
    "import SimpleITK as sitk\n",
    "import six\n",
    "from radiomics import featureextractor \n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "import smogn\n",
    "import pandas\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "plt.rcParams[\"figure.figsize\"] = (45,5)\n",
    "def remove_keymap_conflicts(new_keys_set):\n",
    "    for prop in plt.rcParams:\n",
    "        if prop.startswith('keymap.'):\n",
    "            keys = plt.rcParams[prop]\n",
    "            remove_list = set(keys) & new_keys_set\n",
    "            for key in remove_list:\n",
    "                keys.remove(key)\n",
    "\n",
    "def multi_slice_viewer(volume):\n",
    "    remove_keymap_conflicts({'j', 'k'})\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.volume = volume\n",
    "    ax.index = volume.shape[0]//2\n",
    "    ax.imshow(volume[ax.index])\n",
    "    fig.canvas.mpl_connect('key_press_event', process_key)\n",
    "\n",
    "def process_key(event):\n",
    "    fig = event.canvas.figure\n",
    "    ax = fig.axes[0]\n",
    "    if event.key == 'j':\n",
    "        previous_slice(ax)\n",
    "    elif event.key == 'k':\n",
    "        next_slice(ax)\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "\n",
    "def previous_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index-1) % volume.shape[0] \n",
    "    ax.images[0].set_array(volume[ax.index])\n",
    "\n",
    "def next_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index+1) % volume.shape[0]\n",
    "    ax.images[0].set_array(volume[ax.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set window level\n",
    "level = 0\n",
    "window = 500\n",
    "m1=level-window/2\n",
    "m2=level+window/2\n",
    "visualize = 1\n",
    "reload = 0\n",
    "# Load data\n",
    "nrows = 256\n",
    "ncols = 256\n",
    "nslices = 160\n",
    "segs = []\n",
    "qsms = []\n",
    "laros = []\n",
    "voxel_sizes = []\n",
    "trackers = []\n",
    "q_directory = '/media/mts_dbs/dbs/complete_cases/nii/qsm/'\n",
    "s_directory = '/media/mts_dbs/dbs/complete_cases/nii/seg/'\n",
    "s_directory = os.listdir(s_directory)\n",
    "s_directory = sorted(s_directory)\n",
    "\n",
    "case_list = []\n",
    "d_count = 0\n",
    "if reload == 1:\n",
    "    for seg_filename in s_directory:\n",
    "        id = seg_filename[12:14]\n",
    "        seg = nib.load('/media/mts_dbs/dbs/complete_cases/nii/seg/'+seg_filename)\n",
    "        voxel_size = seg.header['pixdim'][0:3]\n",
    "        voxel_sizes.append(voxel_size)\n",
    "        segs.append(seg.get_fdata())\n",
    "        qsm = nib.load('/media/mts_dbs/dbs/complete_cases/nii/qsm/qsm_'+str(id)+'.nii.gz')\n",
    "        qsms.append(qsm.get_fdata())\n",
    "        print('Appending arrays with segmentation',seg_filename,'and QSM','qsm_'+str(id)+'.nii.gz')\n",
    "        case_list.append('qsm_'+str(id)+'.nii.gz')\n",
    "        n_cases = len(segs)\n",
    "        d_count = d_count+1\n",
    "        qsms_wl = np.asarray(qsms)\n",
    "        segs_wl = np.asarray(segs)\n",
    "        with open('./pickles/segs', 'wb') as fp:  \n",
    "            pickle.dump(segs, fp)\n",
    "\n",
    "        with open('./pickles/qsms', 'wb') as fp:  \n",
    "            pickle.dump(qsms, fp)\n",
    "\n",
    "else:\n",
    "    with open('./pickles/segs', \"rb\") as fp:  \n",
    "        segs = pickle.load(fp)\n",
    "\n",
    "    with open('./pickles/qsms', \"rb\") as fp:  \n",
    "        qsms = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize == 0:\n",
    "    qsms_wl = np.asarray(qsms)\n",
    "    segs_wl = np.asarray(segs)\n",
    "    qsms_wl[qsms_wl < m1] = m1\n",
    "    qsms_wl[qsms_wl > m2] = m2\n",
    "    n_cases = len(segs)\n",
    "    multi_slice_viewer(np.hstack(((np.vstack(qsms_wl[:n_cases//2,:,:,:]/1000+0*segs_wl[:n_cases//2,:,:,:]).T),\n",
    "                                  (np.vstack(qsms_wl[(n_cases-n_cases//2):,:,:,:]/1000+0*segs_wl[(n_cases-n_cases//2):,:,:,:]).T))))\n",
    "   \n",
    "                                    \n",
    "    label_min = np.partition(np.unique(seg.get_fdata().ravel()), 1)[1]\n",
    "    label_max = np.amax(seg.get_fdata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '/data/Ali/RadDBS-QSM/src/csv'\n",
    "# Load patient data\n",
    "os.chdir(file_dir)\n",
    "df = pd.read_csv('QSM anonymus- 6.22.2023-1528.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy\n",
    "dfd = df.copy()\n",
    "# Drop blank columns\n",
    "for (columnName, columnData) in dfd.iteritems():\n",
    "    if columnData.isnull().all():\n",
    "        print('Dropping NaN column at',columnName)\n",
    "        dfd.drop(columnName,axis=1,inplace=True)\n",
    "# Add relevant column names from headers\n",
    "for (columnName, columnData) in dfd.iteritems():\n",
    "        dfd.rename(columns={columnName:columnName+': '+columnData.values[0]},inplace=True)\n",
    "\n",
    "def drop_prefix(self, prefix):\n",
    "    self.columns = self.columns.str.lstrip(prefix)\n",
    "    return self\n",
    "\n",
    "pd.core.frame.DataFrame.drop_prefix = drop_prefix\n",
    "\n",
    "dfd.drop_prefix('Unnamed:')        \n",
    "for (columnName, columnData) in dfd.iteritems():\n",
    "    if columnName[1].isdigit():\n",
    "        dfd.rename(columns={columnName:columnName[4:]},inplace=True)\n",
    "\n",
    "# Make a copy for motor symptoms\n",
    "motor_df = dfd.copy()\n",
    "# Drop non-motor (III) columns\n",
    "for (columnName, columnData) in motor_df.iteritems():\n",
    "    if 'pre-dbs updrs' in columnName:\n",
    "        next\n",
    "    elif 'stim' in columnName:\n",
    "        next\n",
    "    elif 'CORNELL ID' in columnName:\n",
    "        next\n",
    "    else:\n",
    "        motor_df.drop(columnName,axis=1,inplace=True)\n",
    "\n",
    "# Drop subheader\n",
    "motor_df = motor_df.tail(-1)\n",
    "motor_df = motor_df.replace('na',np.nan)\n",
    "motor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = []\n",
    "for seg_filename in s_directory:\n",
    "    id.append(seg_filename[12:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_dbs_off_meds_on_stim = motor_df['OFF meds ON stim 6mo'].to_numpy().astype('float')\n",
    "df_post_dbs_off_meds_off_stim = motor_df[' off stim off med 6mo'].to_numpy().astype('float')\n",
    "df_pre_dbs_off_meds = motor_df['OFF (pre-dbs updrs)'].to_numpy().astype('float')\n",
    "df_pre_dbs_on_meds = motor_df['ON (pre-dbs updrs)'].to_numpy().astype('float')\n",
    "\n",
    "cases = ~np.isnan(df_pre_dbs_off_meds+df_pre_dbs_on_meds+df_post_dbs_off_meds_on_stim)\n",
    "pre_dbs_meds_improvement = (df_pre_dbs_off_meds[cases]-df_pre_dbs_on_meds[cases])/df_pre_dbs_off_meds[cases]\n",
    "dbs_off_meds_improvement = (df_pre_dbs_off_meds[cases]-df_post_dbs_off_meds_on_stim[cases])/df_pre_dbs_off_meds[cases]\n",
    "motor_df['CORNELL ID'].replace('only Ct data ', np.nan, inplace=True)\n",
    "pids = motor_df['CORNELL ID'].to_numpy().astype('float')\n",
    "subs = pids[cases]\n",
    "subs_in = (np.intersect1d(subs,np.asarray(id).astype(float)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_dbs_off_meds_in = df_pre_dbs_off_meds[cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_dbs_off_meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_dbs_off_meds_in = df_pre_dbs_off_meds_in[np.in1d(subs,np.asarray(id).astype(float))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_dbs_off_meds_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_dbs_off_meds_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and fit like Zaidel et. al Figure 3C\n",
    "plt.rcParams[\"figure.figsize\"] = (7,7)\n",
    "lr_rho_med = linregress(dbs_off_meds_improvement,pre_dbs_meds_improvement)\n",
    "plt.scatter(dbs_off_meds_improvement,pre_dbs_meds_improvement)\n",
    "plt.plot(dbs_off_meds_improvement,dbs_off_meds_improvement*lr_rho_med.slope+lr_rho_med.intercept,'-r')\n",
    "text = f\"$y={lr_rho_med.slope:0.3f}\\;x{lr_rho_med.intercept:+0.3f}$\\n$r = {lr_rho_med.rvalue:0.3f}$\\n$p = {lr_rho_med.pvalue:0.3f}$\"\n",
    "plt.gca().text(0.05, 0.95, text,transform=plt.gca().transAxes,\n",
    "     fontsize=14, verticalalignment='top')\n",
    "plt.xlabel(r'$\\rho_{stim}$')\n",
    "plt.ylabel(r'$\\rho_{med}$')\n",
    "plt.ylim([0,1.25])\n",
    "plt.xlim([0,1.25])\n",
    "plt.title('UPDRS-III improvement')\n",
    "plt.style.use('dark_background')\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"radiomics\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "reextract = 1\n",
    "# Assume all voxel sizes are identical\n",
    "voxel_size = (0.5,0.5,0.5)\n",
    "fv_count = 0\n",
    "if reextract == 1:\n",
    "    # Generate feature structure Phi from all ROIs and all cases\n",
    "    extractor = featureextractor.RadiomicsFeatureExtractor()\n",
    "    extractor.enableAllFeatures()\n",
    "    extractor.enableAllImageTypes()\n",
    "    extractor.enableFeatureClassByName('shape2D',enabled = False)\n",
    "\n",
    "    seg_labels_all = [0,1,2,3,4,5,6,7]\n",
    "    Phi_gt = []\n",
    "    seg_labels = []\n",
    "    reextract = 0\n",
    "    x_row_gt = []\n",
    "\n",
    "    keylib = []\n",
    "    roilib = []\n",
    "    loop_count = 1\n",
    "    n_rois = 6\n",
    "    roi_names = []\n",
    "    roi_txt = pd.read_csv(\"/data/Ali/atlas/mcgill_pd_atlas/PD25-subcortical-labels.csv\")\n",
    "    roi_df = roi_txt.astype(str)\n",
    "    for i in np.arange(subs_in.__len__()):\n",
    "        seg_sitk = sitk.GetImageFromArray(segs[i])\n",
    "        seg_sitk.SetSpacing(voxel_size)\n",
    "        qsm_sitk_gt = sitk.GetImageFromArray(qsms[i])\n",
    "        qsm_sitk_gt.SetSpacing(voxel_size)\n",
    "        for j in seg_labels_all:\n",
    "            if 0 < j < 7:\n",
    "                fv_count = 0\n",
    "                featureVector_gt = extractor.execute(qsm_sitk_gt,seg_sitk,label=int(j));\n",
    "                Phi_gt.append(featureVector_gt)\n",
    "                for key, value in six.iteritems(featureVector_gt):\n",
    "                    if 'diagnostic' in key:\n",
    "                        next\n",
    "                    else:\n",
    "                        x_row_gt.append(featureVector_gt[key])\n",
    "                        fv_count = fv_count+1\n",
    "                        keylib.append(key)\n",
    "                        roilib.append(j)\n",
    "                        mask = np.row_stack([roi_df[row].str.contains(str(int(roilib[-1])), na = False) for row in roi_df])\n",
    "                        roi_names.append(np.asarray(roi_df.iloc[mask.any(axis=0),1])[0])\n",
    "                x_row_gt.append(pre_dbs_off_meds_in[i])\n",
    "                fv_count = fv_count+1\n",
    "                print('Extracting features for subject',subs_in[i],'ROI',j,'and appending feature matrix with vector of length',fv_count,'with UPDRS score',df_pre_dbs_off_meds[i])\n",
    "                \n",
    "    X0_gt = np.array(x_row_gt)\n",
    "    np.save('./npy/X0_gt_msw_rois.npy',X0_gt)\n",
    "    K = np.asarray(keylib)\n",
    "    R = np.asarray(roi_names)\n",
    "    np.save('./npy/K_msw.npy',K)\n",
    "    print('Saving ground truth feature vector')\n",
    "    with open('./phi/Phi_mcl_gt_roi_msw', 'wb') as fp:  \n",
    "        pickle.dump(Phi_gt, fp)\n",
    "\n",
    "else:\n",
    "    X0_gt = np.load('./npy/X0_gt_msw_rois.npy')\n",
    "    K = np.load('./npy/K_msw.npy')\n",
    "    R = np.load('./npy/R_msw.npy')\n",
    "    n_rois = R.shape[0]-1\n",
    "    with open('./phi/Phi_mcl_gt_roi_msw', \"rb\") as fp:  \n",
    "        Phi_gt = pickle.load(fp)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(pids[cases.__len__()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of features and check ROIs\n",
    "n_cases = len(cases)\n",
    "n_roisc = Phi_gt.__len__()/n_cases\n",
    "L = int(len(X0_gt)/n_cases)\n",
    "n_features = int(L/n_rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate arrays\n",
    "x_row_gt = X0_gt.tolist()\n",
    "X = np.zeros((n_cases,n_rois,n_features)).transpose((0,2,1))\n",
    "X = X0_gt.reshape((n_cases,n_rois,n_features)).transpose((0,2,1))\n",
    "ut_ls = np.zeros((cases.__len__()))\n",
    "ut_qr = np.zeros((cases.__len__()))\n",
    "\n",
    "# Normalize testing and training cases together\n",
    "#   Set with_mean=False to preserve data sparsity\n",
    "#   And with_std=False \n",
    "#   However, need a significant number of samples to do this\n",
    "X_all = X.reshape(n_cases,((n_features)*n_rois))\n",
    "# Add UPDRS\n",
    "X_all_t = np.concatenate((X_all,df_pre_dbs_off_meds.reshape(df_pre_dbs_off_meds.__len__(),1)),axis=1)\n",
    "scaler = StandardScaler()\n",
    "# Transform feature matrix and UPDRS\n",
    "X_all_t = scaler.fit_transform(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame for SMOGN generation\n",
    "D = pd.DataFrame(np.hstack((X_all_t,(dbs_off_meds_improvement.reshape(dbs_off_meds_improvement.__len__(),1)))))\n",
    "for col in D.columns:\n",
    "    D.rename(columns={col:str(col)},inplace=True)\n",
    "\n",
    "# Specify phi relevance values\n",
    "Rm = [[np.min(dbs_off_meds_improvement),  1, 0],  ## over-sample (\"minority\")\n",
    "    [np.median(dbs_off_meds_improvement), 0, 0],  ## under-sample (\"majority\")\n",
    "    ]\n",
    "\n",
    "# Conduct SMOGN\n",
    "print('Prior to SMOGN sampling, mean is',X_all_t.mean(),'standard deviation is',X_all_t.std())\n",
    "X_smogn = smogn.smoter(data = D, y = str(D.columns[-1]),rel_method = 'manual',rel_ctrl_pts_rg = Rm)\n",
    "\n",
    "# Drop label\n",
    "X_in_s = np.array(X_smogn)[:,:-1] \n",
    "print('After SMOGN sampling, mean is',X_in_s.mean(),'standard deviation is',X_in_s.std())\n",
    "X_in_s = scaler.fit_transform(X_in_s)\n",
    "print('Standardizing the SMOGN dataset gives, mean',X_in_s.mean(),'standard deviation',X_in_s.std())\n",
    "\n",
    "for j in np.arange(X_in_s.shape[1]):\n",
    "    if np.array_equal(X_in_s[:,j],np.array(X_smogn)[:,-1]) == 0:\n",
    "        next\n",
    "    else:\n",
    "        print('Labels detected at column',j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.zeros_like(X_in_s)\n",
    "C = np.zeros_like(X_all_t)\n",
    "for j in np.arange(len(cases)):\n",
    "        # Add UPDRS after scaling\n",
    "        # Initialize training feature matrix\n",
    "        X_in = X_all_t\n",
    "        X_in = np.delete(X_in,j,axis=0)\n",
    "        if j < np.array(X_smogn)[:,:-1].shape[0]: \n",
    "                # Drop the label in SMOGN array\n",
    "                X_in_s = np.array(X_smogn)[:,:-1] \n",
    "                # Drop the test case features\n",
    "                X_in_s = np.delete(X_in_s,j,axis=0)\n",
    "                # Create training label array from the SMOGN array\n",
    "                smogn_per_change_in = np.asarray(X_smogn)[:,-1]\n",
    "                # Drop the test case labels\n",
    "                smogn_per_change_in = np.delete(smogn_per_change_in,j,axis=0)\n",
    "                # Train LASSO on SMOGN\n",
    "                clf_s = Lasso(alpha=1e-4,max_iter=10000).fit(X_in_s,smogn_per_change_in)\n",
    "                # Get the features LASSO-SMOGN uses\n",
    "                Cs[j] = clf_s.coef_\n",
    "\n",
    "        # Initialize training labels\n",
    "        per_change_in = dbs_off_meds_improvement\n",
    "        per_change_in = np.delete(per_change_in,j,axis=0)\n",
    "  \n",
    "        # Cross-validation for model selection\n",
    "        # Identify most important features\n",
    "        clf_ls = Lasso(alpha=1e-4,max_iter=10000).fit(X_in,per_change_in)\n",
    "        print('Fit complete')\n",
    "        ut_ls[j] = clf_ls.predict(X_all_t[j,:].reshape(1, -1))\n",
    "        ut_qr[j] = clf_s.predict(X_all_t[j,:].reshape(1, -1))\n",
    "        C[j] = clf_ls.coef_\n",
    "\n",
    "        print('Patient ID',str(dbs_off_meds_improvement[j]),'with pre-surgical UPDRS score',str(pre_dbs_meds_improvement[int(j)]),'at feature matrix row',str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (25,5)\n",
    "# Cross validation results\n",
    "[fig,ax] = plt.subplots(1,3,sharex=True, sharey=True)\n",
    "lr_prepost = linregress(pre_dbs_meds_improvement,dbs_off_meds_improvement)\n",
    "ax[0].scatter(pre_dbs_meds_improvement,dbs_off_meds_improvement,)\n",
    "ax[0].plot(pre_dbs_meds_improvement,pre_dbs_meds_improvement*lr_prepost.slope+lr_prepost.intercept,'-r')\n",
    "ax[0].set_title('LCT')\n",
    "ax[0].set_ylabel(\"DBS improvement\")\n",
    "ax[0].set_xlabel(\"Prediction\")\n",
    "ax[0].set_ylim([0, 2])\n",
    "text = f\"$y={lr_prepost.slope:0.3f}\\;x{lr_prepost.intercept:+0.3f}$\\n$r = {lr_prepost.rvalue:0.3f}$\\n$p = {lr_prepost.pvalue:0.3f}$\"\n",
    "ax[0].text(0.05, 0.95, text,transform=ax[0].transAxes,\n",
    "     fontsize=14, verticalalignment='top')\n",
    "ax[0].hlines(0.4,0,1,linestyle='dashed',color='white')\n",
    "ax[0].vlines(0.4,0,2,linestyle='dashed',color='white')\n",
    "\n",
    "lr_pred_ls = linregress(ut_ls,dbs_off_meds_improvement)\n",
    "ax[1].scatter(ut_ls,dbs_off_meds_improvement)\n",
    "ax[1].plot(ut_ls,ut_ls*lr_pred_ls.slope+lr_pred_ls.intercept,'-r')\n",
    "ax[1].set_title('Fully-sampled LASSO')\n",
    "ax[1].set_ylabel(\"DBS improvement\")\n",
    "ax[1].set_xlabel(\"Prediction\")\n",
    "text = f\"$y={lr_pred_ls.slope:0.3f}\\;x{lr_pred_ls.intercept:+0.3f}$\\n$r = {lr_pred_ls.rvalue:0.3f}$\\n$p = {lr_pred_ls.pvalue:0.3f}$\"\n",
    "ax[1].text(0.05, 0.95, text,transform=ax[1].transAxes,\n",
    "     fontsize=14, verticalalignment='top')\n",
    "ax[1].hlines(0.4,0,1,linestyle='dashed',color='white')\n",
    "ax[1].vlines(0.4,0,2,linestyle='dashed',color='white')\n",
    "\n",
    "\n",
    "lr_pred_qr = linregress(ut_qr,dbs_off_meds_improvement)\n",
    "ax[2].scatter(ut_qr,dbs_off_meds_improvement)\n",
    "ax[2].plot(ut_qr,ut_qr*lr_pred_qr.slope+lr_pred_qr.intercept,'-r')\n",
    "ax[2].set_title('Fully-sampled LASSO with SMOGN')\n",
    "ax[2].set_ylabel(\"DBS improvement\")\n",
    "ax[2].set_xlabel(\"Prediction\")\n",
    "text = f\"$y={lr_pred_qr.slope:0.3f}\\;x{lr_pred_qr.intercept:+0.3f}$\\n$r = {lr_pred_qr.rvalue:0.3f}$\\n$p = {lr_pred_qr.pvalue:0.10f}$\"\n",
    "ax[2].text(0.05, 0.95, text,transform=ax[2].transAxes,\n",
    "     fontsize=14, verticalalignment='top')\n",
    "ax[2].hlines(0.4,0,1,linestyle='dashed',color='white')\n",
    "ax[2].vlines(0.4,0,2,linestyle='dashed',color='white')\n",
    "\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfs = []\n",
    "rfss = []\n",
    "for j in np.arange(cases.__len__()):\n",
    "    Kr =  K.reshape((n_cases,n_rois,(n_features-1))).transpose((0,2,1))\n",
    "    Kr_extended = np.zeros((n_cases,n_rois,n_features)).transpose((0,2,1)).astype('str')\n",
    "    Kr_extended[:,0:n_features-1,:] = Kr\n",
    "    Kr_extended[:,-1,:] = 'po_updrs'\n",
    "    rfs.append(Kr_extended[j,np.asarray(C[j]!=0).reshape((n_rois,n_features)).transpose((1,0))])\n",
    "    if j < Cs.shape[0]:\n",
    "        rfss.append(Kr_extended[j,np.asarray(Cs[j]!=0).reshape((n_rois,n_features)).transpose((1,0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (30,15)\n",
    "fig, axes = plt.subplots(3,1,sharey=True)\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.8)\n",
    "\n",
    "R = [item for sublist in rfs for item in sublist]\n",
    "letter_counts = Counter(R)\n",
    "df = pandas.DataFrame.from_dict(letter_counts, orient='index')\n",
    "df.sort_values(0, inplace=True)\n",
    "df.plot(ax=axes[0],y=0, kind='bar', legend=False)\n",
    "\n",
    "Rs = [item for sublist in rfss for item in sublist]\n",
    "letter_countss = Counter(Rs)\n",
    "dfs = pandas.DataFrame.from_dict(letter_counts, orient='index')\n",
    "dfs.sort_values(0, inplace=True)\n",
    "dfs.plot(ax=axes[1],y=0, kind='bar', legend=False)\n",
    "\n",
    "Ru = [item for sublist in R[np.in1d(R,Rs).astype(int)[0]] for item in sublist]\n",
    "letter_countss = Counter(Ru)\n",
    "dfu = pandas.DataFrame.from_dict(letter_counts, orient='index')\n",
    "dfu.sort_values(0, inplace=True)\n",
    "dfu.plot(ax=axes[2],y=0, kind='bar', legend=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14732b5bb7ad6abfe54a083b8d194ae3941adfb1b18321b588b21cb8f420fced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
