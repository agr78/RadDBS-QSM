{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary data notebook for\n",
    "# NIH: Imaging Guided Intervention Surgery Study Section\n",
    "\n",
    "# Exploratory aim: evaluate presurgical scans between STN and GPi targets\n",
    "#   Given retrospective GPi acquisitions?\n",
    "#   Search for radiomic differentiators for STN versus GPi selection in presurgical scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import nibabel as nib\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score\n",
    "import SimpleITK as sitk\n",
    "import six\n",
    "from radiomics import featureextractor \n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import RANSACRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "def remove_keymap_conflicts(new_keys_set):\n",
    "    for prop in plt.rcParams:\n",
    "        if prop.startswith('keymap.'):\n",
    "            keys = plt.rcParams[prop]\n",
    "            remove_list = set(keys) & new_keys_set\n",
    "            for key in remove_list:\n",
    "                keys.remove(key)\n",
    "\n",
    "def multi_slice_viewer(volume):\n",
    "    remove_keymap_conflicts({'j', 'k'})\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.volume = volume\n",
    "    ax.index = volume.shape[0]//2\n",
    "    ax.imshow(volume[ax.index])\n",
    "    fig.canvas.mpl_connect('key_press_event', process_key)\n",
    "\n",
    "def process_key(event):\n",
    "    fig = event.canvas.figure\n",
    "    ax = fig.axes[0]\n",
    "    if event.key == 'j':\n",
    "        previous_slice(ax)\n",
    "    elif event.key == 'k':\n",
    "        next_slice(ax)\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "\n",
    "def previous_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index-1) % volume.shape[0] \n",
    "    ax.images[0].set_array(volume[ax.index])\n",
    "\n",
    "def next_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index+1) % volume.shape[0]\n",
    "    ax.images[0].set_array(volume[ax.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or no access: '/media/mts_dbs/dbs/complete_cases/nii/seg/qsm_01.nii.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pdradenv/lib/python3.7/site-packages/nibabel/loadsave.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mstat_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/mts_dbs/dbs/complete_cases/nii/seg/qsm_01.nii.gz'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49515/1493163485.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms_directory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mseg_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_directory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/media/mts_dbs/dbs/complete_cases/nii/seg/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseg_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mvoxel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pixdim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mvoxel_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoxel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pdradenv/lib/python3.7/site-packages/nibabel/loadsave.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mstat_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No such file or no access: '{filename}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstat_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImageFileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Empty file: '{filename}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or no access: '/media/mts_dbs/dbs/complete_cases/nii/seg/qsm_01.nii.gz'"
     ]
    }
   ],
   "source": [
    "# Set window level\n",
    "level = 0\n",
    "window = 500\n",
    "m1=level-window/2\n",
    "m2=level+window/2\n",
    "visualize = 1\n",
    "# Load data\n",
    "nrows = 256\n",
    "ncols = 256\n",
    "nslices = 160\n",
    "segs = []\n",
    "qsms = []\n",
    "laros = []\n",
    "voxel_sizes = []\n",
    "trackers = []\n",
    "q_directory = '/media/mts_dbs/dbs/complete_cases/nii/qsm/'\n",
    "q_directory = os.listdir(q_directory)\n",
    "q_directory = sorted(q_directory)\n",
    "s_directory = '/media/mts_dbs/dbs/complete_cases/nii/seg/'\n",
    "s_directory = os.listdir(s_directory)\n",
    "s_directory = sorted(s_directory)\n",
    "\n",
    "case_list = []\n",
    "d_count = 0\n",
    "if visualize == 1:\n",
    "    for seg_filename in s_directory:\n",
    "        filename = q_directory[d_count]\n",
    "        seg = nib.load('/media/mts_dbs/dbs/complete_cases/nii/seg/'+seg_filename)\n",
    "        voxel_size = seg.header['pixdim'][0:3]\n",
    "        voxel_sizes.append(voxel_size)\n",
    "        segs.append(seg.get_fdata()[:nrows,:ncols,:nslices])\n",
    "        qsm = nib.load('/media/mts_dbs/dbs/complete_cases/nii/qsm/'+filename)\n",
    "        qsms.append(qsm.get_fdata()[:nrows,:ncols,:nslices])\n",
    "        print('Appending arrays with segmentation',seg_filename,'and QSM',filename)\n",
    "        case_list.append(filename)\n",
    "        n_cases = len(segs)\n",
    "        d_count = d_count+1\n",
    "        qsms_wl = np.asarray(qsms)\n",
    "        segs_wl = np.asarray(segs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize == 1:\n",
    "    qsms_wl[qsms_wl < m1] = m1\n",
    "    qsms_wl[qsms_wl > m2] = m2\n",
    "\n",
    "    multi_slice_viewer(np.hstack(((np.vstack(qsms_wl[:n_cases//2,:,:,:]/1000+0*segs_wl[:n_cases//2,:,:,:]).T),\n",
    "                                  (np.vstack(qsms_wl[(n_cases-n_cases//2):,:,:,:]/1000+0*segs_wl[(n_cases-n_cases//2):,:,:,:]).T))))\n",
    "                                    \n",
    "    label_min = np.partition(np.unique(seg.get_fdata().ravel()), 1)[1]\n",
    "    label_max = np.amax(seg.get_fdata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/media/mts_dbs/ChanghaiDBS_QSM/xlsx/updrs_iii_chh.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patient IDs\n",
    "subject_id = np.asarray(df[df.columns[0]])[1:]\n",
    "\n",
    "# Only extract ROI if it is present in all cases\n",
    "seg_labels_all = segs[0]\n",
    "case_number = np.zeros_like(np.asarray(s_directory))\n",
    "for i in range(n_cases):\n",
    "    case_number[i] = float(s_directory[i][:2])\n",
    "subject_id_corr = subject_id[np.in1d(subject_id,case_number)]\n",
    "for i in range(n_cases):\n",
    "    try:\n",
    "        print('Found ROIs',str(np.unique(segs[i])),'at segmentation directory file',s_directory[i],'for case',str(subject_id_corr[i]))\n",
    "    except:\n",
    "        print('Case',subject_id[i],'quarantined')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_updrs_iii_off =  np.asarray(df[df.columns[3]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])                                \n",
    "pre_updrs_iii_on =  np.asarray(df[df.columns[4]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])\n",
    "post_updrs_iii_off =  np.asarray(df[df.columns[6]][np.hstack((False,np.in1d(subject_id,subject_id_corr)))])\n",
    "\n",
    "per_change = (np.asarray(pre_updrs_iii_off).astype(float)-np.asarray(post_updrs_iii_off).astype(float))/(np.asarray(pre_updrs_iii_off).astype(float))\n",
    "lct_change = (np.asarray(pre_updrs_iii_off).astype(float)-(np.asarray(pre_updrs_iii_on)).astype(float))/(np.asarray(pre_updrs_iii_off).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"radiomics\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "reextract = 0\n",
    "\n",
    "# Assume all voxel sizes are identical\n",
    "voxel_size = (0.9,0.9,0.9)\n",
    "if reextract == 1:\n",
    "    # Generate feature structure Phi from all ROIs and all cases\n",
    "    extractor = featureextractor.RadiomicsFeatureExtractor()\n",
    "    extractor.enableAllFeatures()\n",
    "    extractor.enableAllImageTypes()\n",
    "    extractor.enableFeatureClassByName('shape2D',enabled = False)\n",
    "\n",
    "    seg_labels_all = np.unique(np.asarray(segs))\n",
    "    Phi_gt = []\n",
    "    Phi_vd = []\n",
    "    Phi_lr = []\n",
    "    seg_labels = []\n",
    "    reextract = 0\n",
    "    x_row_gt = []\n",
    "    x_row_lr = []\n",
    "\n",
    "    keylib = []\n",
    "    roilib = []\n",
    "    loop_count = 1\n",
    "    n_rois = seg_labels_all[seg_labels_all>0].__len__()\n",
    "    roi_names = np.asarray(['Background','Right substantia nigra','Right subthalamic nucleus',\n",
    "                            'Left subthalamic nucleus', 'Left substantia nigra', 'Right dentate nucleus', 'Left dentate nucleus'])\n",
    "    for i in np.arange(subject_id_corr.__len__()):\n",
    "        seg_sitk = sitk.GetImageFromArray(segs[i])\n",
    "        seg_sitk.SetSpacing(voxel_size)\n",
    "        qsm_sitk_gt = sitk.GetImageFromArray(qsms[i])\n",
    "        qsm_sitk_gt.SetSpacing(voxel_size)\n",
    "        qsm_sitk_lr = sitk.GetImageFromArray(laros[i])\n",
    "        qsm_sitk_lr.SetSpacing(voxel_size)\n",
    "        # Index back since subject 12 is missing ROIs\n",
    "        for j in seg_labels_all:\n",
    "            if j>0:\n",
    "                fv_count = 0\n",
    "                featureVector_gt = extractor.execute(qsm_sitk_gt,seg_sitk,label=int(j));\n",
    "                featureVector_lr = extractor.execute(qsm_sitk_lr,seg_sitk,label=int(j));\n",
    "                Phi_gt.append(featureVector_gt)\n",
    "                Phi_lr.append(featureVector_lr)\n",
    "                for key, value in six.iteritems(featureVector_gt):\n",
    "                    if 'diagnostic' in key:\n",
    "                        next\n",
    "                    else:\n",
    "                        x_row_gt.append(featureVector_gt[key])\n",
    "                        x_row_lr.append(featureVector_lr[key])\n",
    "                        fv_count = fv_count+1\n",
    "                        keylib.append(key)\n",
    "                        roilib.append(roi_names[int(j)])\n",
    "                x_row_gt.append(pre_updrs_iii_off[i])\n",
    "                x_row_lr.append(pre_updrs_iii_off[i])\n",
    "                fv_count = fv_count+1\n",
    "        print('Extracting features for subject',subject_id_corr[i],'and appending feature matrix with vector of length',fv_count,'with UPDRS score',pre_updrs_iii_off[i])\n",
    "                \n",
    "    X0_gt = np.array(x_row_gt)\n",
    "    X0_lr = np.array(x_row_lr)\n",
    "    np.save('./npy/X0_gt_chh_rois.npy',X0_gt)\n",
    "    np.save('./npy/X0_lr_chh_rois.npy',X0_lr)\n",
    "\n",
    "    K = np.asarray(keylib)\n",
    "    R = np.asarray(roi_names)\n",
    "    np.save('./npy/K_chh.npy',K)\n",
    "    np.save('./npy/R_chh.npy',R)\n",
    "\n",
    "    print('Saving ground truth feature vector')\n",
    "    with open('./phi/Phi_mcl_gt_roi_chh', 'wb') as fp:  \n",
    "        pickle.dump(Phi_gt, fp)\n",
    "    \n",
    "    print('Saving undersampled feature vector')\n",
    "    with open('./phi/Phi_mcl_lr_roi_chh', 'wb') as fp:  \n",
    "        pickle.dump(Phi_lr, fp)\n",
    "\n",
    "else:\n",
    "    X0_gt = np.load('./npy/X0_gt_chh_rois.npy')\n",
    "    X0_lr = np.load('./npy/X0_lr_chh_rois.npy')\n",
    "    K = np.load('./npy/K_chh.npy')\n",
    "    R = np.load('./npy/R_chh.npy')\n",
    "    n_rois = R.shape[0]-1\n",
    "    with open('./phi/Phi_mcl_gt_roi_chh', \"rb\") as fp:  \n",
    "        Phi_gt = pickle.load(fp)\n",
    "    \n",
    "    with open('./phi/Phi_mcl_lr_roi_chh', \"rb\") as fp:  \n",
    "        Phi_lr = pickle.load(fp)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cases = len(per_change)\n",
    "n_roisc = Phi_gt.__len__()/n_cases\n",
    "L = int(len(X0_gt)/n_cases)\n",
    "n_features = int(L/n_rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_row_gt = X0_gt.tolist()\n",
    "X = np.zeros((n_cases,n_rois,n_features)).transpose((0,2,1))\n",
    "X = X0_gt.reshape((n_cases,n_rois,n_features)).transpose((0,2,1))\n",
    "X_lr = X0_lr.reshape((n_cases,n_rois,n_features)).transpose((0,2,1))\n",
    "ut_ls = np.zeros((subject_id_corr.__len__()))\n",
    "ut_qr = np.zeros((subject_id_corr.__len__()))\n",
    "ut_qrlr = np.zeros((subject_id_corr.__len__()))\n",
    "ut_lr = np.zeros((subject_id_corr.__len__()))\n",
    "ut_en = np.zeros((subject_id_corr.__len__()))\n",
    "# Normalize testing and training cases together\n",
    "#   Set with_mean=False to preserve data sparsity\n",
    "#   And with_std=False \n",
    "#   However, need a significant number of samples to do this\n",
    "X_all = X.reshape(n_cases,((n_features)*n_rois))\n",
    "X_all_lr = X_lr.reshape(n_cases,((n_features)*n_rois))\n",
    "scaler = StandardScaler()\n",
    "X_all_t = scaler.fit_transform(X_all)\n",
    "X_all_t_lr = scaler.fit_transform(X_all_lr)\n",
    "C = np.zeros_like(X_all_t)\n",
    "for j in np.arange(len(subject_id_corr)):\n",
    "        # Add UPDRS after scaling\n",
    "        # Might be overwriting a feature here\n",
    "        X_all_t[j,-1] = pre_updrs_iii_off[int(j)]\n",
    "        X_all_t_lr[j,-1] = pre_updrs_iii_off[int(j)]\n",
    "        X_in = X_all_t\n",
    "        X_in_lr = X_all_t_lr\n",
    "        X_in = np.delete(X_in,j,axis=0)\n",
    "        X_in_lr = np.delete(X_in_lr,j,axis=0)\n",
    "        p_per_change = per_change\n",
    "        per_change_in = np.delete(p_per_change,j,axis=0)\n",
    "        # Cross-validation for model selection\n",
    "        #cv = KFold(np.sum(subject_id_corr < N_train+1)-1//2)\n",
    "        #print('Kfold complete')\n",
    "        # Identify most important features\n",
    "        clf_ls = Lasso(alpha=1e-4,max_iter=10000).fit(X_in,per_change_in)\n",
    "        clf_lr = Lasso(alpha=1e-4,max_iter=10000).fit(X_in_lr,per_change_in)\n",
    "        #clr_rr = RANSACRegressor().fit(X_in,per_change_in)\n",
    "        clf_qr = QuantileRegressor(quantile=0.25, alpha=1e-2).fit(X_in,per_change_in)\n",
    "        clf_qrlr = QuantileRegressor(quantile=0.25, alpha=1e-2).fit(X_in_lr,per_change_in)\n",
    "        clf_en = ElasticNet(alpha=1e-4,max_iter=10000,l1_ratio=0.1).fit(X_in,per_change_in)\n",
    "        #LassoCV(cv=cv,verbose=1,n_jobs=-1,max_iter=10000).fit(X_in,per_change_in)\n",
    "        #print('Cross validation complete')\n",
    "        #print('Grid search complete')\n",
    "        print('Fit complete')\n",
    "        \n",
    "        ut_ls[j] = clf_ls.predict(X_all_t[j,:].reshape(1, -1))\n",
    "        C[j] = clf_ls.coef_\n",
    "\n",
    "\n",
    "        ut_lr[j] = clf_lr.predict(X_all_t_lr[j,:].reshape(1, -1))\n",
    "        ut_qr[j] = clf_qr.predict(X_all_t[j,:].reshape(1, -1))\n",
    "        # ut_qrlr[j] = clf_qrlr.predict(X_all_t_lr[j,:].reshape(1, -1))\n",
    "        ut_en[j] = clf_en.predict(X_all_t[j,:].reshape(1, -1))\n",
    "        #print('Fully-sampled predicted percentage change of',ut_qr[j],'and under-sampled predicted',ut_qrlr[j],'for case',j)\n",
    "        #print('True percentage change',per_change[j])\n",
    "        #print('Selected alpha:',clf_ls.alpha_)\n",
    "        print('Patient ID',str(subject_id_corr[j]),'with pre-surgical UPDRS score',str(pre_updrs_iii_off[int(j)]),'at feature matrix row',str(j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (25,5)\n",
    "# Cross validation results\n",
    "[fig,ax] = plt.subplots(1,5,sharex=True, sharey=True)\n",
    "lr_prepost = linregress(lct_change,per_change)\n",
    "ax[0].scatter(lct_change,per_change,)\n",
    "ax[0].plot(lct_change,lct_change*lr_prepost.slope+lr_prepost.intercept,'-r')\n",
    "ax[0].set_title('LCT')\n",
    "ax[0].set_ylabel(\"DBS improvement\")\n",
    "ax[0].set_xlabel(\"Prediction\")\n",
    "ax[0].set_ylim([0, 2])\n",
    "text = f\"$y={lr_prepost.slope:0.3f}\\;x{lr_prepost.intercept:+0.3f}$\\n$r = {lr_prepost.rvalue:0.3f}$\\n$p = {lr_prepost.pvalue:0.3f}$\"\n",
    "ax[0].text(0.05, 0.95, text,transform=ax[0].transAxes,\n",
    "     fontsize=14, verticalalignment='top')\n",
    "ax[0].hlines(0.4,0,1,linestyle='dashed',color='white')\n",
    "ax[0].vlines(0.4,0,2,linestyle='dashed',color='white')\n",
    "\n",
    "lr_pred_ls = linregress(ut_ls,per_change)\n",
    "ax[1].scatter(ut_ls,per_change)\n",
    "ax[1].plot(ut_ls,ut_ls*lr_pred_ls.slope+lr_pred_ls.intercept,'-r')\n",
    "ax[1].set_title('Fully-sampled LASSO')\n",
    "ax[1].set_ylabel(\"DBS improvement\")\n",
    "ax[1].set_xlabel(\"Prediction\")\n",
    "text = f\"$y={lr_pred_ls.slope:0.3f}\\;x{lr_pred_ls.intercept:+0.3f}$\\n$r = {lr_pred_ls.rvalue:0.3f}$\\n$p = {lr_pred_ls.pvalue:0.3f}$\"\n",
    "ax[1].text(0.05, 0.95, text,transform=ax[1].transAxes,\n",
    "     fontsize=14, verticalalignment='top')\n",
    "ax[1].hlines(0.4,0,1,linestyle='dashed',color='white')\n",
    "ax[1].vlines(0.4,0,2,linestyle='dashed',color='white')\n",
    "\n",
    "lr_pred_lr = linregress(ut_lr,per_change)\n",
    "ax[2].scatter(ut_lr,per_change)\n",
    "ax[2].plot(ut_lr,ut_lr*lr_pred_lr.slope+lr_pred_lr.intercept,'-r')\n",
    "ax[2].set_title('Under-sampled LASSO')\n",
    "ax[2].set_ylabel(\"DBS improvement\")\n",
    "ax[2].set_xlabel(\"Prediction\")\n",
    "text = f\"$y={lr_pred_lr.slope:0.3f}\\;x{lr_pred_lr.intercept:+0.3f}$\\n$r = {lr_pred_lr.rvalue:0.3f}$\\n$p = {lr_pred_lr.pvalue:0.3f}$\"\n",
    "ax[2].text(0.05, 0.95, text,transform=ax[2].transAxes,\n",
    "     fontsize=14, verticalalignment='top')\n",
    "ax[2].hlines(0.4,0,1,linestyle='dashed',color='white')\n",
    "ax[2].vlines(0.4,0,2,linestyle='dashed',color='white')\n",
    "\n",
    "lr_pred_qr = linregress(ut_qr,per_change)\n",
    "ax[3].scatter(ut_qr,per_change)\n",
    "ax[3].plot(ut_qr,ut_qr*lr_pred_qr.slope+lr_pred_qr.intercept,'-r')\n",
    "ax[3].set_title('Fully-sampled Quartile')\n",
    "ax[3].set_ylabel(\"DBS improvement\")\n",
    "ax[3].set_xlabel(\"Prediction\")\n",
    "text = f\"$y={lr_pred_qr.slope:0.3f}\\;x{lr_pred_qr.intercept:+0.3f}$\\n$r = {lr_pred_qr.rvalue:0.3f}$\\n$p = {lr_pred_qr.pvalue:0.3f}$\"\n",
    "ax[3].text(0.05, 0.95, text,transform=ax[3].transAxes,\n",
    "     fontsize=14, verticalalignment='top')\n",
    "ax[3].hlines(0.4,0,1,linestyle='dashed',color='white')\n",
    "ax[3].vlines(0.4,0,2,linestyle='dashed',color='white')\n",
    "\n",
    "lr_pred_qrlr = linregress(ut_en,per_change)\n",
    "ax[4].scatter(ut_en,per_change)\n",
    "ax[4].plot(ut_en,ut_en*lr_pred_qrlr.slope+lr_pred_qrlr.intercept,'-r')\n",
    "ax[4].set_title('Fully-sampled ElasticNet')\n",
    "ax[4].set_ylabel(\"DBS improvement\")\n",
    "ax[4].set_xlabel(\"Prediction\")\n",
    "text = f\"$y={lr_pred_qrlr.slope:0.3f}\\;x{lr_pred_qr.intercept:+0.3f}$\\n$r = {lr_pred_qrlr.rvalue:0.3f}$\\n$p = {lr_pred_qrlr.pvalue:0.3f}$\"\n",
    "ax[4].text(0.05, 0.95, text,transform=ax[4].transAxes,\n",
    "     fontsize=14, verticalalignment='top')\n",
    "ax[4].hlines(0.4,0,1,linestyle='dashed',color='white')\n",
    "ax[4].vlines(0.4,0,2,linestyle='dashed',color='white')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(per_change<0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfs = []\n",
    "for j in np.arange(subject_id_corr.__len__()):\n",
    "    Kr =  K.reshape((n_cases,n_rois,(n_features-1))).transpose((0,2,1))\n",
    "    Kr_extended = np.zeros((n_cases,n_rois,n_features)).transpose((0,2,1)).astype('str')\n",
    "    Kr_extended[:,0:n_features-1,:] = Kr\n",
    "    Kr_extended[:,-1,:] = 'po_updrs'\n",
    "    rfs.append(Kr_extended[j,np.asarray(C[j]!=0).reshape((n_rois,n_features)).transpose((1,0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from collections import Counter\n",
    "R = [item for sublist in rfs for item in sublist]\n",
    "letter_counts = Counter(R)\n",
    "df = pandas.DataFrame.from_dict(letter_counts, orient='index')\n",
    "df.sort_values('number', inplace=True)\n",
    "df.plot(y='number', kind='bar', legend=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdradenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14732b5bb7ad6abfe54a083b8d194ae3941adfb1b18321b588b21cb8f420fced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
